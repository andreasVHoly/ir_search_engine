121:1161:Q-learning is a model-free reinforcement learning technique. Specifically, Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. A policy is a rule that the agent follows in selecting actions, given the state it is in. When such an action-value function is learned, the optimal policy can be constructed by simply selecting the action with the highest value in each state. One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment. Additionally, Q-learning can handle problems with stochastic transitions and rewards, without requiring any adaptations. It has been proven that for any finite MDP, Q-learning eventually finds an optimal policy, in the sense that the expected value of the total reward return over all successive steps, starting from the current state, is the maximum achievable.
85:658:The combination of quality control and genetic algorithms led to novel solutions of complex quality control design and optimization problems. Quality control is a process by which entities review the quality of all factors involved in production. Quality is the degree to which a set of inherent characteristics fulfils a need or expectation that is stated, general implied or obligatory. Genetic algorithms are search algorithms, based on the mechanics of natural selection and natural genetics.
176:1027:A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.
12:298:Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, scientific discovery and toys. However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore." "Many thousands of AI applications are deeply embedded in the infrastructure of every industry." In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes.
169:906:Conceptual clustering is a machine learning paradigm for unsupervised classification developed mainly during the 1980s. It is distinguished from ordinary data clustering by generating a concept description for each generated class. Most conceptual clustering methods are capable of generating hierarchical category structures; see Categorization for more information on hierarchy. Conceptual clustering is closely related to formal concept analysis, decision tree learning, and mixture model learning.
35:479:Waffles is a collection of command-line tools for performing machine learning operations developed at Brigham Young University. These tools are written in C++, and are available under the GNU Lesser General Public License.
51:424:Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning algorithms.
21:347:The MOEA Framework is an open-source evolutionary computation library for Java that specializes in multi-objective optimization. It supports a variety of multiobjective evolutionary algorithms (MOEAs), including genetic algorithms, genetic programming, grammatical evolution, differential evolution, and particle swarm optimization. As a result, it has been used to conduct numerous comparative studies to assess the efficiency, reliability, and controllability of state-of-the-art MOEAs.
113:0:k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results is a partitioning of the data space into Voronoi cells. The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm. == Description == Given a set of observations (x1, x2, …, xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, …, Sk} so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center). In other words, its objective is to find: where μi is the mean of points in Si. == History == The term "k-means" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1957. The standard algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation, though it wasn't published outside of Bell Labs until 1982. In 1965, E.W.Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy. A more efficient version was proposed and published in Fortran by Hartigan and Wong in 1975/1979. == Algorithms == === Standard algorithm === The most common algorithm uses an iterative refinement technique. Due to its ubiquity it is often called the k-means algorithm; it is also referred to as Lloyd's algorithm, particularly in the computer science community. Given an initial set of k means m1(1),…,mk(1) (see below), the algorithm proceeds by alternating between two steps: Assignment step: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS). Since the sum of squares is the squared Euclidean distance, this is intuitively the "nearest" mean. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means). where each is assigned to exactly one , even if it could be assigned to two or more of them. Update step: Calculate the new means to be the centroids of the observations in the new clusters. Since the arithmetic mean is a least-squares estimator, this also minimizes the within-cluster sum of squares (WCSS) objective. The algorithm has converged when the assignments no longer change. Since both steps optimize the WCSS objective, and there only exists a finite number of such partitionings, the algorithm must converge to a (local) optimum. There is no guarantee that the global optimum is found using this algorithm. The algorithm is often presented as assigning objects to the nearest cluster by distance. The standard algorithm aims at minimizing the WCSS objective, and thus assigns by "least sum of squares", which is exactly equivalent to assigning by the smallest Euclidean distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures. ==== Initialization methods ==== Commonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. Demonstration of the standard algorithm As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, k-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which k-means takes exponential time, that is 2Ω(n), to converge. These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial. The "assignment" step is also referred to as expectation step, the "update step" as maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm. === Complexity === Regarding computational complexity, finding the optimal solution to the k-means clustering problem for observations in d dimensions is: NP-hard in general Euclidean space d even for 2 clusters NP-hard for a general number of clusters k even in the plane If k and d (the dimension) are fixed, the problem can be exactly solved in time , where n is the number of entities to be clustered Thus, a variety of heuristic algorithms such as Lloyds algorithm given above are generally used. The running time of Lloyds algorithm is often given as , where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence. On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyds algorithm is therefore often considered to be of "linear" complexity in practice. Following are some recent insights into this algorithm complexity behavior. Lloyd's k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in , if each point is independently perturbed by a normal distribution with mean 0 and variance , then the expected running time of k-means algorithm is bounded by , which is a polynomial in n, k, d and . Better bounds are proved for simple cases. For example, showed that the running time of k-means algorithm is bounded by for n points in an integer lattice . Lloyd's algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use the triangle inequality in order to create bounds and accelerate Lloyds algorithm. === Variations === Jenks natural breaks optimization: k-means applied to univariate data k-medians clustering uses the median in each dimension instead of the mean, and this way minimizes norm (Taxicab geometry). k-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions. Fuzzy C-Means Clustering is a soft version of K-means, where each data point has a fuzzy degree of belonging to each cluster. Gaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means. k-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective. The filtering algorithm uses kd-trees to speed up each k-means step. Some methods attempt to speed up each k-means step using the triangle inequality. Escape local optima by swapping points between clusters. The Spherical k-means clustering algorithm is suitable for directional data. X-means clustering and G-means clustering try to automatically determine the number of clusters. Internal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters. Minkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters. == Discussion == Three key features of k-means which make it efficient are often regarded as its biggest drawbacks: Euclidean distance is used as a metric and variance is used as a measure of cluster scatter. The number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set. Convergence to a local minimum may produce counterintuitive ("wrong") results (see example in Fig.). A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable in a way so that the mean value converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With , the two visible clusters (one containing two species) will be discovered, whereas with one of the two clusters will be split into two even parts. In fact, is more appropriate for this data set, despite the data set containing 3 classes. As with any other clustering algorithm, the k-means result relies on the data set to satisfy the assumptions made by the clustering algorithms. It works well on some data sets, while failing on others. The result of k-means can also be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the "mouse" example. The Gaussian models used by the Expectation-maximization algorithm (which can be seen as a generalization of k-means) 
4:0:Artificial intelligence has been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, remote sensing, scientific discovery and toys. However, due to the AI effect, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore," Nick Bostrom reports. "Many thousands of AI applications are deeply embedded in the infrastructure of every industry." In the late 90s and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes. == Computer science == AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI. (See AI effect). According to Russell & Norvig (2003, p. 15), all of the following were originally developed in AI laboratories: time sharing, interactive interpreters, graphical user interfaces and the computer mouse, rapid development environments, the linked list data structure, automatic storage management, symbolic programming, functional programming, dynamic programming and object-oriented programming. == Finance == Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. Use of AI in banking can be tracked back to 1987 when Security Pacific National Bank in USA set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Apps like Kasisito and Moneystream are using AI in financial services Banks use artificial intelligence systems to organize operations, invest in stocks, and manage properties. In August 2001, robots beat humans in a simulated financial trading competition. == Hospitals and medicine == Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in Concept Processing technology in EMR software. Other tasks in medicine that can potentially be performed by artificial intelligence include: Computer-aided interpretation of medical images. Such systems help scan digital images, e.g. from computed tomography, for typical appearances and to highlight conspicuous sections, such as possible diseases. A typical application is the detection of a tumor. Heart sound analysis Watson project is another use of AI in this field,a Q/A program that suggest for doctor's of cancer patients. Companion robots for the care of the elderly == Heavy industry == Robots have become common in many industries and are often given jobs that are considered dangerous to humans. Robots have proven effective in jobs that are very repetitive which may lead to mistakes or accidents due to a lapse in concentration and other jobs which humans may find degrading. In 2014, China, Japan, the United States, the Republic of Korea and Germany together amounted to 70% of the total sales volume of robots. In the automotive industry, a sector with particularly high degree of automation, Japan had the highest density of industrial robots in the world: 1,414 per 10,000 employees. == Online and telephone customer service == Artificial intelligence is implemented in automated online assistants that can be seen as avatars on web pages. It can avail for enterprises to reduce their operation and training cost. A major underlying technology to such systems is natural language processing. == Transportation == Fuzzy logic controllers have been developed for automatic gearboxes in automobiles. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission which utilizes Fuzzy Logic. A number of Škoda variants (Škoda Fabia) also currently include a Fuzzy Logic-based controller. == Telecommunications maintenance == Many telecommunications companies make use of heuristic search in the management of their workforces, for example BT Group has deployed heuristic search in a scheduling application that provides the work schedules of 20,000 engineers. == Toys and games == The 1990s saw some of the first attempts to mass-produce domestically aimed types of basic Artificial Intelligence for education, or leisure. This prospered greatly with the Digital Revolution, and helped introduce people, especially children, to a life of dealing with various types of Artificial Intelligence, specifically in the form of Tamagotchis and Giga Pets, iPod Touch, the Internet (example: basic search engine interfaces are one simple form), and the first widely released robot, Furby. A mere year later an improved type of domestic robot was released in the form of Aibo, a robotic dog with intelligent features and autonomy. AI has also been applied to video games, for example video game bots, which are designed to stand in as opponents where humans aren't available or desired; or the AI Director from Left 4 Dead, which decides where enemies spawn and how maps are laid out to be more or less challenging at various points of play. == Music == The evolution of music has always been affected by technology. With AI, scientists are trying to make the computer emulate the activities of the skillful musician. Composition, performance, music theory, sound processing are some of the major areas on which research in Music and Artificial Intelligence are focusing. Among these efforts, Melomics seems to be ahead by powering computer-composers that learn to compose the way humans do. == Aviation == The Air Operations Division (AOD) uses AI for the rule based expert systems. The AOD has use for artificial intelligence for surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries. The use of artificial intelligence in simulators is proving to be very useful for the AOD. Airplane simulators are using artificial intelligence in order to process the data taken from simulated flights. Other than simulated flying, there is also simulated aircraft warfare. The computers are able to come up with the best success scenarios in these situations. The computers can also create strategies based on the placement, size, speed and strength of the forces and counter forces. Pilots may be given assistance in the air during combat by computers. The artificial intelligent programs can sort the information and provide the pilot with the best possible maneuvers, not to mention getting rid of certain maneuvers that would be impossible for a human being to perform. Multiple aircraft are needed to get good approximations for some calculations so computer simulated pilots are used to gather data. These computer simulated pilots are also used to train future air traffic controllers. The system used by the AOD in order to measure performance was the Interactive Fault Diagnosis and Isolation System, or IFDIS. It is a rule based expert system put together by collecting information from TF-30 documents and the expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the RAAF F-111C. The performance system was also used to replace specialized workers. The system allowed the regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers. The AOD also uses artificial intelligence in speech recognition software. The air traffic controllers are giving directions to the artificial pilots and the AOD wants to the pilots to respond to the ATC's with simple responses. The programs that incorporate the speech software must be trained, which means they use neural networks. The program used, the Verbex 7000, is still a very early program that has plenty of room for improvement. The improvements are imperative because ATCs use very specific dialog and the software needs to be able to communicate correctly and promptly every time. The Artificial Intelligence supported Design of Aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective. In 2003, NASA's Dryden Flight Research Center, and many other companies, created software that could enable a damaged aircraft to continue flight until a safe landing zone can be reached. The software compensates for all the damaged components by relying on the undamaged components. The neural network used in the software proved to be effective and marked a triumph for artificial intelligence. The Integrated Vehicle Health Management system, also used by NASA, on board an aircraft must process and interpret data taken from the various sensors on the aircraft. The system needs to be able to determine the structural integrity of the aircraft. The system also needs to implement protocols in case of any damage taken the vehicle. == News, publishing and writing == The company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game in English. It also creates financial reports and real estate analyses. The company Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football. The company is projected to generate one billion stories in 2014, up from 350 million in 2013. Another company, called Yseop, uses artificial intelligence to turn structured data into intelligent comments and recommendations in natural language. Yseop is able to write financial reports, executive summaries, personalized sales or marketing documents and more at a speed of thousands of pages per second and in multiple languages including English, Spanish, French & German. == Other == Various tools of artificial intelligence are also being widely deployed in homeland security, speech and text recognition, data mining, and e-mail spam filtering. Applications are also being developed for gesture recognition (understanding of sign language by machines), individual voice recognition, global voice recognition (from a variety of people in a noisy room), facial expression recognition for interpretation of emotion and non verbal cues. Other applications are robot navigation, obstacle avoidance, and object recognition. == List of applications == Typical problems to which AI methods are applied Other fields in which AI methods are implemented == See also == List of artificial intelligence projects Progress in artificial intelligence == Notes == == External links == AI applications at aitopics.org == References == Russell, Stuart J.; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach (2nd ed.). Upper Saddle River, New Jersey: Prentice Hall. ISBN 0-13-790395-2 Kurzweil, Ray (2005). The Singularity is Near: When Humans Transcend Biology. New York: Viking. ISBN 978-0-670-03384-3 National Research Council (1999). "Developments in Artificial Intelligence". Funding a Revolution: Government Support for Computing Research. National Academy Press. ISBN 0-309-06278-0. OCLC 246584055. Moghaddam, M. J., M. R. Soleymani, and M. A. Farsi. "Sequence planning for stamping operations in 
141:560:Numenta is a machine intelligence company that has developed a cohesive theory, core software, technology and applications based on the principles of the neocortex. The company was founded on February 4, 2005 by Palm founder Jeff Hawkins with his longtime business partner Donna Dubinsky and Stanford graduate student Dileep George. Numenta is headquartered in Redwood City, California and is privately funded.
37:0:In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) which are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning. For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, the output neuron that determines which character was read is activated. Like other machine learning methods – systems that learn from data – neural networks have been used to solve a wide variety of tasks, like computer vision and speech recognition, that are hard to solve using ordinary rule-based programming. == Background == Examinations of humans' central nervous systems inspired the concept of artificial neural networks. In an artificial neural network, simple artificial nodes, known as "neurons", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network. There is no single formal definition of what an artificial neural network is. However, a class of statistical models may commonly be called "neural" if it possesses the following characteristics: contains sets of adaptive weights, i.e. numerical parameters that are tuned by a learning algorithm, and is capable of approximating non-linear functions of their inputs. The adaptive weights can be thought of as connection strengths between neurons, which are activated during training and prediction. Artificial neural networks are similar to biological neural networks in the performing by its units of functions collectively and in parallel, rather than by a clear delineation of subtasks to which individual units are assigned. The term "neural network" usually refers to models employed in statistics, cognitive psychology and artificial intelligence. Neural network models which command the central nervous system and the rest of the brain are part of theoretical neuroscience and computational neuroscience. In modern software implementations of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing. In some of these systems, neural networks or parts of neural networks (like artificial neurons) form components in larger systems that combine both adaptive and non-adaptive elements. While the more general approach of such systems is more suitable for real-world problem solving, it has little to do with the traditional, artificial intelligence connectionist models. What they do have in common, however, is the principle of non-linear, distributed, parallel and local processing and adaptation. Historically, the use of neural network models marked a directional shift in the late eighties from high-level (symbolic) artificial intelligence, characterized by expert systems with knowledge embodied in if-then rules, to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a dynamical system. == History == Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. === Hebbian learning === In the late 1940s psychologist Donald Hebb created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Wesley A. Clark (1954) first used computational machines, then called "calculators," to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956). Frank Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer computer learning network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit which could not be processed by neural networks until after the backpropagation algorithm was created by Paul Werbos (1975). Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969), who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second significant issue was that computers didn't have enough processing power to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. === Backpropagation and Resurgence === A key advance that came later was the backpropagation algorithm which effectively solved the exclusive-or problem, and more generally the problem of quickly training multi-layer neural networks (Werbos 1975). In the mid-1980s, parallel distributed processing became popular under the name connectionism. The textbook by David E. Rumelhart and James McClelland (1986) provided a full exposition of the use of connectionism in computers to simulate neural processes. Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and the biological architecture of the brain is debated; it's not clear to what degree artificial neural networks mirror brain function. Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. But the advent of deep learning in the late 2000s sparked renewed interest in neural networks. === Improvements since 2006 === Computational devices have been created in CMOS, for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, would create a new class of neural computing because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short term memory (LSTM) of Alex Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned. Fast GPU-based implementations of this approach by Dan Ciresan and colleagues at IDSIA have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge, and others. Their neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance on important benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun at NYU. Deep, highly nonlinear neural architectures similar to the 1980 neocognitron by Kunihiko Fukushima and the "standard architecture of vision", inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex, can also be pre-trained by unsupervised methods of Geoff Hinton's lab at University of Toronto. A team from this lab won a 2012 contest sponsored by Merck to design software to help find molecules that might lead to new drugs. == Models == Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function or a distribution over or both and , but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase "ANN model" is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity). === Network function === The word network in the term 'artificial neural network' refers to the inter–connections between the neurons in the different layers of each system. An example system has three layers. The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons. More complex systems will have more layers of neurons, some having increased layers of input neurons and output neurons. The synapses store parameters called "weights" that manipulate the data in the calculations. An ANN is typically defined by three types of parameters: The interconnection pattern between the different layers of neurons The learning process for updating the weights of the interconnections The activation function that converts a neuron's weighted input to its output activation. Mathematically, a neuron's network function is defined as a composition of other functions , which can further be defined as a composition of other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between variables. A widely used type of composition is the nonlinear weighted sum, where , where (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent. It will be convenient for the following to refer to a collection of functions as simply a vector . This figure depicts such a decomposition of , with dependencies between variables indicated by arrows. These can be interpreted in two ways. The first view is the functional view: the input is transformed into a 3-dimensional vector , which is then transformed into a 2-dimensional vector , which is finally transformed into . This view is most commonly encountered in the context of optimization. The second view is the probabilistic view: the random variable depends upon the random variable , which depends upon , which depends upon the random variable . This view is most commonly encountered in the context of graphical models. The two views are largely equivalent. In either case, for this particular network architecture, the components of individual layers are independent of each other (e.g., the components of are independent of each other given their input ). This naturally enables a degree of parallelism in the implementation. Networks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks 
63:1263:In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Artificial evolution (AE) describes a process involving individual evolutionary algorithms; EAs are individual components that participate in an AE..
174:1134:Inductive logic programming (ILP) is a subfield of machine learning which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.
152:439:Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s.
139:0:Hierarchical temporal memory (HTM) is an unsupervised to semi-supervised online machine learning model developed by Jeff Hawkins and Dileep George of Numenta, Inc. that models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on the memory-prediction theory of brain function described by Jeff Hawkins in his book On Intelligence. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences, thus building an increasingly complex model of the world. Jeff Hawkins states that HTM does not present any new idea or theory, but combines existing ideas to mimic the neocortex with a simple design that provides a large range of capabilities. HTM combines and extends approaches used in Sparse distributed memory, Bayesian networks, spatial and temporal clustering algorithms, while using a tree-shaped hierarchy of nodes that is common in neural networks. == HTM structure and algorithms == A typical HTM network is a tree-shaped hierarchy of levels that are composed of smaller elements called nodes or columns. A single level in the hierarchy is also called a region. Higher hierarchy levels often have fewer nodes and therefore less spatial resolvability. Higher hierarchy levels can reuse patterns learned at the lower levels by combining them to memorize more complex patterns. Each HTM node has the same basic functionality. In learning and inference modes, sensory data comes into the bottom level nodes. In generation mode, the bottom level nodes output the generated pattern of a given category. The top level usually has a single node that stores the most general categories (concepts) which determine, or are determined by, smaller concepts in the lower levels which are more restricted in time and space. When in inference mode, a node in each level interprets information coming in from its child nodes in the lower level as probabilities of the categories it has in memory. Each HTM region learns by identifying and memorizing spatial patterns - combinations of input bits that often occur at the same time. It then identifies temporal sequences of spatial patterns that are likely to occur one after another. === Zeta 1: first generation node algorithms === During training, a node receives a temporal sequence of spatial patterns as its input. The learning process consists of two stages: Spatial pooling identifies frequently observed patterns and memorizes them as coincidences. Patterns that are significantly similar to each other are treated as the same coincidence. A large number of possible input patterns are reduced to a manageable number of known coincidences. Temporal pooling partitions coincidences that are likely to follow each other in the training sequence into temporal groups. Each group of patterns represents a "cause" of the input pattern (or "name" in On Intelligence). During inference (recognition), the node calculates the set probabilities that a pattern belongs to each known coincidence. Then it calculates the probabilities that the input represents each temporal group. The set of probabilities assigned to the groups is called a node's "belief" about the input pattern. (In a simplified implementation, node's belief consists of only one winning group). This belief is the result of the inference that is passed to one or more "parent" nodes in the next higher level of the hierarchy. "Unexpected" patterns to the node do not have a dominant probability of belonging to any one temporal group, but have nearly equal probabilities of belonging to several of the groups. If sequences of patterns are similar to the training sequences, then the assigned probabilities to the groups will not change as often as patterns are received. The output of the node will not change as much, and a resolution in time is lost. In a more general scheme, the node's belief can be sent to the input of any node(s) in any level(s), but the connections between the nodes are still fixed. The higher-level node combines this output with the output from other child nodes thus forming its own input pattern. Since resolution in space and time is lost in each node as described above, beliefs formed by higher-level nodes represent an even larger range of space and time. This is meant to reflect the organization of the physical world as it is perceived by human brain. Larger concepts (e.g. causes, actions and objects) are perceived to change more slowly and consist of smaller concepts that change more quickly. Jeff Hawkins postulates that brains evolved this type of hierarchy to match, predict, and affect the organization of the external world. More details about the functioning of Zeta 1 HTM can be found in Numenta's old documentation. === Cortical learning algorithms === The new generation of HTM learning algorithms relies on fixed-sparsity distributed representations. It models cortical columns that tend to inhibit neighboring columns in the neocortex thus creating a sparse activation of columns. A region creates a sparse representation from its input, so that a fixed percentage of columns are active at any one time. Each HTM region consists of a number of highly interconnected cortical columns. A region is similar to layer III of the neocortex. A cortical column is understood as a group of cells that have the same receptive field. Each column has a number of cells that are able to remember several previous states. A cell can be in one of three states: active, inactive and predictive state. Spatial pooling: The receptive field of each column is a fixed number of inputs that are randomly selected from a much larger number of node inputs. Based on the input pattern, some columns will receive more active input values. Spatial pooling selects a relatively constant number of the most active columns and inactivates (inhibits) other columns in the vicinity of the active ones. Similar input patterns tend to activate a stable set of columns. The amount of memory used by each region can be increased to learn more complex spatial patterns or decreased to learn simpler patterns. Representing the input in the context of previous inputs: If one or more cells in the active column are in the predictive state (see below), they will be the only cells to become active in the current time step. If none of the cells in the active column are in the predictive state (during the initial time step or when the activation of this column was not expected), all cells are made active. Predicting future inputs and temporal pooling: When a cell becomes active, it gradually forms connections to nearby cells that tend to be active during several previous time steps. Thus a cell learns to recognize a known sequence by checking whether the connected cells are active. If a large number of connected cells are active, this cell switches to the predictive state in anticipation of one of the few next inputs of the sequence. The output of a region includes columns in both active and predictive states. Thus columns are active over longer periods of time, which leads to greater temporal stability seen by the parent region. Cortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM region to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the region. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted. Cortical learning algorithms are currently being offered as commercial SaaS by Numenta (such as Grok). The following question was posed to Jeff Hawkins September 2011 with regard to Cortical learning algorithms: "How do you know if the changes you are making to the model are good or not?" To which Jeff's response was "There are two categories for the answer: one is to look at neuroscience, and the other is methods for machine intelligence. In the neuroscience realm there are many predictions that we can make, and those can be tested. If our theories explain a vast array of neuroscience observations then it tells us that we’re on the right track. In the machine learning world they don’t care about that, only how well it works on practical problems. In our case that remains to be seen. To the extent you can solve a problem that no one was able to solve before, people will take notice." == Comparing HTM and neocortex == Comparing high-level structures and functionality of neocortex with HTM is most appropriate. HTM attempts to implement the functionality that is characteristic of a hierarchically related group of cortical regions in the neocortex. A region of the neocortex corresponds to one or more levels in the HTM hierarchy, while the hippocampus is remotely similar to the highest HTM level. A single HTM node may represent a group of cortical columns within a certain region. Although it is primarily a functional model, several attempts have been made to relate the algorithms of the HTM with the structure of neuronal connections in the layers of neocortex. The neocortex is organized in vertical columns of 6 horizontal layers. The 6 layers of cells in the neocortex should not be confused with levels in an HTM hierarchy. HTM nodes attempt to model a portion of cortical columns (80 to 100 neurons) with approximately 20 HTM "cells" per column. HTMs model only layers 2 and 3 to detect spatial and temporal features of the input with 1 cell per column in layer 2 for spatial "pooling", and 1 to 2 dozen per column in layer 3 for temporal pooling. A key to HTMs and the cortex's is their ability to deal with noise and variation in the input which is a result of using a "sparse distributive representation" where only about 2% of the columns are active at any given time. An HTM attempts to model a portion of the cortex's learning and plasticity as described above. Differences between HTMs and neurons include: strictly binary signals and synapses no direct inhibition of synapses or dendrites (but simulated indirectly) currently only models layers 2/3 and 4 (no 5 or 6) no "motor" control (layer 5) no feed-back between regions (layer 6 of high to layer 1 of low) == Sparse distributed representations == Integrating memory component with neural networks has a long history dating back to early research in distributed representations and self-organizing maps. For example, in sparse distributed memory (SDM), the patterns encoded by neural networks are used as memory addresses for content-addressable memory, with "neurons" essentially serving as address encoders and decoders. 
53:739:Neuroevolution, or neuro-evolution, is a form of machine learning that uses evolutionary algorithms to train artificial neural networks. It is most commonly applied in artificial life, computer games, and evolutionary robotics. A main benefit is that neuroevolution can be applied more widely than supervised learning algorithms, which require a syllabus of correct input-output pairs. In contrast, neuroevolution requires only a measure of a network's performance at a task. For example, the outcome of a game (i.e. whether one player won or lost) can be easily measured without providing labeled examples of desired strategies.
84:346:Selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).
54:256:Compositional pattern-producing networks (CPPNs), are a variation of artificial neural networks (ANNs) which differ in their set of activation functions and how they are applied.
77:476:In function optimization, fitness approximation is a method for decreasing the number of fitness function evaluations to reach a target solution. It belongs to the general class of evolutionary computation or artificial evolution methodologies.
117:0:Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality. In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible. Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs. == Introduction == The basic reinforcement learning model consists of: a set of environment states ; a set of actions ; rules of transitioning between states; rules that determine the scalar immediate reward of a transition; and rules that describe what the agent observes. The rules are often stochastic. The observation typically involves the scalar immediate reward associated with the last transition. In many works, the agent is also assumed to observe the current environmental state, in which case we talk about full observability, whereas in the opposing case we talk about partial observability. Sometimes the set of actions available to the agent is restricted (e.g., you cannot spend more money than what you possess). A reinforcement learning agent interacts with its environment in discrete time steps. At each time , the agent receives an observation , which typically includes the reward . It then chooses an action from the set of actions available, which is subsequently sent to the environment. The environment moves to a new state and the reward associated with the transition is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can choose any action as a function of the history and it can even randomize its action selection. When the agent's performance is compared to that of an agent which acts optimally from the beginning, the difference in performance gives rise to the notion of regret. Note that in order to act near optimally, the agent must reason about the long term consequences of its actions: In order to maximize my future income I had better go to school now, although the immediate monetary reward associated with this might be negative. Thus, reinforcement learning is particularly well suited to problems which include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers (Sutton and Barto 1998, Chapter 11) and go (AlphaGo). Two components make reinforcement learning powerful: The use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in any of the following situations: A model of the environment is known, but an analytic solution is not available; Only a simulation model of the environment is given (the subject of simulation-based optimization); The only way to collect information about the environment is by interacting with it. The first two of these problems could be considered planning problems (since some form of the model is available), while the last one could be considered as a genuine learning problem. However, under a reinforcement learning methodology both planning problems would be converted to machine learning problems. == Exploration == The reinforcement learning problem as described requires clever exploration mechanisms. Randomly selecting actions, without reference to an estimated probability distribution, is known to give rise to very poor performance. The case of (small) finite MDPs is relatively well understood by now. However, due to the lack of algorithms that would provably scale well with the number of states (or scale to problems with infinite state spaces), in practice people resort to simple exploration methods. One such method is -greedy, when the agent chooses the action that it believes has the best long-term effect with probability , and it chooses an action uniformly at random, otherwise. Here, is a tuning parameter, which is sometimes changed, either according to a fixed schedule (making the agent explore less as time goes by), or adaptively based on some heuristics (Tokic & Palm, 2011). == Algorithms for control learning == Even if the issue of exploration is disregarded and even if the state was observable (which we assume from now on), the problem remains to find out which actions are good based on past experience. === Criterion of optimality === For simplicity, assume for a moment that the problem studied is episodic, an episode ending when some terminal state is reached. Assume further that no matter what course of actions the agent takes, termination is inevitable. Under some additional mild regularity conditions the expectation of the total reward is then well-defined, for any policy and any initial distribution over the states. Here, a policy refers to a mapping that assigns some probability distribution over the actions to all possible histories. Given a fixed initial distribution , we can thus assign the expected return to policy : where the random variable denotes the return and is defined by where is the reward received after the -th transition, the initial state is sampled at random from and actions are selected by policy . Here, denotes the (random) time when a terminal state is reached, i.e., the time when the episode terminates. In the case of non-episodic problems the return is often discounted, giving rise to the total expected discounted reward criterion. Here is the so-called discount-factor. Since the undiscounted return is a special case of the discounted return, from now on we will assume discounting. Although this looks innocent enough, discounting is in fact problematic if one cares about online performance. This is because discounting makes the initial time steps more important. Since a learning agent is likely to make mistakes during the first few steps after its "life" starts, no uninformed learning algorithm can achieve near-optimal performance under discounting even if the class of environments is restricted to that of finite MDPs. (This does not mean though that, given enough time, a learning agent cannot figure how to act near-optimally, if time was restarted.) The problem then is to specify an algorithm that can be used to find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of the so-called stationary policies. A policy is called stationary if the action-distribution returned by it depends only on the last state visited (which is part of the observation history of the agent, by our simplifying assumption). In fact, the search can be further restricted to deterministic stationary policies. A deterministic stationary policy is one which deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality. === Brute force === The brute force approach entails the following two steps: For each possible policy, sample returns while following it Choose the policy with the largest expected return One problem with this is that the number of policies can be extremely large, or even infinite. Another is that variance of the returns might be large, in which case a large number of samples will be required to accurately estimate the return of each policy. These problems can be ameliorated if we assume some structure and perhaps allow samples generated from one policy to influence the estimates made for another. The two main approaches for achieving this are value function estimation and direct policy search. === Value function approaches === Value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the "current" or the optimal one). These methods rely on the theory of MDPs, where optimality is defined in a sense which is stronger than the above one: A policy is called optimal if it achieves the best expected return from any initial state (i.e., initial distributions play no role in this definition). Again, one can always find an optimal policy amongst stationary policies. To define optimality in a formal manner, define the value of a policy by where stands for the random return associated with following from the initial state . Define as the maximum possible value of , where is allowed to change: A policy which achieves these optimal values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return , since , where is a state randomly sampled from the distribution . Although state-values suffice to define optimality, it will prove to be useful to define action-values. Given a state , an action and a policy , the action-value of the pair under is defined by where, now, stands for the random return associated with first taking action in state and following , thereafter. It is well-known from the theory of MDPs that if someone gives us for an optimal policy, we can always choose optimal actions (and thus act optimally) by simply choosing the action with the highest value at each state. The action-value function of such an optimal policy is called the optimal action-value function and is denoted by . In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally. Assuming full knowledge of the MDP, there are two basic approaches to compute the optimal action-value function, value iteration and policy iteration. Both algorithms compute a sequence of functions () which converge to . Computing these functions involves computing expectations over the whole state-space, 
44:1573:Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
158:488:Hans-Paul Schwefel (born December 4, 1940 in Berlin) is a German computer scientist and professor emeritus at University of Dortmund (now Dortmund University of Technology), where he held the chair of systems analysis from 1985 until 2006. He is one of the pioneers in evolutionary computation and one of the authors responsible for the evolution strategies (Evolutionsstrategien). His work has helped to understand the dynamics of evolutionary algorithms and to put evolutionary computation on formal grounds.
28:453:Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.
65:518:Jean Decety is an American and French neuroscientist specializing in developmental neuroscience, affective neuroscience, and social neuroscience. His research focuses on the cognitive and neurobiological mechanisms underpinning social cognition, particularly emotion, empathy, moral reasoning, altruism, pro-social behavior, and more generally interpersonal processes. He is Irving B. Harris Distinguished Service Professor at the University of Chicago.
93:1016:Virtual screening (VS) is a computational technique used in drug discovery to search libraries of small molecules in order to identify those structures which are most likely to bind to a drug target, typically a protein receptor or enzyme.
22:499:In computer science, an evolution strategy (ES) is an optimization technique based on ideas of adaptation and evolution. It belongs to the general class of evolutionary computation or artificial evolution methodologies.
67:640:Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture (an artificial neural network) published in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. Like most RNNs, an LSTM network is universal in the sense that given enough network units it can compute anything a conventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program. Unlike traditional RNNs, an LSTM network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknown size between important events. This is one of the main reasons why LSTM outperforms alternative RNNs and Hidden Markov Models and other sequence learning methods in numerous applications. For example, LSTM achieved the best known results in unsegmented connected handwriting recognition, and in 2009 won the ICDAR handwriting competition. LSTM networks have also been used for automatic speech recognition, and were a major component of a network that in 2013 achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset.
129:281:OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License.
156:0:CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy. Evolution strategies (ES) are stochastic, derivative-free methods for numerical optimization of non-linear or non-convex continuous optimization problems. They belong to the class of evolutionary algorithms and evolutionary computation. An evolutionary algorithm is broadly based on the principle of biological evolution, namely the repeated interplay of variation (via recombination and mutation) and selection: in each generation (iteration) new individuals (candidate solutions, denoted as ) are generated by variation, usually in a stochastic way, of the current parental individuals. Then, some individuals are selected to become the parents in the next generation based on their fitness or objective function value . Like this, over the generation sequence, individuals with better and better -values are generated. In an evolution strategy, new candidate solutions are sampled according to a multivariate normal distribution in the . Recombination amounts to selecting a new mean value for the distribution. Mutation amounts to adding a random vector, a perturbation with zero mean. Pairwise dependencies between the variables in the distribution are represented by a covariance matrix. The covariance matrix adaptation (CMA) is a method to update the covariance matrix of this distribution. This is particularly useful, if the function is ill-conditioned. Adaptation of the covariance matrix amounts to learning a second order model of the underlying objective function similar to the approximation of the inverse Hessian matrix in the Quasi-Newton method in classical optimization. In contrast to most classical methods, fewer assumptions on the nature of the underlying objective function are made. Only the ranking between candidate solutions is exploited for learning the sample distribution and neither derivatives nor even the function values themselves are required by the method. == Principles == Two main principles for the adaptation of parameters of the search distribution are exploited in the CMA-ES algorithm. First, a maximum-likelihood principle, based on the idea to increase the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated principal components analysis of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the Cross-Entropy Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful search steps. Second, two paths of the time evolution of the distribution mean of the strategy are recorded, called search or evolution paths. These paths contain significant information about the correlation between consecutive steps. Specifically, if consecutive steps are taken in a similar direction, the evolution paths become long. The evolution paths are exploited in two ways. One path is used for the covariance matrix adaptation procedure in place of single successful search steps and facilitates a possibly much faster variance increase of favorable directions. The other path is used to conduct an additional step-size control. This step-size control aims to make consecutive movements of the distribution mean orthogonal in expectation. The step-size control effectively prevents premature convergence yet allowing fast convergence to an optimum. == Algorithm == In the following the most commonly used (μ/μw, λ)-CMA-ES is outlined, where in each iteration step a weighted combination of the μ best out of λ new candidate solutions is used to update the distribution parameters. The main loop consists of three main parts: 1) sampling of new solutions, 2) re-ordering of the sampled solutions based on their fitness, 3) update of the internal state variables based on the re-ordered samples. A pseudocode of the algorithm looks as follows. set // number of samples per iteration, at least two, generally > 4 initialize , , , , // initialize state variables while not terminate // iterate for in // sample new solutions and evaluate them = sample_multivariate_normal(mean=, covariance_matrix=) = fitness() ← with = argsort(, ) // sort solutions = // we need later and ← update_m // move mean to better solutions ← update_ps // update isotropic evolution path ← update_pc // update anisotropic evolution path ← update_C // update covariance matrix ← update_sigma // update step-size using isotropic path length return or The order of the five update assignments is relevant. In the following, the update equations for the five state variables are specified. Given are the search space dimension and the iteration step . The five state variables are , the distribution mean and current favorite solution to the optimization problem, , the step-size, , a symmetric and positive definite covariance matrix with and , two evolution paths, initially set to the zero vector. The iteration starts with sampling candidate solutions from a multivariate normal distribution , i.e. for The second line suggests the interpretation as perturbation (mutation) of the current favorite solution vector (the distribution mean vector). The candidate solutions are evaluated on the objective function to be minimized. Denoting the -sorted candidate solutions as the new mean value is computed as where the positive (recombination) weights sum to one. Typically, and the weights are chosen such that . The only feedback used from the objective function here and in the following is an ordering of the sampled candidate solutions due to the indices . The step-size is updated using cumulative step-size adaptation (CSA), sometimes also denoted as path length control. The evolution path (or search path) is updated first. where is the backward time horizon for the evolution path and larger than one, is the variance effective selection mass and by definition of , is the unique symmetric square root of the inverse of , and is the damping parameter usually close to one. For or the step-size remains unchanged. The step-size is increased if and only if is larger than the expected value and decreased if it is smaller. For this reason, the step-size update tends to make consecutive steps -conjugate, in that after the adaptation has been successful . Finally, the covariance matrix is updated, where again the respective evolution path is updated first. where denotes the transpose and is the backward time horizon for the evolution path and larger than one, and the indicator function evaluates to one iff or, in other words, , which is usually the case, makes partly up for the small variance loss in case the indicator is zero, is the learning rate for the rank-one update of the covariance matrix and is the learning rate for the rank- update of the covariance matrix and must not exceed . The covariance matrix update tends to increase the likelihood for and for to be sampled from . This completes the iteration step. The number of candidate samples per iteration, , is not determined a priori and can vary in a wide range. Smaller values, for example , lead to more local search behavior. Larger values, for example with default value , render the search more global. Sometimes the algorithm is repeatedly restarted with increasing by a factor of two for each restart. Besides of setting (or possibly instead, if for example is predetermined by the number of available processors), the above introduced parameters are not specific to the given objective function and therefore not meant to be modified by the user. == Example code in MATLAB/Octave == == Theoretical Foundations == Given the distribution parameters—mean, variances and covariances—the normal probability distribution for sampling new candidate solutions is the maximum entropy probability distribution over , that is, the sample distribution with the minimal amount of prior information built into the distribution. More considerations on the update equations of CMA-ES are made in the following. === Variable Metric === The CMA-ES implements a stochastic variable-metric method. In the very particular case of a convex-quadratic objective function the covariance matrix adapts to the inverse of the Hessian matrix , up to a scalar factor and small random fluctuations. More general, also on the function , where is strictly increasing and therefore order preserving and is convex-quadratic, the covariance matrix adapts to , up to a scalar factor and small random fluctuations. === Maximum-Likelihood Updates === The update equations for mean and covariance matrix maximize a likelihood while resembling an expectation-maximization algorithm. The update of the mean vector maximizes a log-likelihood, such that where denotes the log-likelihood of from a multivariate normal distribution with mean and any positive definite covariance matrix . To see that is independent of remark first that this is the case for any diagonal matrix , because the coordinate-wise maximizer is independent of a scaling factor. Then, rotation of the data points or choosing non-diagonal are equivalent. The rank- update of the covariance matrix, that is, the right most summand in the update equation of , maximizes a log-likelihood in that for (otherwise is singular, but substantially the same result holds for ). Here, denotes the likelihood of from a multivariate normal distribution with zero mean and covariance matrix . Therefore, for and , is the above maximum-likelihood estimator. See estimation of covariance matrices for details on the derivation. === Natural Gradient Descent in the Space of Sample Distributions === Akimoto et al. and Glasmachers et al. discovered independently that the update of the distribution parameters resembles the descend in direction of a sampled natural gradient of the expected objective function value E f (x) (to be minimized), where the expectation is taken under the sample distribution. With the parameter setting of and , i.e. without step-size control and rank-one update, CMA-ES can thus be viewed as an instantiation of Natural Evolution Strategies (NES). The natural gradient is independent of the parameterization of the distribution. Taken with respect to the parameters θ of the sample distribution p, the gradient of E f (x) can be expressed as where depends on the parameter vector , the so-called score function, , indicates the relative sensitivity of p w.r.t. θ, and the expectation is taken with respect to the distribution p. The natural gradient of E f (x), complying with the Fisher information metric (an informational distance measure between probability distributions and the curvature of the relative entropy), now reads where the Fisher information matrix is the expectation of the Hessian of -lnp and renders the expression independent of the chosen parameterization. Combining the previous equalities we get A Monte Carlo approximation of the latter expectation takes the average over λ samples from p where the notation from above is used and therefore are monotonously decreasing in . Ollivier et al. finally found a rigorous formulation for the more robust weights, , as they are defined in the CMA-ES (weights are zero for i > μ), formulated as consistent estimator for the CDF of at the point 
145:228:The junction tree algorithm (also known as 'Clique Tree') is a method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The basic premise is to eliminate cycles by clustering them into single nodes.
133:502:Andrew Yan-Tak Ng (traditional Chinese: 吳恩達; simplified Chinese: 吴恩达; pinyin: Wú Ēndá; born 1976) is Chief Scientist at Baidu Research in Silicon Valley. In addition, he is an associate professor in the Department of Computer Science and the Department of Electrical Engineering by courtesy at Stanford University. He is chairman of the board of Coursera, an online education platform that he co-founded with Daphne Koller.
7:226:A.I. Artificial Intelligence - Music from the Motion Picture is the soundtrack of the 2001 film A.I. Artificial Intelligence. The original score was composed by John Williams and featured singers Lara Fabian on two songs and Josh Groban on one.
149:1487:An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation. It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following maximum expected utility criterion) can be modeled and solved.
132:567:Torch is an open source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep machine learning, and uses an extremely fast scripting language LuaJIT, and an underlying C implementation.
69:0:A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition. == Architectures == === Fully recurrent network === This is the basic architecture developed in the 1980s: a network of neuron-like units, each with a directed connection to every other unit. Each unit has a time-varying real-valued activation. Each connection has a modifiable real-valued weight. Some of the nodes are called input nodes, some output nodes, the rest hidden nodes. Most architectures below are special cases. For supervised learning in discrete time settings, training sequences of real-valued input vectors become sequences of activations of the input nodes, one input vector at a time. At any given time step, each non-input unit computes its current activation as a nonlinear function of the weighted sum of the activations of all units from which it receives connections. There may be teacher-given target activations for some of the output units at certain time steps. For example, if the input sequence is a speech signal corresponding to a spoken digit, the final target output at the end of the sequence may be a label classifying the digit. For each sequence, its error is the sum of the deviations of all target signals from the corresponding activations computed by the network. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences. Algorithms for minimizing this error are mentioned in the section on training algorithms below. In reinforcement learning settings, there is no teacher providing target signals for the RNN, instead a fitness function or reward function is occasionally used to evaluate the RNN's performance, which is influencing its input stream through output units connected to actuators affecting the environment. Again, compare the section on training algorithms below. === Recursive neural networks === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure, by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They were introduced to learn distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN itself whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Hopfield network === The Hopfield network is of historic interest although it is not a general RNN, as it is not designed to process sequences of patterns. Instead it requires stationary inputs. It is a RNN in which all connections are symmetric. Invented by John Hopfield in 1982, it guarantees that its dynamics will converge. If the connections are trained using Hebbian learning then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration. A variation on the Hopfield network is the bidirectional associative memory (BAM). The BAM has two layers, either of which can be driven as an input, to recall an association and produce an output on the other layer. === Elman networks and Jordan networks === The following special case of the basic architecture above was employed by Jeff Elman. A three-layer network is used (arranged vertically as x, y, and z in the illustration), with the addition of a set of "context units" (u in the illustration). There are connections from the middle (hidden) layer to these context units fixed with a weight of one. At each time step, the input is propagated in a standard feed-forward fashion, and then a learning rule is applied. The fixed back connections result in the context units always maintaining a copy of the previous values of the hidden units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks, due to Michael I. Jordan, are similar to Elman networks. The context units are however fed from the output layer instead of the hidden layer. The context units in a Jordan network are also referred to as the state layer, and have a recurrent connection to themselves with no other nodes on this connection. Elman and Jordan networks are also known as "simple recurrent networks" (SRN). === Echo state network === The echo state network (ESN) is a recurrent neural network with a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change and be trained. ESN are good at reproducing certain time series. A variant for spiking neurons is known as Liquid state machines. === Neural history compressor === The vanishing gradient problem of automatic differentiation or backpropagation in neural networks was partially overcome in 1992 by an early generative model called the neural history compressor, implemented as an unsupervised stack of recurrent neural networks (RNNs). The RNN at the input level learns to predict its next input from the previous input history. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN which therefore recomputes its internal state only rarely. Each higher level RNN thus learns a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the sequence representation at the highest level. The system effectively minimises the description length or the negative logarithm of the probability of the data. If there is a lot of learnable predictability in the incoming data sequence, then the highest level RNN can use supervised learning to easily classify even deep sequences with very long time intervals between important events. In 1993, such a system already solved a "Very Deep Learning" task that requires more than 1000 subsequent layers in an RNN unfolded in time. It is also possible to distill the entire RNN hierarchy into only two RNNs called the "conscious" chunker (higher level) and the "subconscious" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are still unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through special additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across very long time intervals. This in turn helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining still unpredictable events, to compress the data even further. === Long short term memory === Numerous researchers now use a deep learning RNN called the Long short term memory (LSTM) network, published by Hochreiter & Schmidhuber in 1997. It is a deep learning system that unlike traditional RNNs doesn't have the vanishing gradient problem (compare the section on training algorithms below). LSTM is normally augmented by recurrent gates called forget gates. LSTM RNNs prevent backpropagated errors from vanishing or exploding. Instead errors can flow backwards through unlimited numbers of virtual layers in LSTM RNNs unfolded in space. That is, LSTM can learn "Very Deep Learning" tasks that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved. LSTM works even when there are long delays, and it can handle signals that have a mix of low and high frequency components. Today, many applications use stacks of LSTM RNNs and train them by Connectionist Temporal Classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition. Around 2007, LSTM started to revolutionise speech recognition, outperforming traditional models in certain speech applications. In 2009, CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, the Chinese search giant Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without using any traditional speech processing methods. LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, also for Google Android, and photo-real talking heads. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users. LSTM has also become very popular in the field of Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages. LSTM improved machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with Convolutional Neural Networks (CNNs) also improved automatic image captioning and a plethora of other applications. === Bi-directional RNN === Invented by Schuster & Paliwal in 1997, bi-directional RNN or BRNN use a finite sequence to predict or label each element of the sequence based on both the past and the future context of the element. This is done by concatenating the outputs of two RNN, one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM RNN. === Continuous-time RNN === A continuous time recurrent neural network (CTRNN) is a dynamical systems model of biological neural networks. A CTRNN uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train. For a neuron in the network with action potential the rate of change of activation is given by: Where: : Time constant of postsynaptic node : Activation of postsynaptic node : Rate of change of activation of postsynaptic node : Weight of connection from pre to postsynaptic node : Sigmoid of x e.g. . : Activation of presynaptic node : Bias of presynaptic node : Input (if any) to node CTRNNs have frequently been applied in the field of evolutionary robotics, where they have been used to address, for example, vision, co-operation and minimally cognitive behaviour. Note that by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous time recurrent neural networks where the differential equation have transformed in an equivalent difference equation after that the postsynaptic node activation functions have been low-pass filtered prior to sampling. === Hierarchical RNN === There are many instances of hierarchical RNN whose elements are connected in various ways to decompose hierarchical 
9:0:In video games, artificial intelligence is used to generate intelligent behaviors primarily in non-player characters (NPCs), often simulating human-like intelligence. The techniques used typically draw upon existing methods from the field of artificial intelligence (AI). However, the term game AI is often used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general. Since game AI for NPCs is centered on appearance of intelligence and good gameplay within environment restrictions, its approach is very different from that of traditional AI; workarounds and cheats are acceptable and, in many cases, the computer abilities must be toned down to give human players a sense of fairness. This, for example, is true in first-person shooter games, where NPCs' otherwise perfect aiming would be beyond human skill. == History == Game playing was an area of research in AI from its inception. One of the first examples of AI is the computerised game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took the form of a relatively small box and was able to regularly win games even against highly skilled players of the game. In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. These were among the first computer programs ever written. Arthur Samuel's checkers program, developed in the middle 50s and early 60s, eventually achieved sufficient skill to challenge a respectable amateur. Work on checkers and chess would culminate in the defeat of Garry Kasparov by IBM's Deep Blue computer in 1997. The first video games developed in the 1960s and early 1970s, like Spacewar!, Pong, and Gotcha (1973), were games implemented on discrete logic and strictly based on the competition of two players, without AI. Games that featured a single player mode with enemies started appearing in the 1970s. The first notable ones for the arcade appeared in 1974: the Taito game Speed Race (racing video game) and the Atari games Qwak (duck hunting light gun shooter) and Pursuit (fighter aircraft dogfighting simulator). Two text-based computer games from 1972, Hunt the Wumpus and Star Trek, also had enemies. Enemy movement was based on stored patterns. The incorporation of microprocessors would allow more computation and random elements overlaid into movement patterns. It was during the golden age of video arcade games that the idea of AI opponents was largely popularized, due to the success of Space Invaders (1978), which sported an increasing difficulty level, distinct movement patterns, and in-game events dependent on hash functions based on the player's input. Galaxian (1979) added more complex and varied enemy movements, including maneuvers by individual enemies who break out of formation. Pac-Man (1980) introduced AI patterns to maze games, with the added quirk of different personalities for each enemy. Karate Champ (1984) later introduced AI patterns to fighting games, although the poor AI prompted the release of a second version. First Queen (1988) was a tactical action RPG which featured characters that can be controlled by the computer's AI in following the leader. The role-playing video game Dragon Quest IV (1990) introduced a "Tactics" system, where the user can adjust the AI routines of non-player characters during battle, a concept later introduced to the action role-playing game genre by Secret of Mana (1993). Games like Madden Football, Earl Weaver Baseball and Tony La Russa Baseball all based their AI on an attempt to duplicate on the computer the coaching or managerial style of the selected celebrity. Madden, Weaver and La Russa all did extensive work with these game development teams to maximize the accuracy of the games. Later sports titles allowed users to "tune" variables in the AI to produce a player-defined managerial or coaching strategy. The emergence of new game genres in the 1990s prompted the use of formal AI tools like finite state machines. Real-time strategy games taxed the AI with many objects, incomplete information, pathfinding problems, real-time decisions and economic planning, among other things. The first games of the genre had notorious problems. Herzog Zwei (1989), for example, had almost broken pathfinding and very basic three-state state machines for unit control, and Dune II (1992) attacked the players' base in a beeline and used numerous cheats. Later games in the genre exhibited more sophisticated AI. Later games have used bottom-up AI methods, such as the emergent behaviour and evaluation of player actions in games like Creatures or Black & White. Façade (interactive story) was released in 2005 and used interactive multiple way dialogs and AI as the main aspect of game. Games have provided an environment for developing artificial intelligence with potential applications beyond gameplay. Examples include Watson, a Jeopardy!-playing computer; and the RoboCup tournament, where robots are trained to compete in soccer. == Views == Purists complain that the "AI" in the term "game AI" overstates its worth, as game AI is not about intelligence, and shares few of the objectives of the academic field of AI. Whereas "real" AI addresses fields of machine learning, decision making based on arbitrary data input, and even the ultimate goal of strong AI that can reason, "game AI" often consists of a half-dozen rules of thumb, or heuristics, that are just enough to give a good gameplay experience. Historically, academic game-AI projects have been relatively separate from commercial products because the academic approaches tended to be simple and non-scalable. Commercial game AI has developed its own set of tools, which have been sufficient to give good performance in many cases. Game developers' increasing awareness of academic AI and a growing interest in computer games by the academic community is causing the definition of what counts as AI in a game to become less idiosyncratic. Nevertheless, significant differences between different application domains of AI mean that game AI can still be viewed as a distinct subfield of AI. In particular, the ability to legitimately solve some AI problems in games by cheating creates an important distinction. For example, inferring the position of an unseen object from past observations can be a difficult problem when AI is applied to robotics, but in a computer game a NPC can simply look up the position in the game's scene graph. Such cheating can lead to unrealistic behavior and so is not always desirable. But its possibility serves to distinguish game AI and leads to new problems to solve, such as when and how to use cheating. The major limitation to strong AI is the inherent depth of thinking and the extreme complexity of the decision making process. This means that although it would be then theoretically possible to make "smart" AI the problem would take considerable processing power. == Usage == === In computer simulations of board games === Computer chess Computer Go Computer checkers Computer poker players Computer bridge Computer shogi Computer Arimaa Logistello, which plays Reversi Rog-O-Matic, which plays Rogue Computer players of Scrabble A variety of board games in the Computer Olympiad General game playing Solved games have a computer strategy which is guaranteed to be optimal, and in some cases force a win or draw. === In modern video games === Game AI/heuristic algorithms are used in a wide variety of quite disparate fields inside a game. The most obvious is in the control of any NPCs in the game, although scripting is currently the most common means of control. Pathfinding is another common use for AI, widely seen in real-time strategy games. Pathfinding is the method for determining how to get a NPC from one point on a map to another, taking into consideration the terrain, obstacles and possibly "fog of war". Beyond pathfinding, navigation is a sub-field of Game AI focusing on giving NPCs the capability to navigate in their environment, finding a path to a target while avoiding collisions with other entities (other NPC, players...) or collaborating with them (group navigation). The concept of emergent AI has recently been explored in games such as Creatures, Black & White and Nintendogs and toys such as Tamagotchi. The "pets" in these games are able to "learn" from actions taken by the player and their behavior is modified accordingly. While these choices are taken from a limited pool, it does often give the desired illusion of an intelligence on the other side of the screen. === Video game combat AI === The many contemporary video games fall under the category of action, first person shooter, or adventure. In most of these types of games there is some level of combat that takes place. The AI's ability to be efficient in combat is important in these genres. A common goal today is to make the AI more human, or at least appear so. One of the more positive and efficient features found in modern-day video game AI is the ability to hunt. AI originally reacted in a very black and white manner. If the player were in a specific area then the AI would react in either a complete offensive manner or be entirely defensive. In recent years, the idea of "hunting" has been introduced; in this 'hunting' state the AI will look for realistic markers, such as sounds made by the character or footprints they may have left behind. These developments ultimately allow for a more complex form of play. With this feature, the player can actually consider how to approach or avoid an enemy. This is a feature that is particularly prevalent in the stealth genre. Another development in recent game AI has been the development of "survival instinct". In-game computers can recognize different objects in an environment and determine whether it is beneficial or detrimental to its survival. Like a user, the AI can "look" for cover in a firefight before taking actions that would leave it otherwise vulnerable, such as reloading a weapon or throwing a grenade. There can be set markers that tell it when to react in a certain way. For example, if the AI is given a command to check its health throughout a game then further commands can be set so that it reacts a specific way at a certain percentage of health. If the health is below a certain threshold then the AI can be set to run away from the player and avoid it until another function is triggered. Another example could be if the AI notices it is out of bullets, it will find a cover object and hide behind it until it has reloaded. Actions like these make the AI seem more human. However, there is still a need for improvement in this area. Unlike a human player the AI must be programmed for all the possible 
127:225:The following table compares some of the most popular software frameworks, libraries and computer programs for deep learning.
180:273:Stephen H. Muggleton FBCS, FIET, FAAAI,FECCAI, FSB, FREng (born 6 December 1959, son of Louis Muggleton) is Professor of Machine Learning and Head of the Computational Bioinformatics Laboratory at Imperial College London.
192:0:Bionics is the application of biological methods and systems found in nature to the study and design of engineering systems and modern technology. The word bionic was coined by Jack E. Steele in 1958, possibly originating from the technical term bion (pronounced BEE-on; from Ancient Greek: βίος), meaning 'unit of life' and the suffix -ic, meaning 'like' or 'in the manner of', hence 'like life'. Some dictionaries, however, explain the word as being formed as a portmanteau from biology and electronics. It was popularized by the 1970s U.S. television series The Six Million Dollar Man and The Bionic Woman, both based upon the novel Cyborg by Martin Caidin, which was itself influenced by Steele's work. All feature humans given superhuman powers by electromechanical implants. The transfer of technology between lifeforms and manufactures is, according to proponents of bionic technology, desirable because evolutionary pressure typically forces living organisms, including fauna and flora, to become highly optimized and efficient. A classical example is the development of dirt- and water-repellent paint (coating) from the observation that the surface of the lotus flower plant is practically unsticky for anything (the lotus effect).. The term "biomimetic" is preferred when reference is made to chemical reactions. In that domain, biomimetic chemistry refers to reactions that, in nature, involve biological macromolecules (for example, enzymes or nucleic acids) whose chemistry can be replicated using much smaller molecules in vitro. Examples of bionics in engineering include the hulls of boats imitating the thick skin of dolphins; sonar, radar, and medical ultrasound imaging imitating the echolocation of bats. In the field of computer science, the study of bionics has produced artificial neurons, artificial neural networks, and swarm intelligence. Evolutionary computation was also motivated by bionics ideas but it took the idea further by simulating evolution in silico and producing well-optimized solutions that had never appeared in nature. It is estimated by Julian Vincent, professor of biomimetics at the University of Bath's department of mechanical engineering Biomimetics group, that "at present there is only a 12% overlap between biology and technology in terms of the mechanisms used". == History == The name biomimetics was coined by Otto Schmitt in the 1950s. The term bionics was coined by Jack E. Steele in 1958 while working at the Aeronautics Division House at Wright-Patterson Air Force Base in Dayton, Ohio. However, terms like biomimicry or biomimetics are more preferred in the technology world in efforts to avoid confusion between the medical term bionics. Coincidentally, Martin Caidin used the word for his 1972 novel Cyborg, which inspired the series The Six Million Dollar Man. Caidin was a long-time aviation industry writer before turning to fiction full-time. == Methods == Often, the study of bionics emphasizes implementing a function found in nature rather than just imitating biological structures. For example, in computer science, cybernetics tries to model the feedback and control mechanisms that are inherent in intelligent behavior, while artificial intelligence tries to model the intelligent function regardless of the particular way it can be achieved. The conscious copying of examples and mechanisms from natural organisms and ecologies is a form of applied case-based reasoning, treating nature itself as a database of solutions that already work. Proponents argue that the selective pressure placed on all natural life forms minimizes and removes failures. Although almost all engineering could be said to be a form of biomimicry, the modern origins of this field are usually attributed to Buckminster Fuller and its later codification as a house or field of study to Janine Benyus. Roughly, we can distinguish three biological levels in the fauna or flora, after which technology can be modeled: Mimicking natural methods of manufacture Imitating mechanisms found in nature (velcro) Studying organizational principles from the social behaviour of organisms, such as the flocking behaviour of birds, optimization of ant foraging and bee foraging, and the swarm intelligence (SI)-based behaviour of a school of fish. == Examples == In robotics, bionics and biomimetics are used to apply the way animals move to the design of robots. BionicKangaroo was based on the movements and physiology of kangaroos. Velcro is the most famous example of biomimetics. In 1948, the Swiss engineer George de Mestral was cleaning his dog of burrs picked up on a walk when he realized how the hooks of the burrs clung to the fur. The horn-shaped, saw-tooth design for lumberjack blades used at the turn of the 19th century to cut down trees when it was still done by hand was modeled after observations of a wood-burrowing beetle. It revolutionized the industry because the blades worked so much faster at felling trees. Cat's eye reflectors were invented by Percy Shaw in 1935 after studying the mechanism of cat eyes. He had found that cats had a system of reflecting cells, known as tapetum lucidum, which was capable of reflecting the tiniest bit of light. Leonardo da Vinci's flying machines and ships are early examples of drawing from nature in engineering. Resilin is a replacement for rubber that has been created by studying the material also found in arthropods. Julian Vincent drew from the study of pinecones when he developed in 2004 "smart" clothing that adapts to changing temperatures. "I wanted a nonliving system which would respond to changes in moisture by changing shape", he said. "There are several such systems in plants, but most are very small — the pinecone is the largest and therefore the easiest to work on". Pinecones respond to higher humidity by opening their scales (to disperse their seeds). The "smart" fabric does the same thing, opening up when the wearer is warm and sweating, and shutting tight when cold. "Morphing aircraft wings" that change shape according to the speed and duration of flight were designed in 2004 by biomimetic scientists from Penn State University. The morphing wings were inspired by different bird species that have differently shaped wings according to the speed at which they fly. In order to change the shape and underlying structure of the aircraft wings, the researchers needed to make the overlying skin also be able to change, which their design does by covering the wings with fish-inspired scales that could slide over each other. In some respects this is a refinement of the swing-wing design. Some paints and roof tiles have been engineered to be self-cleaning by copying the mechanism from the Nelumbo lotus. Cholesteric liquid crystals (CLCs) are the thin-film material often used to fabricate fish tank thermometers or mood rings, that change color with temperature changes. They change color because their molecules are arranged in a helical or chiral arrangement and with temperature the pitch of that helical structure changes, reflecting different wavelengths of light. Chiral Photonics, Inc. has abstracted the self-assembled structure of the organic CLCs to produce analogous optical devices using tiny lengths of inorganic, twisted glass fiber. Nanostructures and physical mechanisms that produce the shining color of butterfly wings were reproduced in silico by Greg Parker, professor of Electronics and Computer Science at the University of Southampton and research student Luca Plattner in the field of photonics, which is electronics using photons as the information carrier instead of electrons. The wing structure of the blue morpho butterfly was studied and the way it reflects light was mimicked to create an RFID tag that can be read through water and on metal. The wing structure of butterflies has also inspired the creation of new nanosensors to detect explosives. Neuromorphic chips, silicon retinae or cochleae, has wiring that is modelled after real neural networks. S.a.: connectivity. Technoecosystems or 'EcoCyborg' systems involve the coupling of natural ecological processes to technological ones which mimic ecological functions. This results in the creation of a self-regulating hybrid system. Research into this field was initiated by Howard T. Odum, who perceived the structure and emergy dynamics of ecosystems as being analogous to energy flow between components of an electrical circuit. Medical adhesives involving glue and tiny nano-hairs are being developed based on the physical structures found in the feet of geckos. Computer viruses also show similarities with biological viruses in their way to curb program-oriented information towards self-reproduction and dissemination. The cooling system of the Eastgate Centre building, in Harare was modeled after a termite mound to achieve very efficient passive cooling. Adhesive which allows mussels to stick to rocks, piers and boat hulls inspired bioadhesive gel for blood vessels. Through the field of bionics, new aircraft designs with far greater agility and other advantages may be created. This has been described by Geoff Spedding and Anders Hedenström in an article in Journal of Experimental Biology. Similar statements were also made by John Videler and Eize Stamhuis in their book Avian Flight and in the article they present in Science about LEVs. John Videler and Eize Stamhuis have since worked out real-life improvements to airplane wings, using bionics research. This research in bionics may also be used to create more efficient helicopters or miniature UAVs. This latter was stated by Bret Tobalske in an article in Science about Hummingbirds. Bret Tobalske has thus now started work on creating these miniature UAVs which may be used for espionage. UC Berkeley as well as ESA have finally also been working in a similar direction and created the Robofly (a miniature UAV) and the Entomopter (a UAV which can walk, crawl and fly). == Specific uses of the term == === In medicine === Bionics is a term which refers to the flow of concepts from biology to engineering and vice versa. Hence, there are two slightly different points of view regarding the meaning of the word. In medicine, bionics means the replacement or enhancement of organs or other body parts by mechanical versions. Bionic implants differ from mere prostheses by mimicking the original function very closely, or even surpassing it. Bionics' German equivalent, Bionik, always adheres to the broader meaning, in that it tries to develop engineering solutions from biological models. This approach is motivated by the fact that biological solutions will usually be optimized by evolutionary forces. While the technologies that make bionic implants possible are still in a very early stage, a few bionic items already exist, the best known being the cochlear implant, a device for deaf people. By 2004 fully functional artificial hearts were developed. Significant further progress is expected to take place with the advent of nanotechnologies. A well-known example of a proposed nanodevice is a respirocyte, an artificial red cell, designed (though not built yet) by Robert Freitas. Kwabena Boahen from Ghana was a professor in the Department of Bioengineering at the University of Pennsylvania. During his eight years at Penn, he developed a silicon retina that was able to process images in the same manner as a living retina. He confirmed the results by comparing the electrical signals from his silicon retina 
186:0:In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. This algorithm is a member of the ant colony algorithms family, in swarm intelligence methods, and it constitutes some metaheuristic optimizations. Initially proposed by Marco Dorigo in 1992 in his PhD thesis, the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of ants seeking a path between their colony and a source of food. The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants. == OverviewEdit == In the natural world, ants (initially) wander randomly, and upon finding food return to their colony while laying down pheromone trails. If other ants find such a path, they are likely not to keep travelling at random, but instead to follow the trail, returning and reinforcing it if they eventually find food (see Ant communication). Over time, however, the pheromone trail starts to evaporate, thus reducing its attractive strength. The more time it takes for an ant to travel down the path and back again, the more time the pheromones have to evaporate. A short path, by comparison, gets marched over more frequently, and thus the pheromone density becomes higher on shorter paths than longer ones. Pheromone evaporation also has the advantage of avoiding the convergence to a locally optimal solution. If there were no evaporation at all, the paths chosen by the first ants would tend to be excessively attractive to the following ones. In that case, the exploration of the solution space would be constrained. Thus, when one ant finds a good (i.e., short) path from the colony to a food source, other ants are more likely to follow that path, and positive feedback eventually leads to all the ants following a single path. The idea of the ant colony algorithm is to mimic this behavior with "simulated ants" walking around the graph representing the problem to solve. == Common extensionsEdit == Here are some of the most popular variations of ACO algorithms. === Elitist ant systemEdit === The global best solution deposits pheromone on every iteration along with all the other ants. === Max-min ant system (MMAS)Edit === Added maximum and minimum pheromone amounts [τmax,τmin]. Only global best or iteration best tour deposited pheromone <MAZ>. All edges are initialized to τmax and reinitialized to τmax when nearing stagnation. === Ant colony systemEdit === It has been presented above. === Rank-based ant system (ASrank)Edit === All solutions are ranked according to their length. The amount of pheromone deposited is then weighted for each solution, such that solutions with shorter paths deposit more pheromone than the solutions with longer paths. === Continuous orthogonal ant colony (COAC)Edit === The pheromone deposit mechanism of COAC is to enable ants to search for solutions collaboratively and effectively. By using an orthogonal design method, ants in the feasible domain can explore their chosen regions rapidly and efficiently, with enhanced global search capability and accuracy. The orthogonal design method and the adaptive radius adjustment method can also be extended to other optimization algorithms for delivering wider advantages in solving practical problems. === Recursive ant colony optimizationEdit === It is a recursive form of ant system which divides the whole search domain into several sub-domains and solves the objective on these subdomains. The results from all the subdomains are compared and the best few of them are promoted for the next level. The subdomains corresponding to the selected results are further subdivided and the process is repeated until an output of desired precision is obtained. This method has been tested on ill-posed geophysical inversion problems and works well. == ConvergenceEdit == For some versions of the algorithm, it is possible to prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of a convergence ant colony algorithm was made in 2000, the graph-based ant system algorithm, and then algorithms for ACS and MMAS. Like most metaheuristics, it is very difficult to estimate the theoretical speed of convergence. In 2004, Zlochin and his colleagues showed that COA-type algorithms could be assimilated methods of stochastic gradient descent, on the cross-entropy and estimation of distribution algorithm. They proposed these metaheuristics as a "research-based model". A performance analysis of continuous ant colony algorithm based on its various parameter suggest its sensitivity of convergence on parameter tuning. == Example pseudo-code and formulaEdit == === Edge selectionEdit === An ant is a simple computational agent in the ant colony optimization algorithm. It iteratively constructs a solution for the problem at hand. The intermediate solutions are referred to as solution states. At each iteration of the algorithm, each ant moves from a state to state , corresponding to a more complete intermediate solution. Thus, each ant computes a set of feasible expansions to its current state in each iteration, and moves to one of these in probability. For ant , the probability of moving from state to state depends on the combination of two values, viz., the attractiveness of the move, as computed by some heuristic indicating the a priori desirability of that move and the trail level of the move, indicating how proficient it has been in the past to make that particular move. The trail level represents a posteriori indication of the desirability of that move. Trails are updated usually when all ants have completed their solution, increasing or decreasing the level of trails corresponding to moves that were part of "good" or "bad" solutions, respectively. In general, the th ant moves from state to state with probability where is the amount of pheromone deposited for transition from state to , 0 ≤ is a parameter to control the influence of , is the desirability of state transition (a priori knowledge, typically , where is the distance) and ≥ 1 is a parameter to control the influence of . and represent the attractiveness and trail level for the other possible state transitions. === Pheromone updateEdit === When all the ants have completed a solution, the trails are updated by where is the amount of pheromone deposited for a state transition , is the pheromone evaporation coefficient and is the amount of pheromone deposited by th ant, typically given for a TSP problem (with moves corresponding to arcs of the graph) by where is the cost of the th ant's tour (typically length) and is a constant. == ApplicationsEdit == Ant colony optimization algorithms have been applied to many combinatorial optimization problems, ranging from quadratic assignment to protein folding or routing vehicles and a lot of derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and parallel implementations. It has also been used to produce near-optimal solutions to the travelling salesman problem. They have an advantage over simulated annealing and genetic algorithm approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time. This is of interest in network routing and urban transportation systems. The first ACO algorithm was called the ant system and it was aimed to solve the travelling salesman problem, in which the goal is to find the shortest round-trip to link a series of cities. The general algorithm is relatively simple and based on a set of ants, each making one of the possible round-trips along the cities. At each stage, the ant chooses to move from one city to another according to some rules: It must visit each city exactly once; A distant city has less chance of being chosen (the visibility); The more intense the pheromone trail laid out on an edge between two cities, the greater the probability that that edge will be chosen; Having completed its journey, the ant deposits more pheromones on all edges it traversed, if the journey is short; After each iteration, trails of pheromones evaporate. === Scheduling problemEdit === Job-shop scheduling problem (JSP) Open-shop scheduling problem (OSP) Permutation flow shop problem (PFSP) Single machine total tardiness problem (SMTTP) Single machine total weighted tardiness problem (SMTWTP) Resource-constrained project scheduling problem (RCPSP) Group-shop scheduling problem (GSP) Single-machine total tardiness problem with sequence dependent setup times (SMTTPDST) Multistage flowshop scheduling problem (MFSP) with sequence dependent setup/changeover times === Vehicle routing problemEdit === Capacitated vehicle routing problem (CVRP) Multi-depot vehicle routing problem (MDVRP) Period vehicle routing problem (PVRP) Split delivery vehicle routing problem (SDVRP) Stochastic vehicle routing problem (SVRP) Vehicle routing problem with pick-up and delivery (VRPPD) Vehicle routing problem with time windows (VRPTW) Time dependent vehicle routing problem with time windows (TDVRPTW) Vehicle routing problem with time windows and multiple service workers (VRPTWMS) === Assignment problemEdit === Quadratic assignment problem (QAP) Generalized assignment problem (GAP) Frequency assignment problem (FAP) Redundancy allocation problem (RAP) === Set problemEdit === Set cover problem (SCP) Partition problem (SPP) Weight constrained graph tree partition problem (WCGTPP) Arc-weighted l-cardinality tree problem (AWlCTP) Multiple knapsack problem (MKP) Maximum independent set problem (MIS) === Device sizing problem in nanoelectronics physical designEdit === Ant colony optimization (ACO) based optimization of 45 nm CMOS-based sense amplifier circuit could converge to optimal solutions in very minimal time. Ant colony optimization (ACO) based reversible circuit synthesis could improve efficiency significantly. === Image processingEdit === ACO algorithm is used in image processing for image edge detection and edge linking. Edge detection: The graph here is the 2-D image and the ants traverse from one pixel depositing pheromone.The movement of ants from one pixel to another is directed by the local variation of the image’s intensity values. This movement causes the highest density of the pheromone to be deposited at the edges. The following are the steps involved in edge detection using ACO: Step1: Initialization:Randomly place ants on the image where . Pheromone matrix are initialized with a random value. The major challenge in the initialization process is determining the heuristic matrix. There are various methods to determine the heuristic matrix. For the below example the heuristic matrix was calculated based on the local statistics: the local statistics at the pixel position (i,j). Where is the image of size ,which is a normalization factor can be calculated using the following functions: The parameter in each of above functions adjusts the functions’ respective shapes.Step 2 Construction process:The ant’s movement is based on 4-connected pixels or 8-connected pixels. The probability with which the ant moves is given by the probability equation Step 3 and Step 5 Update process:The pheromone matrix is updated twice. in step 3 the trail of the ant (given by ) is updated where as in step 5 the 
39:1026:A feedforward neural network is an artificial neural network where connections between the units do not form a cycle. This is different from recurrent neural networks.
135:281:AlchemyAPI is a company that uses machine learning (specifically, deep learning) to do natural language processing (specifically, semantic text analysis, including sentiment analysis) and computer vision (specifically, face detection and recognition) for its clients both over the cloud and on-premises. As of February 2014, it claims to have clients in 36 countries and process over 3 billion documents a month. ProgrammableWeb added AlchemyAPI to its API Billionaires Club in September 2011.
30:462:Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning.
183:0:Swarm behaviour, or swarming, is a collective behaviour exhibited by entities, particularly animals, of similar size which aggregate together, perhaps milling about the same spot or perhaps moving en masse or migrating in some direction. It is highly interdisciplinary topic. As a term, swarming is applied particularly to insects, but can also be applied to any other entity or animal that exhibits swarm behaviour. The term flocking is usually used to refer specifically to swarm behaviour in birds, herding to refer to swarm behaviour in quadrupeds, shoaling or schooling to refer to swarm behaviour in fish. Phytoplankton also gather in huge swarms called blooms, although these organisms are algae and are not self-propelled the way animals are. By extension, the term swarm is applied also to inanimate entities which exhibit parallel behaviours, as in a robot swarm, an earthquake swarm, or a swarm of stars. From a more abstract point of view, swarm behaviour is the collective motion of a large number of self-propelled entities. From the perspective of the mathematical modeller, it is an emergent behaviour arising from simple rules that are followed by individuals and does not involve any central coordination. Swarm behaviour is also studied by active matter physicists as a phenomenon which is not in thermodynamic equilibrium, and as such requires the development of tools beyond those available from the statistical physics of systems in thermodynamic equilibrium. Swarm behaviour was first simulated on a computer in 1986 with the simulation program boids. This program simulates simple agents (boids) that are allowed to move according to a set of basic rules. The model was originally designed to mimic the flocking behaviour of birds, but it can be applied also to schooling fish and other swarming entities. == Models == In recent decades, scientists have turned to modeling swarm behaviour to gain a deeper understanding of the behaviour. === Mathematical models === Early studies of swarm behaviour employed mathematical models to simulate and understand the behaviour. The simplest mathematical models of animal swarms generally represent individual animals as following three rules: Move in the same direction as your neighbours Remain close to your neighbours Avoid collisions with your neighbours The boids computer program, created by Craig Reynolds in 1986, simulates swarm behaviour following the above rules. Many subsequent and current models use variations on these rules, often implementing them by means of concentric "zones" around each animal. In the zone of repulsion, very close to the animal, the focal animal will seek to distance itself from its neighbours to avoid collision. Slightly further away, in the zone of alignment, the focal animal will seek to align its direction of motion with its neighbours. In the outermost zone of attraction, which extends as far away from the focal animal as it is able to sense, the focal animal will seek to move towards a neighbour. The shape of these zones will necessarily be affected by the sensory capabilities of the given animal. For example, the visual field of a bird does not extend behind its body. Fish rely on both vision and on hydrodynamic perceptions relayed through their lateral line, while Antarctic krill rely both on vision and hydrodynamic signals relayed through antennae. However recent studies of starling flocks have shown that each bird modifies its position, relative to the six or seven animals directly surrounding it, no matter how close or how far away those animals are. Interactions between flocking starlings are thus based on a topological rule rather than a metric rule. It remains to be seen whether this applies to other animals. Another recent study, based on an analysis of high speed camera footage of flocks above Rome and assuming minimal behavioural rules, has convincingly simulated a number of aspects of flock behaviour. === Evolutionary models === In order to gain insight into why animals evolve swarming behaviour, scientists have turned to evolutionary models that simulate populations of evolving animals. Typically these studies use a genetic algorithm to simulate evolution over many generations in the model. These studies have investigated a number of hypotheses explaining why animals evolve swarming behaviour, such as the selfish herd theory the predator confusion effect, the dilution effect, and the many eyes theory. === Agents === Mach, Robert; Schweitzer, Frank (2003). "Multi-Agent Model of Biological Swarming". Advances In Artificial Life. Lecture Notes in Computer Science 2801. pp. 810–820. doi:10.1007/978-3-540-39432-7_87. ISBN 978-3-540-20057-4. CiteSeerX: 10.1.1.87.8022. === Self-organization === === Emergence === The concept of emergence—that the properties and functions found at a hierarchical level are not present and are irrelevant at the lower levels–is often a basic principle behind self-organizing systems. An example of self-organization in biology leading to emergence in the natural world occurs in ant colonies. The queen does not give direct orders and does not tell the ants what to do. Instead, each ant reacts to stimuli in the form of chemical scent from larvae, other ants, intruders, food and buildup of waste, and leaves behind a chemical trail, which, in turn, provides a stimulus to other ants. Here each ant is an autonomous unit that reacts depending only on its local environment and the genetically encoded rules for its variety of ant. Despite the lack of centralized decision making, ant colonies exhibit complex behaviour and have even been able to demonstrate the ability to solve geometric problems. For example, colonies routinely find the maximum distance from all colony entrances to dispose of dead bodies. === Stigmergy === A further key concept in the field of swarm intelligence is stigmergy. Stigmergy is a mechanism of indirect coordination between agents or actions. The principle is that the trace left in the environment by an action stimulates the performance of a next action, by the same or a different agent. In that way, subsequent actions tend to reinforce and build on each other, leading to the spontaneous emergence of coherent, apparently systematic activity. Stigmergy is a form of self-organization. It produces complex, seemingly intelligent structures, without need for any planning, control, or even direct communication between the agents. As such it supports efficient collaboration between extremely simple agents, who lack any memory, intelligence or even awareness of each other. === Swarm intelligence === Swarm intelligence is the collective behaviour of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems. Swarm intelligence systems are typically made up of a population of simple agents such as boids interacting locally with one another and with their environment. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of intelligent global behaviour, unknown to the individual agents. Swarm intelligence research is multidisciplinary. It can be divided into natural swarm research studying biological systems and artificial swarm research studying human artefacts. There is also a scientific stream attempting to model the swarm systems themselves and understand their underlying mechanisms, and an engineering stream focused on applying the insights developed by the scientific stream to solve practical problems in other areas. === Algorithms === Swarm algorithms follow a Lagrangian approach or an Eulerian approach. The Eulerian approach views the swarm as a field, working with the density of the swarm and deriving mean field properties. It is a hydrodynamic approach, and can be useful for modelling the overall dynamics of large swarms. However, most models work with the Lagrangian approach, which is an agent-based model following the individual agents (points or particles) that make up the swarm. Individual particle models can follow information on heading and spacing that is lost in the Eulerian approach. ==== Ant colony optimization ==== Ant colony optimization is a widely used algorithm which was inspired by the behaviours of ants, and has been effective solving discrete optimization problems related to swarming. The algorithm was initially proposed by Marco Dorigo in 1992, and has since been diversified to solve a wider class of numerical problems. Species that have multiple queens may have a queen leaving the nest along with some workers to found a colony at a new site, a process akin to swarming in honeybees. Ants are behaviourally unsophisticated; collectively they perform complex tasks. Ants have highly developed sophisticated sign-based communication. Ants communicate using pheromones; trails are laid that can be followed by other ants. Routing problem ants drop different pheromones used to compute the "shortest" path from source to destination(s). Rauch EM, Millonas MM and Chialvo DR (1995) "Pattern formation and functionality in swarm models" Physics Letters A, 207: 185. arXiv:adap-org/9507003 ==== Self-propelled particles ==== Self-propelled particles (SPP) is a concept introduced in 1995 by Vicsek et al. as a special case of the boids model introduced in 1986 by Reynolds. A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood. Simulations demonstrate that a suitable "nearest neighbour rule" eventually results in all the particles swarming together, or moving in the same direction. This emerges, even though there is no centralized coordination, and even though the neighbours for each particle constantly change over time (see the interactive simulation in the box on the right). SPP models predict that swarming animals share certain properties at the group level, regardless of the type of animals in the swarm. Swarming systems give rise to emergent behaviours which occur at many different scales, some of which are turning out to be both universal and robust. It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours. ==== Particle swarm optimization ==== Particle swarm optimization is another algorithm widely used to solve problems related to swarms. It was developed in 1995 by Kennedy and Eberhart and was first aimed at simulating the social behaviour and choreography of bird flocks and fish schools. The algorithm was simplified and it was observed to be performing optimization. The system initially seeds a population with random solutions. It then searches in the problem space through successive generations using stochastic optimization to find the best solutions. The solutions it finds are called particles. Each particle stores its position as well as the best solution it has achieved so far. The particle swarm optimizer tracks the best local value obtained so far by any particle in the local neighbourhood. The remaining particles then move through the problem space following the lead of the optimum particles. At each time iteration, the particle swarm optimiser accelerates each particle toward its optimum locations according to simple mathematical rules. 
137:1119:AMAX is a privately held company based in Fremont, California in the United States that specializes in application-tailored Cloud, Data Center, HPC and OEM Solutions. From white box server-to-rack integration, High Performance Deep Learning Platforms or Converged Infrastructure solutions featuring OpenStack, Open Compute and SDN, to a comprehensive menu of professional services, AMAX helps modernize IT operations for corporate, scientific, military, and government use. AMAX introduced its Deep Learning product line in 2015 featuring both workstations and high-performance servers. AMAX's diverse product lines, including the award-winning CloudMax Converged Cloud solution, are upgraded with the powerful Intel Xeon Processor E5-2600 V4 product family.
178:0:Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems. SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of "intelligent" global behavior, unknown to the individual agents. Examples in natural systems of SI include ant colonies, bird flocking, animal herding, bacterial growth, fish schooling and microbial intelligence. The application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms. 'Swarm prediction' has been used in the context of forecasting problems. == Example algorithms == === Particle swarm optimization === Particle swarm optimization (PSO) is a global optimization algorithm for dealing with problems in which a best solution can be represented as a point or surface in an n-dimensional space. Hypotheses are plotted in this space and seeded with an initial velocity, as well as a communication channel between the particles. Particles then move through the solution space, and are evaluated according to some fitness criterion after each timestep. Over time, particles are accelerated towards those particles within their communication grouping which have better fitness values. The main advantage of such an approach over other global minimization strategies such as simulated annealing is that the large number of members that make up the particle swarm make the technique impressively resilient to the problem of local minima. === Ant colony optimization === Ant colony optimization (ACO), introduced by Dorigo in his doctoral dissertation, is a class of optimization algorithms modeled on the actions of an ant colony. ACO is a probabilistic technique useful in problems that deal with finding better paths through graphs. Artificial 'ants'—simulation agents—locate optimal solutions by moving through a parameter space representing all possible solutions. Natural ants lay down pheromones directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions. === Artificial bee colony algorithm === Artificial bee colony algorithm (ABC) is a meta-heuristic algorithm introduced by Karaboga in 2005, and simulates the foraging behaviour of honey bees. The ABC algorithm has three phases: employed bee, onlooker bee and scout bee. In the employed bee and the onlooker bee phases, bees exploit the sources by local searches in the neighbourhood of the solutions selected based on deterministic selection in the employed bee phase and the probabilistic selection in the onlooker bee phase. In the scout bee phase which is an analogy of abandoning exhausted food sources in the foraging process, solutions that are not beneficial anymore for search progress are abandoned, and new solutions are inserted instead of them to explore new regions in the search space. The algorithm has a well-balanced exploration and exploitation ability. === Differential evolution === Differential evolution is similar to genetic algorithm and pattern search. It uses multiagents or search vectors to carry out search. It has mutation and crossover, but does not have the global best solution in its search equations, in contrast with the particle swarm optimization. === The bees algorithm === The bees algorithm in its basic formulation was created by Pham and his co-workers in 2005, and further refined in the following years. Modelled on the foraging behaviour of honey bees, the algorithm combines global explorative search with local exploitative search. A small number of artificial bees (scouts) explores randomly the solution space (environment) for solutions of high fitness (highly profitable food sources), whilst the bulk of the population search (harvest) the neighbourhood of the fittest solutions looking for the fitness optimum. A deterministics recruitment procedure which simulates the waggle dance of biological bees is used to communicate the scouts' findings to the foragers, and distribute the foragers depending on the fitness of the neighbourhoods selected for local search. Once the search in the neighbourhood of a solution stagnates, the local fitness optimum is considered to be found, and the site is abandoned. In summary, the Bees Algorithm searches concurrently the most promising regions of the solution space, whilst continuously sampling it in search of new favourable regions. === Artificial immune systems === Artificial immune systems (AIS) concerns the usage of abstract structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. AIS is a sub-field of Biologically inspired computing, and natural computation, with interests in Machine Learning and belonging to the broader field of Artificial Intelligence. === Bat algorithm === Bat algorithm (BA) is a swarm-intelligence-based algorithm, inspired by the echolocation behavior of microbats. BA automatically balances exploration (long-range jumps around the global search space to avoid getting stuck around one local maxima) with exploitation (searching in more detail around known good solutions to find local maxima) by controlling loudness and pulse emission rates of simulated bats in the multi-dimensional search space. === Glowworm swarm optimization === Glowworm swarm optimization (GSO), introduced by Krishnanand and Ghose in 2005 for simultaneous computation of multiple optima of multimodal functions. The algorithm shares a few features with some better known algorithms, such as ant colony optimization and particle swarm optimization, but with several significant differences. The agents in GSO are thought of as glowworms that carry a luminescence quantity called luciferin along with them. The glowworms encode the fitness of their current locations, evaluated using the objective function, into a luciferin value that they broadcast to their neighbors. The glowworm identifies its neighbors and computes its movements by exploiting an adaptive neighborhood, which is bounded above by its sensor range. Each glowworm selects, using a probabilistic mechanism, a neighbor that has a luciferin value higher than its own and moves toward it. These movements—based only on local information and selective neighbor interactions—enable the swarm of glowworms to partition into disjoint subgroups that converge on multiple optima of a given multimodal function. === Gravitational search algorithm === Gravitational search algorithm (GSA) based on the law of gravity and the notion of mass interactions. The GSA algorithm uses the theory of Newtonian physics and its searcher agents are the collection of masses. In GSA, there is an isolated system of masses. Using the gravitational force, every mass in the system can see the situation of other masses. The gravitational force is therefore a way of transferring information between different masses (Rashedi, Nezamabadi-pour and Saryazdi 2009). In GSA, agents are considered as objects and their performance is measured by their masses. All these objects attract each other by a gravity force, and this force causes a movement of all objects globally towards the objects with heavier masses. The heavy masses correspond to good solutions of the problem. The position of the agent corresponds to a solution of the problem, and its mass is determined using a fitness function. By lapse of time, masses are attracted by the heaviest mass, which would ideally present an optimum solution in the search space. The GSA could be considered as an isolated system of masses. It is like a small artificial world of masses obeying the Newtonian laws of gravitation and motion (Rashedi, Nezamabadi-pour and Saryazdi 2009). A multi-objective variant of GSA, called MOGSA, was first proposed by Hassanzadeh et al. in 2010. === River formation dynamics === River formation dynamics (RFD) is based on imitating how water forms rivers by eroding the ground and depositing sediments (the drops act as the swarm). After drops transform the landscape by increasing/decreasing the altitude of places, solutions are given in the form of paths of decreasing altitudes. Decreasing gradients are constructed, and these gradients are followed by subsequent drops to compose new gradients and reinforce the best ones. This heuristic optimization method was first presented in 2007 by Rabanal et al. The applicability of RFD to other NP-complete problems has been studied, and the algorithm has been applied to fields such as routing and robot navigation. === Self-propelled particles === Self-propelled particles (SPP), also referred to as the Vicsek model, was introduced in 1995 by Vicsek et al. as a special case of the boids model introduced in 1986 by Reynolds. A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood. SPP models predict that swarming animals share certain properties at the group level, regardless of the type of animals in the swarm. Swarming systems give rise to emergent behaviours which occur at many different scales, some of which are turning out to be both universal and robust. It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours. === Stochastic diffusion search === Stochastic diffusion search (SDS) is an agent-based probabilistic global search and optimization technique best suited to problems where the objective function can be decomposed into multiple independent partial-functions. Each agent maintains a hypothesis which is iteratively tested by evaluating a randomly selected partial objective function parameterised by the agent's current hypothesis. In the standard version of SDS such partial function evaluations are binary, resulting in each agent becoming active or inactive. Information on hypotheses is diffused across the population via inter-agent communication. Unlike the stigmergic communication used in ACO, in SDS agents communicate hypotheses via a one-to-one communication strategy analogous to the tandem running procedure observed in Leptothorax acervorum. A positive feedback mechanism ensures that, over time, a population of agents stabilise around the global-best solution. SDS is both an efficient and robust global search and optimisation algorithm, which has been extensively mathematically described. Recent work has involved merging the global search properties of SDS with other swarm intelligence algorithms. === Multi-swarm optimization === Multi-swarm optimization is a variant of particle swarm optimization (PSO) based on the use of multiple sub-swarms instead of one (standard) swarm. The general approach in multi-swarm optimization is that each sub-swarm focuses on a specific region while a specific diversification method decides where and when to launch the sub-swarms. The multi-swarm framework is especially fitted for the optimization on multi-modal problems, where multiple (local) optima exist. == Applications == Swarm Intelligence-based techniques can be used in a number of applications. The U.S. military is investigating swarm techniques for controlling unmanned vehicles. The European Space Agency 
6:1093:The following is a list of current and past, nonclassified notable artificial intelligence projects.
87:420:In genetic algorithms, inheritance is the ability of modeled objects to mate, mutate (similar to biological mutation), and propagate their problem solving genes to the next generation, in order to produce an evolved solution to a particular problem. The selection of objects that will be inherited from in each successive generation is determined by a fitness function, which varies depending upon the problem being addressed.
26:260:
105:371:Hierarchical clustering is one method for finding community structures in a network. The technique arranges the network into a hierarchy of groups according to a specified weight function. The data can then be represented in a tree structure known as a dendrogram. Hierarchical clustering can either be agglomerative or divisive depending on whether one proceeds through the algorithm by adding links to or removing links from the network, respectively. One divisive technique is the Girvan–Newman algorithm.
165:1009:A cart is a vehicle designed for transport, using two wheels and normally pulled by one or a pair of draught animals. A handcart is pulled or pushed by one or more people. It is different from a dray or wagon, which is a heavy transport vehicle with four wheels and typically two or more horses, or a carriage, which is used exclusively for transporting humans.
190:456:In computer science, Artificial Ants stand for multi-agent methods inspired by the behavior of real ants. The pheromone-based communication of biological ants is often the predominant paradigm used. Combinations of Artificial Ants and local search algorithms have become a method of choice for numerous optimization tasks involving some sort of graph, e. g., vehicle routing and internet routing. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as AntOptima. As an example, Ant colony optimization is a class of optimization algorithms modeled on the actions of an ant colony. Artificial 'ants' (e.g. simulation agents) locate optimal solutions by moving through a parameter space representing all possible solutions. Real ants lay down pheromones directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions. One variation on this approach is the bees algorithm, which is more analogous to the foraging patterns of the honey bee, another social insect.
122:493:A focused crawler is a web crawler that collects Web pages that satisfy some specific property, by carefully prioritizing the crawl frontier and managing the hyperlink exploration process. Some predicates may be based on simple, deterministic and surface properties. For example, a crawler's mission may be to crawl pages from only the .jp domain. Other predicates may be softer or comparative, e.g., "crawl pages with large PageRank", or "crawl pages about baseball". An important page property pertains to topics, leading to topical crawlers. For example, a topical crawler may be deployed to collect pages about solar power, or swine flu, while minimizing resources spent fetching pages on other topics. Crawl frontier management may not be the only device used by focused crawlers; they may use a Web directory, a Web text index, backlinks, or any other Web artifact.
193:931:An autoencoder, autoassociator or Diabolo network is an artificial neural network used for learning efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.
128:388:Google Brain is a deep learning research project at Google.
61:237:NEAT Particles is an Interactive evolutionary computation program that enables users to evolve particle systems intended for use as special effects in video games or movie graphics. Rather than being hand-coded like typical particle systems, the behaviors of NEAT Particle effects are evolved by user preference. Therefore, non-programmer, non-artist users may evolve complex and unique special effects in real time. NEAT Particles is meant to augment and assist the time-consuming computer graphics content generation process.
1:0:Artificial intelligence (AI) is the intelligence exhibited by machines or software. It is also the name of the academic field of study which studies how to create computers and computer software that are capable of intelligent behavior. Major AI researchers and textbooks define this field as "the study and design of intelligent agents", in which an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. John McCarthy, who coined the term in 1955, defines it as "the science and engineering of making intelligent machines". AI research is highly technical and specialized, and is deeply divided into subfields that often fail to communicate with each other. Some of the division is due to social and cultural factors: subfields have grown up around particular institutions and the work of individual researchers. AI research is also divided by several technical issues. Some subfields focus on the solution of specific problems. Others focus on one of several possible approaches or on the use of a particular tool or towards the accomplishment of particular applications. The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects. General intelligence is still among the field's long-term goals. Currently popular approaches include statistical methods, computational intelligence and traditional symbolic AI. There are a large number of tools used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others. The AI field is interdisciplinary, in which a number of sciences and professions converge, including computer science, mathematics, psychology, linguistics, philosophy and neuroscience, as well as other specialized fields such as artificial psychology. The field was founded on the claim that a central property of humans, human intelligence—the sapience of Homo sapiens sapiens—"can be so precisely described that a machine can be made to simulate it." This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity. Artificial intelligence has been the subject of tremendous optimism but has also suffered stunning setbacks. Today AI techniques have become an essential part of the technology industry, providing the heavy lifting for many of the most challenging problems in computer science. == History == Thinking machines and artificial beings appear in Greek myths, such as Talos of Crete, the bronze robot of Hephaestus, and Pygmalion's Galatea. Human likenesses believed to have intelligence were built in every major civilization: animated cult images were worshiped in Egypt and Greece and humanoid automatons were built by Yan Shi, Hero of Alexandria and Al-Jazari. It was also widely believed that artificial beings had been created by Jābir ibn Hayyān, Judah Loew and Paracelsus. By the 19th and 20th centuries, artificial beings had become a common feature in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Rossum's Universal Robots). Pamela McCorduck argues that all of these are some examples of an ancient urge, as she describes it, "to forge the gods". Stories of these creatures and their fates discuss many of the same hopes, fears and ethical concerns that are presented by artificial intelligence. Mechanical or "formal" reasoning has been developed by philosophers and mathematicians since antiquity. The study of logic led directly to the invention of the programmable digital electronic computer, based on the work of mathematician Alan Turing and others. Turing's theory of computation suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable act of mathematical deduction. This, along with concurrent discoveries in neurology, information theory and cybernetics, inspired a small group of researchers to begin to seriously consider the possibility of building an electronic brain. The field of AI research was founded at a conference on the campus of Dartmouth College in the summer of 1956. The attendees, including John McCarthy, Marvin Minsky, Allen Newell, Arthur Samuel, and Herbert Simon, became the leaders of AI research for many decades. They and their students wrote programs that were, to most people, simply astonishing: computers were winning at checkers, solving word problems in algebra, proving logical theorems and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were profoundly optimistic about the future of the new field: Herbert Simon predicted that "machines will be capable, within twenty years, of doing any work a man can do" and Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved". They had failed to recognize the difficulty of some of the problems they faced. In 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off all undirected exploratory research in AI. The next few years would later be called an "AI winter", a period when funding for AI projects was hard to find. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of one or more human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research in the field. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting AI winter began. In the late 1990s and early 21st century, AI achieved its greatest successes, and began to be used for logistics, data mining, medical diagnosis and many other areas throughout the technology industry. The success was due to several factors: the increasing computational power of computers (see Moore's law), a greater emphasis on solving specific subproblems, the creation of new ties between AI and other fields working on similar problems, and a new commitment by researchers to solid mathematical methods and rigorous scientific standards. Faster computers were the principal reason that Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997. A set of advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers produced enormous advances in machine learning and perception. By the mid 2010s, successful machine learning applications were being used widely throughout the world. Programs that incorporated these techniques began to accomplish things that had seemed impossible in the mid-80s. In a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin. The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional human Go player without handicaps. == Research == === Goals === The general problem of simulating (or creating) intelligence has been broken down into a number of specific sub-problems. These consist of particular traits or capabilities that researchers would like an intelligent system to display. The traits described below have received the most attention. ==== Deduction, reasoning, problem solving ==== Early AI researchers developed algorithms that imitated the step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had also developed highly successful methods for dealing with uncertain or incomplete information, employing concepts from probability and economics. For difficult problems, most of these algorithms can require enormous computational resources – most experience a "combinatorial explosion": the amount of memory or computer time required becomes astronomical when the problem goes beyond a certain size. The search for more efficient problem-solving algorithms is a high priority for AI research. Human beings solve most of their problems using fast, intuitive judgements rather than the conscious, step-by-step deduction that early AI research was able to model. AI has made some progress at imitating this kind of "sub-symbolic" problem solving: embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning; neural net research attempts to simulate the structures inside the brain that give rise to this skill; statistical approaches to AI mimic the probabilistic nature of the human ability to guess. ==== Knowledge representation ==== Knowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of "what exists" is an ontology: the set of objects, relations, concepts and so on that the machine knows about. The most general are called upper ontologies, which attempt to provide a foundation for all other knowledge. Among the most difficult problems in knowledge representation are: Default reasoning and the qualification problem Many of the things people know take the form of "working assumptions." For example, if a bird comes up in conversation, people typically picture an animal that is fist sized, sings, and flies. None of these things are true about all birds. John McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem. The breadth of commonsense knowledge The number of atomic facts that the average person knows is astronomical. Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineering—they must be built, by hand, one complicated concept at a time. A major goal is to have the computer understand enough concepts to be able to learn by reading from sources like the Internet, and thus be able to add to its own ontology. The subsymbolic form of some commonsense knowledge Much of what people know is not represented as "facts" or "statements" that they could express verbally. For example, a chess master will avoid a particular chess position because it "feels too exposed" or an art critic can 
185:236:Marco Dorigo (born 26 August 1961, in Milan, Italy) is a research director for the Belgian Funds for Scientific Research (FNRS) and a co-director of IRIDIA, the artificial intelligence lab of the Université Libre de Bruxelles. He is the proponent of the ant colony optimization metaheuristic (see his book published by MIT Press in 2004), and one of the founders of the swarm intelligence research field. Recently he got involved with research in swarm robotics: he is the coordinator of Swarm-bots: Swarms of self-assembling artefacts and of Swarmanoid: Towards humanoid robotic swarms two swarm robotics projects funded by the Future and Emerging Technologies Program of the European Commission. He is also the founding editor and editor in chief of Swarm Intelligence, the principal peer reviewed publication dedicated to reporting research and new developments in this multidisciplinary field.
198:599:Peter Fonagy OBE FMedSci FBA (born 1952) is a Hungarian-born British psychoanalyst and clinical psychologist. He studied clinical psychology at University College London. He is Freud Memorial Professor of Psychoanalysis and head of the department of Clinical, Educational and Health Psychology at University College London, Chief Executive of the Anna Freud Centre, a training and supervising analyst in the British Psycho-Analytical Society in child and adult analysis, a Fellow of the British Academy, the Faculty of Medical Sciences and a registrant of the BPC. His clinical interests centre on issues of borderline psychopathology, violence and early attachment relationships. His work attempts to integrate empirical research with psychoanalytic theory. He has published numerous articles and has authored or edited 16 books.
116:615:In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.
148:457:In statistics, latent variables (from Latin: present participle of lateo (“lie hidden”), as opposed to observable variables), are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, economics, medicine, physics, machine learning/artificial intelligence, bioinformatics, natural language processing, econometrics, management and the social sciences.
167:280:In decision tree learning, Information gain ratio is a ratio of information gain to the intrinsic information. It is used to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.
120:313:Richard S. Sutton is an American computer scientist. Currently he is professor of Computer Science and iCORE chair at the University of Alberta. Dr. Sutton is considered one of the founding fathers of modern computational reinforcement learning, having several significant contributions to the field, including temporal difference learning, policy gradient methods, the Dyna architecture.
136:434:TensorFlow is an open source software library for machine learning in various kinds of perceptual and language understanding tasks. It is a second-generation API which is currently used for both research and production by 50 different teams in dozens of commercial Google products, such as speech recognition, Gmail, Google Photos, and Search. These teams had previously used DistBelief, a first-generation API. TensorFlow was originally developed by the Google Brain team for Google's research and production purposes and later released under the Apache 2.0 open source license on November 9, 2015.
15:795:Human-based evolutionary computation (HBEC) is a set of evolutionary computation techniques that rely on human innovation. Human-based evolutionary computation techniques can be classified into three more specific classes analogous to ones in evolutionary computation. There are three basic types of innovation: initialization, mutation, and recombination. Here is a table illustrating which type of human innovation are supported in different classes of HBEC:
97:0:Educational Data Mining (EDM) describes a research field concerned with the application of data mining, machine learning and statistics to information generated from educational settings (e.g., universities and intelligent tutoring systems). At a high level, the field seeks to develop and improve methods for exploring this data, which often has multiple levels of meaningful hierarchy, in order to discover new insights about how people learn in the context of such settings. In doing so, EDM has contributed to theories of learning investigated by researchers in educational psychology and the learning sciences. The field is closely tied to that of learning analytics, and the two have been compared and contrasted. == Definition == Educational Data Mining refers to techniques, tools, and research designed for automatically extracting meaning from large repositories of data generated by or related to people's learning activities in educational settings. Quite often, this data is extensive, fine-grained, and precise. For example, several learning management systems (LMSs) track information such as when each student accessed each learning object, how many times they accessed it, and how many minutes the learning object was displayed on the user's computer screen. As another example, Intelligent tutoring systems record data every time a learner submits a solution to a problem; they may collect the time of the submission, whether or not the solution matches the expected solution, the amount of time that has passed since the last submission, the order in which solution components were entered into the interface, etc. The precision of this data is such that even a fairly short session with a computer-based learning environment (e.g., 30 minutes) may produce a large amount of process data for analysis. In other cases, the data is less fine-grained. For example, a student's university transcript may contain a temporally ordered list of courses taken by the student, the grade that the student earned in each course, and when the student selected or changed his or her academic major. EDM leverages both types of data to discover meaningful information about different types of learners and how they learn, the structure of domain knowledge, and the effect of instructional strategies embedded within various learning environments. These analyses provide new information that would be difficult to discern by looking at the raw data. For example, analyzing data from an LMS may reveal a relationship between the learning objects that a student accessed during the course and their final course grade. Similarly, analyzing student transcript data may reveal a relationship between a student's grade in a particular course and their decision to change their academic major. Such information provides insight into the design of learning environments, which allows students, teachers, school administrators, and educational policy makers to make informed decisions about how to interact with, provide, and manage educational resources. == History == While the analysis of educational data is not itself a new practice, recent advances in educational technology, including the increase in computing power and the ability to log fine-grained data about students' use of a computer-based learning environment, have led to an increased interest in developing techniques for analyzing the large amounts of data generated in educational settings. This interest translated into a series of EDM workshops held from 2000-2007 as part of several international research conferences. In 2008, a group of researchers established what has become an annual international research conference on EDM, the first of which took place in Montreal, Canada. As interest in EDM continued to increase, EDM researchers established an academic journal in 2009, the Journal of Educational Data Mining, for sharing and disseminating research results. In 2011, EDM researchers established the International Educational Data Mining Society to connect EDM researchers and continue to grow the field. With the introduction of public educational data repositories in 2008, such as the Pittsburgh Science of Learning Centre’s (PSLC) DataShop and the National Center for Education Statistics (NCES), public data sets have made educational data mining more accessible and feasible, contributing to its growth. == Goals == Baker and Yacef identified the following four goals of EDM: Predicting students' future learning behavior – With the use of student modeling, this goal can be achieved by creating student models that incorporate the learner’s characteristics, including detailed information such as their knowledge, behaviours and motivation to learn. The user experience of the learner and their overall satisfaction with learning are also measured. Discovering or improving domain models – Through the various methods and applications of EDM, discovery of new and improvements to existing models is possible. Examples include illustrating the educational content to engage learners and determining optimal instructional sequences to support the student’s learning style. Studying the effects of educational support that can be achieved through learning systems. Advancing scientific knowledge about learning and learners by building and incorporating student models, the field of EDM research and the technology and software used. == Users and Stakeholders == There are four main users and stakeholders involved with educational data mining. These include: Learners - Learners are interested in understanding student needs and methods to improve the learner’s experience and performance. For example, learners can also benefit from the discovered knowledge by using the EDM tools to suggest activities and resources that they can use based on their interactions with the online learning tool and insights from past or similar learners. For younger learners, educational data mining can also inform parents about their child’s learning progress. It is also necessary to effectively group learners in an online environment. The challenge is to learn these groups based on the complex data as well as develop actionable models to interpret these groups. Educators - Educators attempt to understand the learning process and the methods they can use to improve their teaching methods. Educators can use the applications of EDM to determine how to organize and structure the curriculum, the best methods to deliver course information and the tools to use to engage their learners for optimal learning outcomes. In particular, the distillation of data for human judgment technique provides an opportunity for educators to benefit from EDM because it enables educators to quickly identify behavioural patterns, which can support their teaching methods during the duration of the course or to improve future courses. Educators can determine indicators that show student satisfaction and engagement of course material, and also monitor learning progress. Researchers - Researchers focus on the development and the evaluation of data mining techniques for effectiveness. A yearly international conference for researchers began in 2008, followed by the establishment of the Journal of Educational Data Mining in 2009. The wide range of topics in EDM ranges from using data mining to improve institutional effectiveness to student performance. Administrators - Administrators are responsible for allocating the resources for implementation in institutions. As institutions are increasingly held responsible for student success, the administering of EDM applications are becoming more common in educational settings. Faculty and advisors are becoming more proactive in identifying and addressing at-risk students. However, it is sometimes a challenge to get the information to the decision makers to administer the application in a timely and efficient manner. == Phases of Educational Data Mining == As research in the field of educational data mining has continued to grow, a myriad of data mining techniques have been applied to a variety of educational contexts. In each case, the goal is to translate raw data into meaningful information about the learning process in order to make better decisions about the design and trajectory of a learning environment. Thus, EDM generally consists of four phases: The first phase of the EDM process (not counting pre-processing) is discovering relationships in data. This involves searching through a repository of data from an educational environment with the goal of finding consistent relationships between variables. Several algorithms for identifying such relationships have been utilized, including classification, regression, clustering, factor analysis, social network analysis, association rule mining, and sequential pattern mining. Discovered relationships must then be validated in order to avoid overfitting. Validated relationships are applied to make predictions about future events in the learning environment. Predictions are used to support decision-making processes and policy decisions. During phases 3 and 4, data is often visualized or in some other way distilled for human judgment. A large amount of research has been conducted in best practices for visualizing data. == Main Approaches == Of the general categories of methods mentioned, prediction, clustering and relationship mining are considered universal methods across all types of data mining; however, Discovery with Models and Distillation of Data for Human Judgment are considered more prominent approaches within educational data mining. === Discovery with Models === In the Discovery with Model method, a model is developed via prediction, clustering or by human reasoning knowledge engineering and then used as a component in another analysis, namely in prediction and relationship mining. In the prediction method use, the created model’s predictions are used to predict a new variable. For the use of relationship mining, the created model enables the analysis between new predictions and additional variables in the study. In many cases, discovery with models uses validated prediction models that have proven generalizability across contexts. Key applications of this method include discovering relationships between student behaviors, characteristics and contextual variables in the learning environment. Further discovery of broad and specific research questions across a wide range of contexts can also be explored using this method. === Distillation of Data for Human Judgment === Humans can make inferences about data that may be beyond the scope in which an automated data mining method provides. For the use of education data mining, data is distilled for human judgment for two key purposes, identification and classification. For the purpose of identification, data is distilled to enable humans to identify well-known patterns, which may otherwise be difficult to interpret. For example, the learning curve, classic to educational studies, is a pattern that clearly reflects the relationship between learning and experience over time. Data is also distilled for the purposes of classifying features of data, which for educational data mining, is used to support the development of the prediction model. Classification helps expedite the development of the prediction model, tremendously. The goal of this method is to summarize and present the information in a useful, interactive and visually appealing way in order to understand the large amounts of education data and to support decision making. In particular, this method is beneficial to educators in understanding usage information and effectiveness in course activities. Key applications for the distillation of data for human judgment include identifying patterns in student learning, behavior, opportunities for collaboration and labeling data for future uses in prediction models. == Applications == A list of the primary applications of EDM is provided by Cristobal Romero and Sebastian Ventura. In their taxonomy, the areas of EDM application are: 
138:794:A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.
100:0:Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics and data compression. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties. Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς "grape") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals. Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology. == Definition == The notion of a "cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these "cluster models" is key to understanding the differences between the various algorithms. Typical cluster models include: Connectivity models: for example, hierarchical clustering builds models based on distance connectivity. Centroid models: for example, the k-means algorithm represents each cluster by a single mean vector. Distribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the Expectation-maximization algorithm. Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space. Subspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes. Group models: some algorithms do not provide a refined model for their results and just provide the grouping information. Graph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm. A "clustering" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as: hard clustering: each object belongs to a cluster or not soft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster) There are also finer distinctions possible, for example: strict partitioning clustering: here each object belongs to exactly one cluster strict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers. overlapping clustering (also: alternative clustering, multi-view clustering): while usually a hard clustering, objects may belong to more than one cluster. hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster subspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap. == Algorithms == Clustering algorithms can be categorized based on their cluster model, as listed above. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms. There is no objectively "correct" clustering algorithm, but as it was noted, "clustering is in the eye of the beholder." The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. It should be noted that an algorithm that is designed for one kind of model has no chance on a data set that contains a radically different kind of model. For example, k-means cannot find non-convex clusters. === Connectivity-based clustering (hierarchical clustering) === Connectivity based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect "objects" to form "clusters" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name "hierarchical clustering" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix. Connectivity based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance to) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances) or UPGMA ("Unweighted Pair Group Method with Arithmetic Mean", also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions). These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as "chaining phenomenon", in particular with single-linkage clustering). In the general case, the complexity is for agglomerative clustering and for divisive clustering, which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete. They did however provide inspiration for many later methods such as density based clustering. Linkage clustering examples === Centroid-based clustering === In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized. The optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well known approximative method is Lloyd's algorithm, often actually referred to as "k-means algorithm". It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (K-means++) or allowing a fuzzy cluster assignment (Fuzzy c-means). Most k-means-type algorithms require the number of clusters - - to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders in between of clusters (which is not surprising, as the algorithm optimized cluster centers, not cluster borders). K-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model based classification, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below. k-Means clustering examples === Distribution-based clustering === The clustering model most closely related to statistics is based on distribution models. Clusters can then easily be defined as objects belonging most likely to the same distribution. A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution. While the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult. One prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modelled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to fit better to the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a 
80:937:In genetic algorithms, crossover is a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next. It is analogous to reproduction and biological crossover, upon which genetic algorithms are based. Cross over is a process of taking more than one parent solutions and producing a child solution from them. There are methods for selection of the chromosomes. Those are also given below.
89:260:Hod Lipson (born 1967 in Haifa, Israel) is an American robotics engineer. He is the director of Cornell University's Creative Machines Lab (CCML), formerly known as Computational Synthesis Lab (CCSL), at the Sibley School of Mechanical and Aerospace Engineering. Lipson's work focuses on evolutionary robotics, design automation, rapid prototyping, artificial life, and creating machines that can demonstrate some aspects of human creativity. His publications have been cited close to 10,000 times, and he has an h-index of 50, as of November 8, 2015.
81:446:Genetic algorithms have increasingly been applied to economics since the pioneering work by John H. Miller in 1986. It has been used to characterize a variety of models including the cobweb model, the overlapping generations model, game theory, schedule optimization and asset pricing. Specifically, it has been used as a model to represent learning, rather than as a means for fitting a model.
172:302:Classification chart or classification tree is a synopsis of the classification scheme, designed to illustrate the structure of any particular field.
43:0:In machine learning, feature learning or representation learning is a set of techniques that learn a feature: a transformation of raw data input to a representation that can be effectively exploited in machine learning tasks. This obviates manual feature engineering, which is otherwise necessary, and allows a machine to both learn at a specific task (using the features) and learn the features themselves: to learn how to learn. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor measurement is usually complex, redundant, and highly variable. Thus, it is necessary to discover useful features or representations from raw data. Traditional hand-crafted features often require expensive human labor and often rely on expert knowledge. Also, they normally do not generalize well. This motivates the design of efficient feature learning techniques, to automate and generalize this. Feature learning can be divided into two categories: supervised and unsupervised feature learning, analogous to these categories in machine learning generally. In supervised feature learning, features are learned with labeled input data. Examples include neural networks, multilayer perceptron, and (supervised) dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, sparse autoencoders, matrix factorization, and various forms of clustering. == Supervised feature learning == Supervised feature learning is to learn features from labeled data. Several approaches are introduced in the following. === Supervised dictionary learning === Dictionary learning is to learn a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights). Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique was proposed by Mairal et al. in 2009. The authors apply dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier. === Neural networks === Neural networks are used to illustrate a family of learning algorithms via a "network" consisting of multiple layers of inter-connected nodes. It is inspired by the nervous system, where the nodes are viewed as neurons and edges are viewed as synapse. Each edge has an associated weight, and the network defines computational rules that passes input data from the input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights). Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. == Unsupervised feature learning == Unsupervised feature learning is to learn features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where first, features are learned from an unlabeled dataset, which are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following. === K-means clustering === K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, and suboptimal greedy algorithms have been developed for k-means clustering. In feature learning, k-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest way is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration. It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has used to train RBF networks). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms. In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task. K-means has also been shown to improve performance in the domain of NLP, specifically for named-entity recognition; there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings). === Principal component analysis === Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations. PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix. PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case in many applications. PCA only relies on orthogonal transformations of the original data, and it only exploits the first- and second-order moments of the data, which may not well characterize the distribution of the data. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues). === Local linear embedding === Local linear embedding (LLE) is a nonlinear unsupervised learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Sam T. Roweis and Lawrence K. Saul in 2000. The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set. LLE consists of two major steps. The first step is for "neighbor-preserving," where each input data point Xi is reconstructed as a weighted sum of K nearest neighboring data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between a point and its reconstruction) under the constraint that the weights associated to each point sum up to one. The second step is for "dimension reduction," by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with data being fixed, which can be solved as a least squares problem; while in the second step, lower-dimensional points are optimized with the weights being fixed, which can be solved via sparse eigenvalue decomposition. The reconstruction weights obtained in the first step captures the "intrinsic geometric properties" of a neighborhood in the input data. It is assumed that original data lie on a smooth lower-dimensional manifold, and the "intrinsic geometric properties" captured by the weights of the original data are expected also on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying structure of data. === Independent component analysis === Independent component analysis (ICA) is a technique for learning a representation of data using a weighted sum of independent non-Gaussian components. The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution. === Unsupervised dictionary learning === Different from supervised dictionary learning, unsupervised dictionary learning does not utilize the labels of the data and only exploits the structure underlying the data for optimizing the dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionary, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed an algorithm known as K-SVD for learning from unlabeled input data a dictionary of elements that enables sparse representation of the data. == Multilayer/Deep architectures == The hierarchical architecture of the neural system inspires deep learning architectures for feature learning by stacking multiple layers of simple learning blocks. These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input of bottom layer is the raw data, and the output of the final layer is the final low-dimensional feature or representation. === Restricted Boltzmann machine === Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures. An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It 
153:1561:Subjective logic is a type of probabilistic logic that explicitly takes uncertainty and belief ownership into account. In general, subjective logic is suitable for modeling and analysing situations involving uncertainty and incomplete knowledge. For example, it can be used for modeling trust networks and for analysing Bayesian networks.
189:418:James Kennedy (born November 5, 1950) is an American social psychologist, best known as an originator and researcher of particle swarm optimization. The first papers on the topic, by Kennedy and Russell C. Eberhart, were presented in 1995; since then more than ten thousand papers have been published on particle swarms. The Academic Press / Morgan Kaufmann book, Swarm Intelligence, by Kennedy and Eberhart with Yuhui Shi, was published in 2001.
119:252:Apprenticeship learning, or apprenticeship via inverse reinforcement learning (AIRP), is a concept in the field of artificial intelligence and machine learning, developed by Pieter Abbeel, Associate Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. It was incepted in 2004. AIRP deals with "Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform"
151:595:Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involve predicting structured objects, rather than scalar discrete or real values.
23:265:ECJ is a freeware evolutionary computation research system written in Java. It is a framework that supports a variety of evolutionary computation techniques, such as genetic algorithms, genetic programming, evolution strategies, coevolution, particle swarm optimization, and differential evolution. The framework models iterative evolutionary processes using a series of pipelines arranged to connect one or more subpopulations of individuals with selection, breeding (such as crossover, and mutation operators that produce new individuals. The framework is open source and is distributed under the Academic Free License. ECJ was created by Sean Luke, a computer science professor at George Mason University, and is maintained by Sean Luke and a variety of contributors.
159:511:Evolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications.
72:687:MCACEA (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) is a general framework that uses a single evolutionary algorithm (EA) per agent sharing their optimal solutions to coordinate the evolutions of the EAs populations using cooperation objectives. This framework can be used to optimize some characteristics of multiple cooperating agents in mathematical optimization problems. More specifically, due to its nature in which both individual and cooperation objectives are optimize, MCACEA is used in multi-objective optimization problems.
191:630:In computer science, soft computing is the use of inexact solutions to computationally hard tasks such as the solution of NP-complete problems, for which there is no known algorithm that can compute an exact solution in polynomial time. Soft computing differs from conventional (hard) computing in that, unlike hard computing, it is tolerant of imprecision, uncertainty, partial truth, and approximation. In effect, the role model for soft computing is the human mind.
197:673:Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.
78:0:In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection and crossover. == Methodology == === Optimization problems === In a genetic algorithm, a population of candidate solutions (called individuals, creatures, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible. The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population. A typical genetic algorithm requires: a genetic representation of the solution domain, a fitness function to evaluate the solution domain. A standard representation of each candidate solution is as an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming. Once the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators. ==== Initialization ==== The population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the search space). Occasionally, the solutions may be "seeded" in areas where optimal solutions are likely to be found. ==== Selection ==== During each successive generation, a proportion of the existing population is selected to breed a new generation. Individual solutions are selected through a fitness-based process, where fitter solutions (as measured by a fitness function) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming. The fitness function is defined over the genetic representation and measures the quality of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise. In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used. ==== Genetic operators ==== The next step is to generate a second generation population of solutions from those selected through a combination of genetic operators: crossover (also called recombination), and mutation. For each new solution to be produced, a pair of "parent" solutions is selected for breeding from the pool selected previously. By producing a "child" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its "parents". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated. Although reproduction methods that are based on the use of two parents are more "biology inspired", some research suggests that more than two "parents" generate higher quality chromosomes. These processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children. Opinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search. Although crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms. It is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed. ==== Termination ==== This generational process is repeated until a termination condition has been reached. Common terminating conditions are: A solution is found that satisfies minimum criteria Fixed number of generations reached Allocated budget (computation time/money) reached The highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results Manual inspection Combinations of the above == The building block hypothesis == Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of: A description of a heuristic that performs adaptation by identifying and recombining "building blocks", i.e. low order, low defining-length schemata with above average fitness. A hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic. Goldberg describes the heuristic as follows: "Short, low order, and highly fit schemata are sampled, recombined [crossed over], and resampled to form strings of potentially higher fitness. In a way, by working with these particular schemata [the building blocks], we have reduced the complexity of our problem; instead of building high-performance strings by trying every conceivable combination, we construct better and better strings from the best partial solutions of past samplings. "Because highly fit schemata of low defining length and low order play such an important role in the action of genetic algorithms, we have already given them a special name: building blocks. Just as a child creates magnificent fortresses through the arrangement of simple blocks of wood, so does a genetic algorithm seek near optimal performance through the juxtaposition of short, low-order, high-performance schemata, or building blocks." == Limitations == There are limitations of the use of a genetic algorithm compared to alternative optimization algorithms: Repeated fitness function evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive fitness function evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods can not deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an approximated fitness that is computationally efficient. It is apparent that amalgamation of approximate models may be one of the most promising approaches to convincingly use GA to solve complex real life problems. Genetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, and airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts. The "better" solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem. In many problems, GAs may have a tendency to converge towards local optima or even arbitrary points rather than the global optimum of the problem. This means that it does not "know how" to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the fitness landscape: certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions, although the No Free Lunch theorem proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a "niche penalty", wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, 
18:407:The Learnable Evolution Model (LEM) is a novel, non-Darwinian methodology for evolutionary computation that employs machine learning to guide the generation of new individuals (candidate problem solutions). Unlike standard, Darwinian-type evolutionary computation methods that use random or semi-random operators for generating new individuals (such as mutations and/or recombinations), LEM employs hypothesis generation and instantiation operators.
8:270:The European Coordinating Committee for Artificial Intelligence (ECCAI) is the representative body for the European artificial intelligence community. Its aim is to promote study, research, and applications of artificial intelligence (AI) in Europe. It was established in 1982.
57:1444:The evolution of nervous systems dates back to the first development of nervous systems in animals (or metazoans). Neurons developed as specialized electrical signaling cells in multicellular animals, adapting the mechanism of action potentials present in motile single-celled and colonial eukaryotes. Simple nerve nets seen in animals like cnidaria evolved first, followed by nerve cords in bilateral animals - ventral nerve cords in invertebrates and dorsal nerve cords surrounded by a notochord in chordates. Bilateralization led to the evolution of brains, a process called cephalization.
195:1468:Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as coding) in the form of a linear combination of basis elements as well as those basis elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal. This problem setup also allows the dimensionality of representation space to be higher than the one of the input space. The above two properties lead to having seemingly redundant atoms that allow multiple reconstruction ways but also provide an improvement in sparsity and flexibility of the representation.
33:618:Active learning is a special case of semi-supervised machine learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points. In statistics literature it is sometimes also called optimal experimental design.
155:668:Natural evolution strategies (NES) are a family of numerical optimization algorithms for black-box problems. Similar in spirit to evolution strategies, they iteratively update the (continuous) parameters of a search distribution by following the natural gradient towards higher expected fitness.
142:0:A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if parent nodes represent Boolean variables then the probability function could be represented by a table of entries, one entry for each of the possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks. Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. == Example == Suppose that there are two events which could cause grass to be wet: either the sprinkler is on or it's raining. Also, suppose that the rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler is usually not turned on). Then the situation can be modeled with a Bayesian network (shown). All three variables have two possible values, T (for true) and F (for false). The joint probability function is: where the names of the variables have been abbreviated to G = Grass wet (yes/no), S = Sprinkler turned on (yes/no), and R = Raining (yes/no). The model can answer questions like "What is the probability that it is raining, given the grass is wet?" by using the conditional probability formula and summing over all nuisance variables: Using the expansion for the joint probability function and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example, Then the numerical results (subscripted by the associated variable values) are If, on the other hand, we wish to answer an interventional question: "What is the probability that it would rain, given that we wet the grass?" the answer would be governed by the post-intervention joint distribution function obtained by removing the factor from the pre-intervention distribution. As expected, the probability of rain is unaffected by the action: . If, moreover, we wish to predict the impact of turning the sprinkler on, we have with the term removed, showing that the action has an effect on the grass but not on the rain. These predictions may not be feasible when some of the variables are unobserved, as in most policy evaluation problems. The effect of the action can still be predicted, however, whenever a criterion called "back-door" is satisfied. It states that, if a set Z of nodes can be observed that d-separates (or blocks) all back-door paths from X to Y then . A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called "sufficient" or "admissible." For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separate the (only) back-door path S ← R → G. However, if S is not observed, there is no other set that d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. We then say that P(G | do(S = T)) is not "identified." This reflects the fact that, lacking interventional data, we cannot determine if the observed dependence between S and G is due to a causal connection or is spurious (apparent dependence arising from a common cause, R). (see Simpson's paradox) To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of "do-calculus" and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data. Using a Bayesian network can save considerable amounts of memory, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for values. If the local distributions of no variable depends on more than three parent variables, the Bayesian network representation only needs to store at most values. One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions. == Inference and learning == There are three main inference tasks for Bayesian networks. === Inferring unobserved variables === Because a Bayesian network is a complete model for the variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to find out updated knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when one wants to choose values for the variable subset which minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems. The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space-time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation, and variational methods. === Parameter learning === In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on a distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, one commonly specifies the conditional distribution for the hidden state's temporal evolution to maximize the entropy rate of the implied stochastic process.) Often these conditional distributions include parameters which are unknown and must be estimated from data, sometimes using the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex when there are unobserved variables. A classical approach to this problem is the expectation-maximization algorithm which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions this process converges on maximum likelihood (or maximum posterior) values for parameters. A more fully Bayesian approach to parameters is to treat parameters as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, so in practice classical parameter-setting approaches are more common. === Structure learning === In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications the task of defining the network is too complex for humans. In this case the network structure and the parameters of the local distributions must be learned from data. Automatically learning the graph structure of a Bayesian network is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl (1987) and rests on the distinction between the three possible types of adjacent triplets allowed in a directed acyclic graph (DAG): Type 1 and type 2 represent the same dependencies ( and are independent given ) and are, therefore, indistinguishable. Type 3, however, can be uniquely identified, since and are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when and have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independencies observed. An alternative method of structural learning uses optimization based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein. Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables. A Bayesian network can be augmented with nodes and edges using rule-based machine learning techniques. Inductive logic programming can be used to 
19:705:Design Automation usually refers to electronic design automation, or Design Automation which is a Product configurator. Extending Computer-Aided Design (CAD), automated design and Computer-Automated Design (CAutoD)  are more concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.
41:326:The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.
79:349:In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used.
194:1309:Toxicology (from the Ancient Greek words τοξικός toxikos "poisonous" and λόγος logos) is a branch of biology, chemistry, and medicine (more specifically pharmacology) concerned with the study of the adverse effects of chemicals on living organisms. It also studies the harmful effects of chemical, biological and physical agents in biological systems that establishes the extent of damage in living organisms. The relationship between dose and its effects on the exposed organism is of high significance in toxicology. Factors that influence chemical toxicity include the dosage (and whether it is acute or chronic); the route of exposure, the species, age, sex and environment.
10:0:The following outline is provided as an overview of and topical guide to artificial intelligence: Artificial intelligence (AI) – intelligence exhibited by machines or software. It is also the name of the academic field which studies how to create computers and computer software that are capable of intelligent behaviour. == What type of thing is artificial intelligence? == Artificial intelligence can be described as all of the following: A misnomer – something named in error. Artificial grass isn't genuinely grass, but that doesn't apply to artificial intelligence. If a computer program is intelligent, its intelligence is real. See "synthetic intelligence", below. A form of intelligence Synthetic intelligence – intelligence of a man-made yet real quality: actual, not fake, not simulated A type of technology A type of computer technology A computer program that performs some intellectual function An emerging technology A field: An academic discipline A branch of science A branch of applied science A branch of computer science == Types of artificial intelligence == Weak AI – non-sentient computer intelligence, typically focused on a narrow task. The intelligence of weak AI is limited. Artificial general intelligence (strong AI) – hypothetical artificial intelligence at least as smart as a human. Such an AI would be recursive, in that it could improve itself. In successive intervals of increased intelligence, such an entity could theoretically achieve superintelligence in a relatively short period of time. One or more superintelligences could potentially change the world so profoundly and at such a high rate, that it may result in a technological singularity. Strong AI does not yet exist. The prospect of its creation inspires expections of both promise and peril, and has become the subject of an intense ongoing ethical debate. == Branches of artificial intelligence == === By approach === Symbolic AI – When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. Good Old Fashioned AI – Sub-symbolic – Early cybernetics and brain simulation – Behavior based AI – Subsumption architecture – Nouvelle AI – Computational intelligence (CI) – Computational creativity – Neural networks – Hybrid neural network – Recurrent neural network – Fuzzy systems – Evolutionary computation, including: Evolutionary algorithms – Genetic algorithm – Brain Emotional Learning Based Intelligent Controller – Swarm intelligence – Ant colony optimization – Statistical AI – === By application === Applications of artificial intelligence – Artificial Creativity – Artificial life – Automated planning and scheduling – Automated reasoning – Automation – Automatic target recognition – Biologically inspired computing – Computer Audition – Speech recognition – Speaker recognition – Computer vision – Image processing – Intelligent word recognition – Object recognition – Optical mark recognition – Handwriting recognition – Optical character recognition – Automatic number plate recognition – Facial recognition systems – Silent speech interface – Diagnosis (artificial intelligence) – Expert system – Decision support system – Clinical decision support system – Game artificial intelligence – Computer game bot – Video game AI – Computer chess – Computer Go – General game playing – General video game playing – Game theory – Hybrid intelligent system – Intelligent agent – Agent architecture – Cognitive architecture – Intelligent control – Knowledge management – Concept mining – Data mining – Text mining – E-mail spam filtering – Information extraction – Activity recognition – Image retrieval – Automatic image annotation – Named-entity extraction – Coreference resolution – Named-entity recognition – Relationship extraction – Terminology extraction – Knowledge representation – Semantic Web – Machine learning – Constrained Conditional Models – Deep learning – Neural modeling fields – Natural language processing – Chatterbots – Language identification – Natural language user interface – Natural language understanding – Machine translation – Statistical semantics – Question answering – Semantic translation – Nonlinear control – Pattern recognition – Optical character recognition – Handwriting recognition – Speech recognition – Face recognition – Robotics – Behavior-based robotics – Cognitive – Cybernetics – Developmental robotics – Epigenetic robotics – Evolutionary robotics – Speech generating device – Strategic planning – Vehicle infrastructure integration – Virtual Intelligence – Virtual reality – == Further AI design elements == Action selection – Affective computing – AI box – AI-complete – Algorithmic probability – Automated reasoning – Autonomic Computing – Autonomic Networking – Backward chaining – Bayesian network – Bio-inspired computing – Artificial immune systems – Blackboard system – Chatterbot – Combs method – Commonsense reasoning – Computational humor – Computer-assisted proof – Conceptual dependency theory – Darwin machine – Description logic – Frame problem – Game theory – Grammar systems theory – Informatics (academic field) – Intelligent control – Kinect – LIDA (cognitive architecture) – Means-ends analysis – Moravec's paradox – Music and artificial intelligence – Ordered weighted averaging aggregation operator – PEAS – Performance, Environment, Actuators, Sensors Percept (artificial intelligence) – Perceptual Computing – Rule-based system – Self-management (computer science) – Soft computing – Software agent – Intelligent agent / Rational agent – Autonomous agent – Automated planning and scheduling Control system Hierarchical control system Networked control system Distributed artificial intelligence – Multi-agent system – Monitoring and Surveillance Agents Embodied agent – Situated AI Sussman Anomaly – Wetware (brain) – == AI projects == List of artificial intelligence projects Automated Mathematician (1977) – Allen (robot) (late 1980s) – Open Mind Common Sense (1999 - ) – Mindpixel (2000–2005) – Cognitive Assistant that Learns and Organizes (2003–2008) – Google DeepMind (2011) – === AI systems === Asimo (2000 to present) – humanoid robot developed by Honda, capable of walking, running, negotiating through pedestrian traffic, climbing and descending stairs, recognizing speech commands and the faces of specific individuals, among a growing set of capabilities. Watson (2011) – computer developed by IBM that played and won the game show Jeopardy! It is now being used to guide nurses in medical procedures. Purpose: Open domain question answering Technologies employed: Natural language processing Information retrieval Knowledge representation Automated reasoning Machine learning === Notable AI software === OpenAIR – OpenCog – OpenIRIS – RapidMiner – == Psychology and AI == Artificial psychology AI effect Uncanny valley == History of artificial intelligence == History of artificial intelligence GOFAI Progress in artificial intelligence Timeline of artificial intelligence History of natural language processing History of optical character recognition AI effect – as soon as AI successfully solves a problem, the problem is no longer considered by the public to be a part of AI. This phenomenon has occurred in relation to every AI application produced throughout the history of development of AI. AI winter – Moore's Law – observation that, over the history of computing hardware, the number of transistors in a dense integrated circuit has doubled approximately every two years. One way this relates to AI is that hypothetically a computer would need at least as much capacity as a human brain to be able to be programmed to be as smart as a human. So as long as the aforementioned rate of development met or beat the 2-year doubling time, one could roughly forecast when a computer would have as much memory and calculation capacity as a human brain, a milestone which was reached in 2010. Though it may take as much as 3 magnitudes (1000 times) more computer capacity (since computers calculate things in a much more linear fashion) to emulate the massively parallel structure of the human brain. At a doubling time of 2 years, an increase in capacity by 1000-fold would take a little less than 18 years (9 doublings), if reaching the limit of integrated circuit technology did not pose an obstacle before then. == AI safety == Existential risk from advanced artificial intelligence Friendly AI – hypothetical AI that is designed not to harm humans and to prevent unfriendly AI from being developed AI takeover – point at which humans are no longer the dominant form of intelligence on Earth and machine intelligence is Superintelligence – AI may grow to such an advanced state to become as proportionately superior to humans as humans are to ants. Theoretically, there would be little humans could do to prevent such an intelligence from reaching its goals. Intelligence explosion – through recursive self-improvement and self-replication, the magnitude of intelligent machinery could surpass human ability to resist it. Artificial intelligence as a global catastrophic risk == AI and the future == Artificial general intelligence (Strong AI) – hypothetical artificial intelligence that matches or exceeds human intelligence — an intelligent machine that could perform intellectual tasks at least as well as a human Aspects or features Self-replicating machines – smart computers and robots would be able to make more of themselves, in a geometric progression or via mass production. Or smart programs may be uploaded into hardware existing at the time (because linear architecture of sufficient speeds could be used to emulate massively parallel analog systems such as human brains). Recursive self improvement (aka seed AI) – speculative ability of strong artificial intelligence to reprogram itself to make itself even more intelligent. The more intelligent it got, the more capable it would be of further improving itself, in successively more rapid iterations, potentially resulting in an intelligence explosion leading to the emergence of a superintelligence. Hive mind – Robot swarm – Technological singularity – the development of strong AI may cause an intelligence explosion in which greater-than-human intelligence emerges, radically changing civilization, and perhaps even human nature. The TS has been identified by Berglas (2012) and others to be an existential risk. Singularitarianism – Human enhancement – humans may be enhanced, either by the efforts of AI or by merging with it. Transhumanism – philosophy of human transformation Posthumanism – people may survive, but not be recognizable in comparison to present modern-day humans. Cyborgs – Mind uploading – == Philosophy of artificial intelligence == Philosophy of artificial intelligence Artificial brain – Philosophical views of artificial consciousness – User illusion – Artificial intelligence and law – Chinese room – Cognitive science Artificial consciousness Embodied cognitive science Embodied cognition – Ethics of artificial intelligence – Philosophy of the Mind – Computational theory of mind – Functionalism – Physical symbol system – Synthetic intelligence – Transhumanism – See Artificial intelligence and the future, below Turing Test – === Artificial intelligence debate === ==== Supporters of AI ==== ==== Critics of AI ==== Stephen Hawking – AI "could spell end of human race""Hawking warns AI 'could spell end of human race'". http://phys.org. Phys.org. 3 December 2014. Retrieved 20 April 2015. == Artificial intelligence in fiction == Artificial intelligence in fiction – Some examples of artificially intelligent entities depicted in science fiction include: AC created by merging 2 AIs in the Sprawl trilogy by William Gibson Agents in the simulated reality known as "The Matrix" in The Matrix franchise Agent Smith, began as an Agent in The Matrix, then became 
173:0:Category utility is a measure of "category goodness" defined in Gluck & Corter (1985) and Corter & Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as "cue validity" (Reed 1972; Rosch & Mervis 1975) and "collocation index" (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten & Frank (2005, pp. 260–262). == Probability-theoretic definition of Category Utility == The probability-theoretic definition of category utility given in Fisher (1987) and Witten & Frank (2005) is as follows: where is a size- set of -ary features, and is a set of categories. The term designates the marginal probability that feature takes on value , and the term designates the category-conditional probability that feature takes on value given that the object in question belongs to category . The motivation and development of this expression for category utility, and the role of the multiplicand as a crude overfitting control, is given in the above sources. Loosely (Fisher 1987), the term is the expected number of attribute values that can be correctly guessed by an observer using a probability-matching strategy together with knowledge of the category labels, while is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels. Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure. == Information-theoretic definition of the Category Utility == The information-theoretic definition of category utility for a set of entities with size- binary feature set , and a binary category is given in Gluck & Corter (1985) as follows: where is the prior probability of an entity belonging to the positive category (in the absence of any feature information), is the conditional probability of an entity having feature given that the entity belongs to category , is likewise the conditional probability of an entity having feature given that the entity belongs to category , and is the prior probability of an entity possessing feature (in the absence of any category information). The intuition behind the above expression is as follows: The term represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category . Similarly, the term represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category . The sum of these two terms in the brackets is therefore the weighted average of these two costs. The final term, , represents the cost (in bits) of optimally encoding (or transmitting) feature information when no category information is available. The value of the category utility will, in the above formulation, be negative (???). === Category Utility and Mutual Information === It is mentioned in Gluck & Corter (1985) and Corter & Gluck (1992) that the category utility is equivalent to the mutual information. Here we provide a simple demonstration of the nature of this equivalence. Let us assume a set of entities each having the same features, i.e., feature set , with each feature variable having cardinality . That is, each feature has the capacity to adopt any of distinct values (which need not be ordered; all variables can be nominal); for the special case these features would be considered binary, but more generally, for any , the features are simply m-ary. For our purposes, without loss of generality, we can replace feature set with a single aggregate variable that has cardinality , and adopts a unique value corresponding to each feature combination in the Cartesian product . (Ordinality does not matter, because the mutual information is not sensitive to ordinality.) In what follows, a term such as or simply refers to the probability with which adopts the particular value . (Using the aggregate feature variable replaces multiple summations, and simplifies the presentation to follow.) We assume also a single category variable , which has cardinality . This is equivalent to a classification system in which there are non-intersecting categories. In the special case of we have the two-category case discussed above. From the definition of mutual information for discrete variables, the mutual information between the aggregate feature variable and the category variable is given by: where is the prior probability of feature variable adopting value , is the marginal probability of category variable adopting value , and is the joint probability of variables and simultaneously adopting those respective values. In terms of the conditional probabilities this can be re-written (or defined) as If we will rewrite the original definition of the category utility from above, with , we have This equation clearly has the same form as the (blue) equation expressing the mutual information between the feature set and the category variable; the difference is that the sum in the category utility equation runs over independent binary variables , whereas the sum in the mutual information runs over values of the single -ary variable . The two measures are actually equivalent then only when the features , are independent (and assuming that terms in the sum corresponding to are also added). == Insensitivity of category utility to ordinality == Like the mutual information, the category utility is not sensitive to any ordering in the feature or category variable values. That is, as far as the category utility is concerned, the category set {small,medium,large,jumbo} is not qualitatively different from the category set {desk,fish,tree,mop} since the formulation of the category utility does not account for any ordering of the class variable. Similarly, a feature variable adopting values {1,2,3,4,5} is not qualitatively different from a feature variable adopting values {fred,joe,bob,sue,elaine}. As far as the category utility or mutual information are concerned, all category and feature variables are nominal variables. For this reason, category utility does not reflect any gestalt aspects of "category goodness" that might be based on such ordering effects. One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information. == Category "goodness": Models and Philosophy == This section provides some background on the origins of, and need for, formal measures of "category goodness" such as the category utility, and some of the history that lead to the development of this particular metric. === What makes a good category? === At least since the time of Aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals. What kind of entity is a concept such as "horse"? Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use. Does the concept "horse" therefore have an independent existence outside of the mind? If it does, then what is the locus of this independent existence? The question of locus was an important issue on which the classical schools of Plato and Aristotle famously differed. However, they remained in agreement that universals did indeed have a mind-independent existence. There was, therefore, always a fact to the matter about which concepts and universals exist in the world. In the late Middle Ages (perhaps beginning with Occam, although Porphyry also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue began to erode, and it became acceptable among the so-called nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language. On this view of concepts—that they are purely representational constructs—a new question then comes to the fore: Why do we possess one set of concepts rather than another? What makes one set of concepts "good" and another set of concepts "bad"? This is a question that modern philosophers, and subsequently machine learning theorists and cognitive scientists, have struggled with for many decades. === What purpose do concepts serve? === One approach to answering such questions is to investigate the "role" or "purpose" of concepts in cognition. Thus, we ask: What are concepts good for in the first place? The answer provided by Mill (1843/1936, p. 425) and many others is that classification (conception) is a precursor to induction: By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage (Smith & Medin 1981;Harnad 2005). As J.S. Mill puts it (Mill 1843/1936, pp. 466–468), The general problem of classification... [is] to provide that things shall be thought of in such groups, and those groups in such an order, as will best conduce to the remembrance and to the ascertainment of their laws... [and] one of the uses of such a classification that by drawing attention to the properties on which it is founded, and which, if the classification be good, are marks of many others, it facilitates the discovery of those others. From this base, Mill reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of category utility: The ends of scientific classification are best answered when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed. The properties, therefore, according to which objects are classified should, if possible, be those which are causes of many other properties; or, at any rate, which are sure marks of them. One may compare this to the "category utility hypothesis" proposed by Corter & Gluck (1992): "A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category." Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the object's class, and, simultaneously, the object class is maximally informative about the object's features. In other words, a useful classification scheme is one in 
150:419:Plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.
73:857:In artificial intelligence, genetic programming (GP) is a technique whereby computer programs are encoded as a set of genes that are then modified (evolved) using an evolutionary algorithm (often a genetic algorithm). The result is a computer program able to perform well in a predefined task. Often confused to be a kind of genetic algorithm, GP can indeed be seen as an application of genetic algorithms to problems where each individual is a computer program. The methods used to encode a computer program in an artificial chromosome and to evaluate its fitness with respect to the predefined task are central in the GP technique and still the subject of active research.
182:426:The glowworm swarm optimization (GSO) is a swarm intelligence optimization algorithm developed based on the behaviour of glowworms (also known as fireflies or lightning bugs). The behaviour pattern of glowworms which is used for this algorithm is the apparent capability of the glowworms to change the intensity of the luciferin emission and thus appear to glow at different intensities.
181:852:Swarm robotics is a new approach to the coordination of multirobot systems which consist of large numbers of mostly simple physical robots. It is supposed that a desired collective behavior emerges from the interactions between the robots and interactions of robots with the environment. This approach emerged on the field of artificial swarm intelligence, as well as the biological studies of insects, ants and other fields in nature, where swarm behaviour occurs.
60:564:This is a list of genetic algorithm (GA) applications.
96:1401:Fossil Detectives is a 2008 BBC Television documentary series in which presenter Hermione Cockburn travels across Great Britain exploring fossil sites and discovering the latest scientific developments in geology and palaeontology. The show is a spin-off of Coast.
130:491:Deeplearning4j is an open source deep learning library written for Java and the Java Virtual Machine and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, as well as word2vec, doc2vec and GloVe. These algorithms all include distributed parallel versions that integrate with Hadoop and Spark. It is commercially supported by the startup Skymind.
42:637:Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.
50:0:In information science, profiling refers to the process of construction and application of profiles generated by computerized data analysis. This involves the use of algorithms or other mathematical techniques that allow the discovery of patterns or correlations in large quantities of data, aggregated in databases. When these patterns or correlations are used to identify or represent people, they can be called profiles. Other than a discussion of profiling technologies or population profiling, the notion of profiling in this sense is not just about the construction of profiles, but also concerns the application of group profiles to individuals, e. g., in the cases of credit scoring, price discrimination, or identification of security risks (Hildebrandt & Gutwirth 2008) (Elmer 2004). Profiling is not simply a matter of computerized pattern-recognition; it enables refined price-discrimination, targeted servicing, detection of fraud, and extensive social sorting. Real-time machine profiling constitutes the precondition for emerging socio-technical infrastructures envisioned by advocates of ambient intelligence, autonomic computing (Kephart & Chess 2003) and ubiquitous computing (Weiser 1991). One of the most challenging problems of the information society involves dealing with increasing data-overload. With the digitizing of all sorts of content as well as the improvement and drop in cost of recording technologies, the amount of available information has become enormous and increases exponentially. It has thus become important for companies, governments, and individuals to discriminate information from noise, detecting useful or interesting data. The development of profiling technologies must be seen against this background. These technologies are thought to efficiently collect and analyse data in order to find or test knowledge in the form of statistical patterns between data. This process, called Knowledge Discovery in Databases (KDD) (Fayyad, Piatetsky-Shapiro & Smyth 1996), provides the profiler with sets of correlated data usable as "profiles". == The profiling process == The technical process of profiling can be separated in several steps: Preliminary grounding: The profiling process starts with a specification of the applicable problem domain and the identification of the goals of analysis. Data collection: The target dataset or database for analysis is formed by selecting the relevant data in the light of existing domain knowledge and data understanding. Data preparation: The data are preprocessed for removing noise and reducing complexity by eliminating attributes. Data mining: The data are analysed with the algorithm or heuristics developed to suit the data, model and goals. Interpretation: The mined patterns are evaluated on their relevance and validity by specialists and/or professionals in the application domain (e.g. excluding spurious correlations). Application: The constructed profiles are applied, e.g. to categories of persons, to test and fine-tune the algorithms. Institutional decision: The institution decides what actions or policies to apply to groups or individuals whose data match a relevant profile. Data collection, preparation and mining all belong to the phase in which the profile is under construction. However, profiling also refers to the application of profiles, meaning the usage of profiles for the identification or categorization of groups or individual persons. As can be seen in step six (application), the process is circular. There is a feedback loop between the construction and the application of profiles. The interpretation of profiles can lead to the reiterant – possibly real-time – fine-tuning of specific previous steps in the profiling process. The application of profiles to people whose data were not used to construct the profile is based on data matching, which provides new data that allows for further adjustments. The process of profiling is both dynamic and adaptive. A good illustration of the dynamic and adaptive nature of profiling is the Cross-Industry Standard Process for Data Mining (CRISP-DM). == Types of profiling practices == In order to clarify the nature of profiling technologies some crucial distinctions have to be made between different types of profiling practices, apart from the distinction between the construction and the application of profiles. The main distinctions are those between bottom-up and top-down profiling (or supervised and unsupervised learning), and between individual and group profiles. === Supervised and unsupervised learning === Profiles can be classified according to the way they have been generated (Fayyad, Piatetsky-Shapiro & Smyth 1996) (Zarsky 2002-3). On the one hand, profiles can be generated by testing a hypothesized correlation. This is called top-down profiling or supervised learning. This is similar to the methodology of traditional scientific research in that it starts with a hypothesis and consists of testing its validity. The result of this type of profiling is the verification or refutation of the hypothesis. One could also speak of deductive profiling. On the other hand, profiles can be generated by exploring a data base, using the data mining process to detect patterns in the data base that were not previously hypothesized. In a way, this is a matter of generating hypothesis: finding correlations one did not expect or even think of. Once the patterns have been mined, they will enter the loop – described above – and will be tested with the use of new data. This is called unsupervised learning. Two things are important with regard to this distinction. First, unsupervised learning algorithms seem to allow the construction of a new type of knowledge, not based on hypothesis developed by a researcher and not based on causal or motivational relations but exclusively based on stochastical correlations. Second, unsupervised learning algorithms thus seem to allow for an inductive type of knowledge construction that does not require theoretical justification or causal explanation (Custers 2004). Some authors claim that if the application of profiles based on computerized stochastical pattern recognition 'works', i.e. allows for reliable predictions of future behaviours, the theoretical or causal explanation of these patterns does not matter anymore (Anderson 2008). However, the idea that 'blind' algorithms provide reliable information does not imply that the information is neutral. In the process of collecting and aggregating data into a database (the first three steps of the process of profile construction), translations are made from real-life events to machine-readable data. These data are then prepared and cleansed to allow for initial computability. Potential bias will have to be located at these points, as well as in the choice of algorithms that are developed. It is not possible to mine a database for all possible linear and non-linear correlations, meaning that the mathematical techniques developed to search for patterns will be determinate of the patterns that can be found. In the case of machine profiling, potential bias is not informed by common sense prejudice or what psychologists call stereotyping, but by the computer techniques employed in the initial steps of the process. These techniques are mostly invisible for those to whom profiles are applied (because their data match the relevant group profiles). === Individual and group profiles === Profiles must also be classified according to the kind of subject they refer to. This subject can either be an individual or a group of people. When a profile is constructed with the data of a single person, this is called individual profiling (Jaquet-Chiffelle 2008). This kind of profiling is used to discover the particular characteristics of a certain individual, to enable unique identification or the provision of personalized services. However, personalized servicing is most often also based on group profiling, which allows categorisation of a person as a certain type of person, based on the fact that her profile matches with a profile that has been constructed on the basis of massive amounts of data about massive numbers of other people. A group profile can refer to the result of data mining in data sets that refer to an existing community that considers itself as such, like a religious group, a tennis club, a university, a political party etc. In that case it can describe previously unknown patterns of behaviour or other characteristics of such a group (community). A group profile can also refer to a category of people that do not form a community, but are found to share previously unknown patterns of behaviour or other characteristics (Custers 2004). In that case the group profile describes specific behaviours or other characteristics of a category of people, like for instance women with blue eyes and red hair, or adults with relatively short arms and legs. These categories may be found to correlate with health risks, earning capacity, mortality rates, credit risks, etc. If an individual profile is applied to the individual that it was mined from, then that is direct individual profiling. If a group profile is applied to an individual whose data match the profile, then that is indirect individual profiling, because the profile was generated using data of other people. Similarly, if a group profile is applied to the group that it was mined from, then that is direct group profiling (Jaquet-Chiffelle 2008). However, in as far as the application of a group profile to a group implies the application of the group profile to individual members of the group, it makes sense to speak of indirect group profiling, especially if the group profile is non-distributive. === Distributive and non-distributive profiling === Group profiles can also be divided in terms of their distributive character (Vedder 1999). A group profile is distributive when its properties apply equally to all the members of its group: all bachelors are unmarried, or all persons with a specific gene have 80% chance to contract a specific disease. A profile is non-distributive when the profile does not necessarily apply to all the members of the group: the group of persons with a specific postal code have an average earning capacity of XX, or the category of persons with blue eyes has an average chance of 37% to contract a specific disease. Note that in this case the chance of an individual to have a particular earning capacity or to contract the specific disease will depend on other factors, e.g. sex, age, background of parents, previous health, education. It should be obvious that, apart from tautological profiles like that of bachelors, most group profiles generated by means of computer techniques are non-distributive. This has far-reaching implications for the accuracy of indirect individual profiling based on data matching with non-distributive group profiles. Quite apart from the fact that the application of accurate profiles may be unfair or cause undue stigmatisation, most group profiles will not be accurate. == Application domains == Profiling technologies can be applied in a variety of different domains and for a variety of purposes. These profiling practices will all have different effect and raise different issues. Knowledge about the behaviour and preferences of customers is of great interest to the commercial sector. On the basis of profiling technologies, companies can predict the behaviour of different types of customers. Marketing strategies can then be tailored to the people fitting these types. Examples of profiling practices in marketing are customers loyalty cards, customer relationship management in general, and personalized 
164:783:A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm.
110:774:Clustering is the problem of partitioning data points into groups based on their similarity. Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.
86:450:Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.
124:693:Google DeepMind is a British artificial intelligence company founded in September 2010 as DeepMind Technologies. It was renamed when it was acquired by Google in 2014. The company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. The company made headlines in 2016 after its AlphaGo program beat a human professional Go player for the first time.
74:324:In radio communications, an evolved antenna is an antenna designed fully or substantially by an automatic computer design program that uses an evolutionary algorithm that mimics Darwinian evolution. This sophisticated procedure has been used in recent years to design a few antennas for mission-critical applications involving stringent, conflicting, or unusual design requirements, such as unusual radiation patterns, for which none of the many existing antenna types are adequate.
168:1009:In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.
154:804:Judea Pearl (born 1936) is an Israeli-born American computer scientist and philosopher, best known for championing the probabilistic approach to artificial intelligence and the development of Bayesian networks (see the article on belief propagation). He is also credited for developing a theory of causal and counterfactual inference based on structural models (see article on causality). He is the 2011 winner of the ACM Turing Award, the highest distinction in computer science, "for fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning".
140:242:Instantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization. This training can be done in a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use unary coding for an effective representation of the data sets.
88:887:Peter Nordin is a Swedish computer scientist, entrepreneur and author who has contributed to artificial intelligence, automatic programming, machine learning, and evolutionary robotics.
16:880:Evolutionary robotics (ER) is a methodology that uses evolutionary computation to develop controllers for autonomous robots. Algorithms in ER frequently operate on populations of candidate controllers, initially selected from some distribution. This population is then repeatedly modified according to a fitness function. In the case of genetic algorithms (or "GAs"), a common method in evolutionary computation, the population of candidate controllers is repeatedly grown according to crossover, mutation and other GA operators and then culled according to the fitness function. The candidate controllers used in ER applications may be drawn from some subset of the set of artificial neural networks, although some applications (including SAMUEL, developed at the Naval Center for Applied Research in Artificial Intelligence) use collections of "IF THEN ELSE" rules as the constituent parts of an individual controller. It is theoretically possible to use any set of symbolic formulations of a control law (sometimes called a policy in the machine learning community) as the space of possible candidate controllers. Artificial neural networks can also be used for robot learning outside of the context of evolutionary robotics. In particular, other forms of reinforcement learning can be used for learning robot controllers.
70:0:There are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation). Some ANNs are adaptive systems and are used for example to model populations and environments, which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms. == Feedforward neural network == The feedforward neural network was the first and arguably most simple type of artificial neural network devised. In this network the information moves in only one direction — forward: From the input nodes data goes through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. Feedforward networks can be constructed from different types of units, e.g. binary McCulloch-Pitts neurons, the simplest example being the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation of error. == Radial basis function (RBF) network == Radial basis functions are powerful techniques for interpolation in multidimensional space. A RBF is a function which has built into it a distance criterion with respect to a center. Radial basis functions have been applied in the area of neural networks where they may be used as a replacement for the sigmoidal hidden layer transfer characteristic in multi-layer perceptrons. RBF networks have two layers of processing: In the first, input is mapped onto each RBF in the 'hidden' layer. The RBF chosen is usually a Gaussian. In regression problems the output layer is then a linear combination of hidden layer values representing mean predicted output. The interpretation of this output layer value is the same as a regression model in statistics. In classification problems the output layer is typically a sigmoid function of a linear combination of hidden layer values, representing a posterior probability. Performance in both cases is often improved by shrinkage techniques, known as ridge regression in classical statistics and known to correspond to a prior belief in small parameter values (and therefore smooth output functions) in a Bayesian framework. RBF networks have the advantage of not suffering from local minima in the same way as Multi-Layer Perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the error surface is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one matrix operation. In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares. RBF networks have the disadvantage of requiring good coverage of the input space by radial basis functions. RBF centres are determined with reference to the distribution of the input data, but without reference to the prediction task. As a result, representational resources may be wasted on areas of the input space that are irrelevant to the learning task. A common solution is to associate each data point with its own centre, although this can make the linear system to be solved in the final layer rather large, and requires shrinkage techniques to avoid overfitting. Associating each input datum with an RBF leads naturally to kernel methods such as support vector machines and Gaussian processes (the RBF is the kernel function). All three approaches use a non-linear kernel function to project the input data into a space where the learning problem can be solved using a linear model. Like Gaussian Processes, and unlike SVMs, RBF networks are typically trained in a Maximum Likelihood framework by maximizing the probability (minimizing the error) of the data under the model. SVMs take a different approach to avoiding overfitting by maximizing instead a margin. RBF networks are outperformed in most classification applications by SVMs. In regression applications they can be competitive when the dimensionality of the input space is relatively small. === How RBF networks work === Although the implementation is very different, RBF neural networks are conceptually similar to K-Nearest Neighbor (k-NN) models. The basic idea is that a predicted target value of an item is likely to be about the same as other items that have close values of the predictor variables. Consider this figure: Assume that each case in the training set has two predictor variables, x and y. The cases are plotted using their x,y coordinates as shown in the figure. Also assume that the target variable has two categories, positive which is denoted by a square and negative which is denoted by a dash. Now, suppose we are trying to predict the value of a new case represented by the triangle with predictor values x=6, y=5.1. Should we predict the target as positive or negative? Notice that the triangle is position almost exactly on top of a dash representing a negative value. But that dash is in a fairly unusual position compared to the other dashes which are clustered below the squares and left of center. So it could be that the underlying negative value is an odd case. The nearest neighbor classification performed for this example depends on how many neighboring points are considered. If 1-NN is used and only the closest point is considered, then clearly the new point should be classified as negative since it is on top of a known negative point. On the other hand, if 9-NN classification is used and the closest 9 points are considered, then the effect of the surrounding 8 positive points may overbalance the close negative point. An RBF network positions one or more RBF neurons in the space described by the predictor variables (x,y in this example). This space has as many dimensions as there are predictor variables. The Euclidean distance is computed from the point being evaluated (e.g., the triangle in this figure) to the center of each neuron, and a radial basis function (RBF) (also called a kernel function) is applied to the distance to compute the weight (influence) for each neuron. The radial basis function is so named because the radius distance is the argument to the function. Weight = RBF(distance) The further a neuron is from the point being evaluated, the less influence it has. ==== Radial Basis Function ==== Different types of radial basis functions could be used, but the most common is the Gaussian function: If there is more than one predictor variable, then the RBF function has as many dimensions as there are variables. The following picture illustrates three neurons in a space with two predictor variables, X and Y. Z is the value coming out of the RBF functions: The best predicted value for the new point is found by summing the output values of the RBF functions multiplied by weights computed for each neuron. The radial basis function for a neuron has a center and a radius (also called a spread). The radius may be different for each neuron, and, in RBF networks generated by DTREG, the radius may be different in each dimension. With larger spread, neurons at a distance from a point have a greater influence. ==== RBF Network Architecture ==== RBF networks have three layers: Input layer: There is one neuron in the input layer for each predictor variable. In the case of categorical variables, N-1 neurons are used where N is the number of categories. The input neurons (or processing before the input layer) standardizes the range of the values by subtracting the median and dividing by the interquartile range. The input neurons then feed the values to each of the neurons in the hidden layer. Hidden layer: This layer has a variable number of neurons (the optimal number is determined by the training process). Each neuron consists of a radial basis function centered on a point with as many dimensions as there are predictor variables. The spread (radius) of the RBF function may be different for each dimension. The centers and spreads are determined by the training process. When presented with the x vector of input values from the input layer, a hidden neuron computes the Euclidean distance of the test case from the neuron’s center point and then applies the RBF kernel function to this distance using the spread values. The resulting value is passed to the summation layer. Summation layer: The value coming out of a neuron in the hidden layer is multiplied by a weight associated with the neuron (W1, W2, ...,Wn in this figure) and passed to the summation which adds up the weighted values and presents this sum as the output of the network. Not shown in this figure is a bias value of 1.0 that is multiplied by a weight W0 and fed into the summation layer. For classification problems, there is one output (and a separate set of weights and summation unit) for each target category. The value output for a category is the probability that the case being evaluated has that category. ==== Training RBF Networks ==== The following parameters are determined by the training process: The number of neurons in the hidden layer The coordinates of the center of each hidden-layer RBF function The radius (spread) of each RBF function in each dimension The weights applied to the RBF function outputs as they are passed to the summation layer Various methods have been used to train RBF networks. One approach first uses K-means clustering to find cluster centers which are then used as the centers for the RBF functions. However, K-means clustering is a computationally intensive procedure, and it often does not generate the optimal number of centers. Another approach is to use a random subset of the training points as the centers. DTREG uses a training algorithm developed by Sheng Chen, Xia Hong and Chris J. Harris. This algorithm uses an evolutionary approach to determine the optimal center points and spreads for each neuron. It also determines when to stop adding neurons to the network by monitoring the estimated leave-one-out (LOO) error and terminating when the LOO error begins to increase because of overfitting. The computation of the optimal weights between the neurons in the hidden layer and the summation layer is done using ridge regression. An iterative procedure developed by Mark Orr (Orr, 1966) is used to compute 
163:0:Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a finite set of values are called classification trees. In these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data but not decisions; rather the resulting classification tree can be an input for decision making. This page deals with decision trees in data mining. == General == Decision tree learning is a method commonly used in data mining. The goal is to create a model that predicts the value of a target variable based on several input variables. An example is shown below. Each interior node corresponds to one of the input variables; there are edges to children for each of the possible values of that input variable. Each leaf represents a value of the target variable given the values of the input variables represented by the path from the root to the leaf. A decision tree is a simple representation for classifying examples. For this section, assume that all of the features have finite discrete domains, and there is a single target feature called the classification. Each element of the domain of the classification is called a class. A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with a feature are labeled with each of the possible values of the feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes. A tree can be "learned" by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT) is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data. In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorisation and generalisation of a given set of data. Data comes in records of the form: The dependent variable, Y, is the target variable that we are trying to understand, classify or generalize. The vector x is composed of the input variables, x1, x2, x3 etc., that are used for that task. == Types == Decision trees used in data mining are of two main types: Classification tree analysis is when the predicted outcome is the class to which the data belongs. Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital). The term Classification And Regression Tree (CART) analysis is an umbrella term used to refer to both of the above procedures, first introduced by Breiman et al. Trees used for regression and trees used for classification have some similarities - but also some differences, such as the procedure used to determine where to split. Some techniques, often called ensemble methods, construct more than one decision tree: Bagging decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction. A Random Forest classifier uses a number of decision trees, in order to improve the classification rate. Boosted Trees can be used for regression-type and classification-type problems. Rotation forest - in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features. A special case of a decision tree is a Decision list, which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node). While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity, permit non-greedy learning methods and monotonic constraints to be imposed. Decision tree learning is the construction of a decision tree from class-labeled training tuples. A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node. There are many specific decision-tree algorithms. Notable ones include: ID3 (Iterative Dichotomiser 3) C4.5 (successor of ID3) CART (Classification And Regression Tree) CHAID (CHi-squared Automatic Interaction Detector). Performs multi-level splits when computing classification trees. MARS: extends decision trees to handle numerical data better. Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning. ID3 and CART were invented independently at around the same time (between 1970 and 1980), yet follow a similar approach for learning decision tree from training tuples. == Metrics == Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items. Different algorithms use different metrics for measuring "best". These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split. === Gini impurity === Used by the CART (classification and regression tree) algorithm, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. Gini impurity can be computed by summing the probability of each item being chosen times the probability of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category. To compute Gini impurity for a set of items, suppose , and let be the fraction of items labeled with value in the set. === Information gain === Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy from information theory. Entropy is defined as below Information Gain = Entropy(parent) - Weighted Sum of Entropy(Children) === Variance reduction === Introduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable x due to the split at this node: where , , and are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean. == Decision tree advantages == Amongst other data mining methods, decision trees have various advantages: Simple to understand and interpret. People are able to understand decision tree models after a brief explanation. Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. (For example, relation rules can be used only with nominal variables while neural networks can be used only with numerical variables.) Uses a white box model. If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model, the explanation for the results is typically difficult to understand, for example with an artificial neural network. Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model. Robust. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated. Performs well with large datasets. Large amounts of data can be analysed using standard computing resources in reasonable time. == Limitations == The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally-optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally-optimal decision tree. To reduce the greedy effect of local-optimality some methods such as the dual information distance (DID) tree were proposed. [1] Decision-tree learners can create over-complex trees that do not generalise well from the training data. (This is known as overfitting.) Mechanisms such as pruning are necessary to avoid this problem (with the exception of some algorithms such as the Conditional Inference approach, that does not require pruning ). There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. In such cases, the decision tree becomes prohibitively large. Approaches to solve the problem involve either changing the representation of the problem domain (known as propositionalisation) or using learning algorithms based on more expressive representations (such as statistical relational learning or inductive logic programming). For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels. However, the issue of biased predictor selection is avoided by the Conditional Inference approach. == Extensions == === Decision graphs === In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using Minimum message length (MML). Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different 
170:0:AdaBoost, short for "Adaptive Boosting", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (e.g., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner. While every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples. == Overview == Problems in machine learning often suffer from the curse of dimensionality — each sample may consist of a huge number of potential features (for instance, there can be 162,336 Haar features, as used by the Viola–Jones object detection framework, in a 24×24 pixel image window), and evaluating every feature can reduce not only the speed of classifier training and execution, but in fact reduce predictive power, per the Hughes Effect. Unlike neural networks and SVMs, the AdaBoost training process selects only those features known to improve the predictive power of the model, reducing dimensionality and potentially improving execution time as irrelevant features do not need to be computed. === Training === AdaBoost refers to a particular method of training a boosted classifier. A boost classifier is a classifier in the form where each is a weak learner that takes an object as input and returns a real valued result indicating the class of the object. The sign of the weak learner output identifies the predicted object class and the absolute value gives the confidence in that classification. Similarly, the -layer classifier will be positive if the sample is believed to be in the positive class and negative otherwise. Each weak learner produces an output, hypothesis , for each sample in the training set. At each iteration , a weak learner is selected and assigned a coefficient such that the sum training error of the resulting -stage boost classifier is minimized. Here is the boosted classifier that has been built up to the previous stage of training, is some error function and is the weak learner that is being considered for addition to the final classifier. === Weighting === At each iteration of the training process, a weight is assigned to each sample in the training set equal to the current error on that sample. These weights can be used to inform the training of the weak learner, for instance, decision trees can be grown that favor splitting sets of samples with high weights. == Derivation == This derivation follows Rojas (2009): Suppose we have a data set where each item has an associated class , and a set of weak classifiers each of which outputs a classification for each item. After the -th iteration our boosted classifier is a linear combination of the weak classifiers of the form: At the -th iteration we want to extend this to a better boosted classifier by adding a multiple of one of the weak classifiers: So it remains to determine which weak classifier is the best choice for , and what its weight should be. We define the total error of to be the sum of its exponential loss on each data point, given as follows: Letting and for , we have: We can split this summation between those data points that are correctly classified by (so ) and those which are misclassified (so ): Since the only part of the right-hand side of this equation that depends on is , we see that the that minimizes is the one that minimizes , i.e. the weak classifier with the lowest weighted error (with weights ). In order to determine the desired weight that minimizes with the that we just determined, we differentiate: Setting this to zero and solving for yields: We calculate the weighted error rate of the weak classifier to be , so it follows that: which is the negative logit function multiplied by 0.5. Thus we have derived the AdaBoost algorithm: At each iteration, choose the classifier which minimizes the total weighted error , use this to calculate the error rate , use this to calculate the weight , and finally use this to improve the boosted classifier to . == Statistical understanding of boosting == Boosting is a form of linear regression in which the features of each sample are the outputs of some weak learner applied to . Specifically, in the case where all weak learners are known a priori, AdaBoost corresponds to a single iteration of the backfitting algorithm in which the smoothing splines are the minimizers of , that is: fits an exponential cost function and is linear with respect to the observation. Thus, boosting is seen to be a specific type of linear regression. While regression tries to fit to as precisely as possible without loss of generalization, typically using least square error , the AdaBoost error function takes into account the fact that only the sign of the final result will be used, thus can be far larger than 1 without increasing error. However, the exponential increase in the error for sample as increases results in excessive weight being assigned to outliers. One feature of the choice of exponential error function is that the error of the final additive model is the product of the error of each stage, that is, . Thus it can be seen that the weight update in the AdaBoost algorithm is equivalent to recalculating the error on after each stage. There is a lot of flexibility allowed in the choice of loss function. As long as the loss function is monotonic and continuously differentiable, the classifier will always be driven toward purer solutions. Zhang (2004) provides a loss function based on least squares, a modified Huber loss function: This function is more well-behaved than LogitBoost for close to 1 or -1, does not penalise ‘overconfident’ predictions (), unlike unmodified least squares, and only penalises samples misclassified with confidence greater than 1 linearly, as opposed to quadratically or exponentially, and is thus less susceptible to the effects of outliers. == Boosting as gradient descent == Boosting can be seen as minimization of a convex loss function over a convex set of functions. Specifically, the loss being minimized by AdaBoost is the exponential loss , whereas LogitBoost performs logistic regression, minimizing . In the gradient descent analogy, the output of the classifier for each training point is considered to be a point in n-dimensional space, where each axis corresponds to a training sample, each weak learner corresponds to a vector of fixed orientation and length, and the goal is to reach the target point (or any region where the value of loss function is less than the value at that point), in the least number of steps. Thus AdaBoost algorithms perform either Cauchy (find with the steepest gradient, choose to minimize test error) or Newton (choose some target point, find that will bring closest to that point) optimization of training error. == Example algorithm (Discrete AdaBoost) == With: Samples Desired outputs Initial weights set to Error function Weak learners For in : Choose : Find weak learner that minimizes , the weighted sum error for misclassified points Choose Add to ensemble: Update weights: for all i Renormalize such that (Note: It can be shown that at every step, which can simplify the calculation of the new weights.) === Choosing αt === is chosen as it can be analytically shown to be the minimizer of the exponential error function for Discrete AdaBoost. Minimize: Using the convexity of the exponential function, and assuming that we have: We then differentiate that expression with respect to and set it to zero to find the minimum of the upper bound: Note that this only applies when , though it can be a good starting guess in other cases, such as when the weak learner is biased (), has multiple leaves () or is some other function . In such cases the choice of weak learner and coefficient can be condensed to a single step in which is chosen from all possible as the minimizer of by some numerical searching routine. == Variants == === Real AdaBoost === The output of decision trees is a class probability estimate , the probability that is in the positive class. Friedman, Hastie and Tibshirani derive an analytical minimizer for for some fixed (typically chosen using weighted least squares error): . Thus, rather than multiplying the output of the entire tree by some fixed value, each leaf node is changed to output half the logit transform of its previous value. === LogitBoost === LogitBoost represents an application of established logistic regression techniques to the AdaBoost method. Rather than minimizing error with respect to y, weak learners are chosen to minimize the (weighted least-squares) error of with respect to , where , and . That is is the Newton-Raphson approximation of the minimizer of the log-likelihood error at stage , and the weak learner is chosen as the learner that best approximates by weighted least squares. As p approaches either 1 or 0, the value of becomes very small and the z term, which will be large for misclassified samples, can become numerically unstable, due to machine precision rounding errors. This can be overcome by enforcing some limit on the absolute value of z and the minimum value of w. === Gentle AdaBoost === While previous boosting algorithms choose greedily, minimizing the overall test error as much as possible at each step GentleBoost features a bounded step size. is chosen to minimize , and no further coefficient is applied. Thus, in the case where a weak learner exhibits perfect classification performance, GentleBoost will choose exactly equal to , while steepest descent algorithms will try to set . Empirical observations about the good performance 
115:280:In computer science, constrained clustering is a class of semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should be associated with the same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of constraint violation should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions. 
196:568:Neuronal tuning refers to the property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Neuronal responses are optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1), or weak and broad, as observed in neural ensembles. Single neurons may be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons that are tuned to different signals often integrate information from the different sources. In neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems.
92:212:Phil Husbands (born 27 June, 1961) is a professor of computer science and artificial intelligence at the English University of Sussex, situated next to the East Sussex village of Falmer, within the city of Brighton and Hove. He is head of the Evolutionary and Adaptive Systems group and co-director of the Centre for Computational Neuroscience and Robotics (CCNR). Husbands is also one of the founders of the field of evolutionary robotics.
104:601:Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering.
14:612:Interactive evolutionary computation (IEC) or aesthetic selection is a general term for methods of evolutionary computation that use human evaluation. Usually human evaluation is necessary when the form of fitness function is not known (for example, visual appeal or attractiveness; as in Dawkins, 1986) or the result of optimization should fit a particular user preference (for example, taste of coffee or color set of the user interface).
171:1143:Nonparametric regression is a form of regression analysis in which the predictor does not take a predetermined form but is constructed according to information derived from the data. Nonparametric regression requires larger sample sizes than regression based on parametric models because the data must supply the model structure as well as the model estimates.
125:339:Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).
143:247:A Dynamic Bayesian Network (DBN) is a Bayesian network which relates variables to each other over adjacent time steps. This is often called a Two-Timeslice BN (2TBN) because it says that at any point in time T, the value of a variable can be calculated from the internal regressors and the immediate prior value (time T-1). DBNs were developed by Paul Dagum in the early 1990s when he led research funded by two National Science Foundation grants at Stanford University's Section on Medical Informatics. Dagum developed DBNs to unify and extend traditional linear state-space models such as Kalman filters, linear and normal forecasting models such as ARMA and simple dependency models such as hidden Markov models into a general probabilistic representation and inference mechanism for arbitrary nonlinear and non-normal time-dependent domains.
123:883:Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be "a combination of Monte Carlo ideas and dynamic programming (DP) ideas." TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning.
62:0:The Atari 2600 (or Atari VCS before 1982) is a home video game console released on September 11, 1977 by Atari, Inc. It is credited with popularizing the use of microprocessor-based hardware and ROM cartridges containing game code, a format first used with the Fairchild Channel F video game console in 1976. This format contrasts with the older model of having non-microprocessor dedicated hardware, which could only play the games which were physically built into the unit. The console was originally sold as the Atari VCS, an abbreviation for Video Computer System. Following the release of the Atari 5200 in 1982, the VCS was renamed to the "Atari 2600", after the unit's Atari part number, CX2600. The 2600 was typically bundled with two joystick controllers, a conjoined pair of paddle controllers, and a game cartridge, initially Combat, and later Pac-Man. == History == Ted Dabney and Nolan Bushnell developed the Atari gaming system in the 1970s. Originally operating under the name "Syzygy", Bushnell and Dabney changed the name of their company to "Atari" in 1972. In 1973, Atari Inc. had purchased an engineering think tank called Cyan Engineering to research next-generation video game systems, and had been working on a prototype known as "Stella" (named after one of the engineers' bicycles) for some time. Unlike prior generations of machines that use custom logic to play a small number of games, its core is a complete CPU, the famous MOS Technology 6502 in a cost-reduced version known as the 6507. It was combined with a RAM-and-I/O chip, the MOS Technology 6532, and a display and sound chip known as the Television Interface Adaptor (TIA). The first two versions of the machine contain a fourth chip, a standard CMOS logic buffer IC, making Stella cost-effective. Some later versions of the console eliminated the buffer chip. Programs for small computers of the time were generally stored on cassette tapes, floppy disks, or paper tape. By the early 1970s, Hewlett-Packard manufactured desktop computers costing thousands of dollars such as the HP 9830, which packaged Read Only Memory (ROM) into removable cartridges to add special programming features, and these were being considered for use in games. At first, the design was not going to be cartridge-based, but after seeing a "fake" cartridge system on another machine, they realized they could place the games on cartridges essentially for the price of the connector and packaging. In 1976, Fairchild Semiconductor released their own CPU-based system, the Video Entertainment System. Stella was still not ready for production, but it was clear that it needed to be before there were a number of "me too" products filling up the market, which had happened after they released Pong. Atari Inc. didn't have the cash flow to complete the system quickly, given that sales of their Pong systems were cooling. Nolan Bushnell eventually turned to Warner Communications, and sold the company to them in 1976 for US$28 million on the promise that Stella would be produced as soon as possible. Key to the eventual success of the machine was the hiring of Jay Miner, a chip designer who managed to squeeze an entire wire wrap of equipment making up the TIA into a single chip. Once that was completed and debugged, the system was ready for shipping. === Launch and success === The unit was originally priced at US$199 ($777 adjusted for inflation), and shipped with two joysticks and a Combat cartridge (eight additional games were available at launch and sold separately). In a move to compete directly with the Channel F, Atari Inc. named the machine the Video Computer System (or VCS for short), as the Channel F was at that point known as the VES, for Video Entertainment System. The VCS was also rebadged as the Sears Video Arcade and sold through Sears, Roebuck and Company stores. Another break-through for gaming systems was Atari's invention of a computer-controlled opponent, rather than the usual two-player or asymmetric challenges of the past. When Fairchild learned of Atari Inc.'s naming, they quickly changed the name of their system to become the Channel F. However, both systems were now in the midst of a vicious round of price-cutting: Pong clones that had been made obsolete by these newer and more powerful machines were sold off to discounters for ever-lower prices. Soon many of the clone companies were out of business, and both Fairchild and Atari Inc. were selling to a public that was completely burnt out on Pong. In 1977, Atari Inc. sold 250,000 Video Computer Systems. For the first year of production, the Video Computer System was manufactured in Sunnyvale, California. The consoles manufactured there had thick plastic molding around the sides and bottom. These added weight to the console, and because all six switches were on the front, these consoles were nicknamed "Heavy Sixers". After this first year, production moved to Hong Kong, and the consoles manufactured there had thinner plastic molding. In 1978, only 550,000 units from a production run of 800,000 were sold, requiring further financial support from Warner to cover losses. This led directly to the disagreements that caused Atari Inc. founder Nolan Bushnell to leave the company in 1978. Despite Bushnell's retirement in 1978, Warren Robinett’s invention of the first graphical adventure game, Adventure, was developed the same year and changed the fundamentals of gaming as it unlocked a game with a “virtual space bigger than the screen." Once the public realized it was possible to play video games other than Pong, and programmers learned how to push its hardware's capabilities, the VCS gained popularity. By this point, Fairchild had given up, thinking video games were a passing fad, thereby handing the entire quickly growing market to Atari Inc. By 1979, the VCS was the best-selling Christmas gift (and console), due to its exclusive content, and 1 million units were sold that year. Atari Inc. then licensed the smash arcade hit Space Invaders by Taito, which greatly increased the unit's popularity when it was released in January 1980, doubling sales to over 2 million units. The VCS and its cartridges were the main factor behind Atari Inc. grossing more than $2 billion in 1980. Sales then doubled again for the next two years; by 1982, the console had sold 10 million units, while its best-selling game Pac-Man sold 7 million copies. The console also sold 450,000 units in West Germany by 1984. By 1982 the 2600 console cost Atari about $40 to make and was sold for an average of $125. The company spent $4.50 to $6 to manufacture each cartridge and $1 to $2 for advertising, and sold it for $18.95 wholesale. In 1980, the VCS was given a minor revision in which the left and right difficulty switches were moved to the back of the console, leaving four switches on the front. Other than this, these four-switch consoles looked nearly identical to the earlier six-switch models. In 1982, another version of the four-switch console was released without woodgrain. They were nicknamed "Darth Vader" consoles due to their all-black appearance. These were also the first consoles to be officially called "Atari 2600", as the Atari 5200 was released the same year. During this period, Atari Inc. expanded the 2600 family with two other compatible consoles. Despite the faux-wood panels and what would now appear to be primitive graphics, the game console became widely popular for the time. Later however, they designed the Atari 2700, a wireless version of the console that was never released because of a design flaw. The company also built a sleeker version of the machine dubbed the Atari 2800 to sell directly to the Japanese market in early 1983, but it suffered from competition with the newly released Nintendo Famicom. In a survey mentioned by Jeff Rovin it is reported that more stores reported breakdowns of the Atari 2600 system than any other, and that Atari repair centers seemed to have the most trouble with consoles manufactured in 1980. In one case it is stated that a system was repaired five times before static electricity from a carpet was discovered as having caused the problem. The controllers were also a source of breakage because of the way they could be gripped by a player holding it with their fist, allowing players to get carried away and over control, which was less likely with other systems released at the time, such as the Magnavox Odyssey², which has controllers that are nearly half its size. === Sears Tele-Games 2600s === Atari Inc. also continued their OEM relationship with Sears under the latter's Tele-Games brand label, which started in 1975 with the original Pong. Sears released several versions of the 2600 as the Sears Video Arcade series from 1977 to 1983. These include the Rev. A "Heavy Sixer" model in 1977, the Rev. B "4 switch" model in 1980, and the US version of the Atari 2800 branded as the Sears Video Arcade II in 1983. Sears also released their own versions of Atari Inc.'s games under the Tele-Games brand — often with different titles — which included the Tele-Games branded variations of text and picture labels. Three games were also produced by Atari Inc. for Sears as exclusive releases under the Tele-Games brand: Steeplechase, Stellar Track, and Submarine Commander. Sears's Tele-Games brand was unrelated to the company Telegames, which also produced cartridges for the Atari 2600 — mostly re-issues of M Network games. === Decline and remodel === During the 1970s, Atari Inc. continued to grow until it had one of the largest R&D divisions in Silicon Valley. However, it spent much of its R&D budget on projects that seemed out of place at a video game (or even home computer) company; many of these projects never saw the light of day. Meanwhile, several attempts to bring out newer consoles failed for one reason or another, although Atari Inc.'s home computer system (the Atari 8-bit family) sold reasonably well, Warner was pleased as it seemed to have no end to the sales of the 2600, and Atari Inc. was responsible for over half of the company's income. The programmers of many of Atari Inc.'s biggest hits grew disgruntled with the company for not crediting game developers and many left the company and formed their own independent software companies. The most prominent and longest-lasting of these third-party developers was Activision, founded in 1980, whose titles quickly became more popular than those of Atari Inc. itself. Atari Inc. attempted to block third-party development for the 2600 in court but failed, and soon other publishers, such as Imagic and Coleco, entered the market. Atari Inc. suffered from an image problem when a company named Mystique produced a number of pornographic games for the 2600. The most notorious of these, Custer's Revenge, was protested by women's and Native 
45:503:Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network. It is well suited to finding clusters within data.
76:305:Evolutionary data mining, or genetic data mining is an umbrella term for any data mining using evolutionary algorithms. While it can be used for mining data from DNA sequences, it is not limited to biological contexts and can be used in any classification-based prediction scenario, which helps "predict the value ... of a user-specified goal attribute based on the values of other attributes." For instance, a banking institution might want to predict whether a customer's credit would be "good" or "bad" based on their age, income and current savings. Evolutionary algorithms for data mining work by creating a series of random rules to be checked against a training dataset. The rules which most closely fit the data are selected and are mutated. The process is iterated many times and eventually, a rule will arise that approaches 100% similarity with the training data. This rule is then checked against a test dataset, which was previously invisible to the genetic algorithm.
103:392:Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.
177:423:In mathematics, a piecewise linear function is a real-valued function defined on the real numbers or a segment thereof, whose graph is composed of straight-line sections. It is a piecewise-defined function, each of whose pieces is an affine function.
161:0:The Nintendo Entertainment System (also abbreviated as NES) is an 8-bit home video game console that was developed and manufactured by Nintendo. It was initially released in Japan as the Family Computer (Japanese: ファミリーコンピュータ, Hepburn: Famirī Konpyūta) (also known by the portmanteau abbreviation Famicom (ファミコン, Famikon) and abbreviated as FC) on July 15, 1983, and was later released in North America during 1985, in Europe during 1986, and Australia in 1987. In South Korea, it was known as the Hyundai Comboy (현대 컴보이 Hyeondae Keomboi) and was distributed by SK Hynix which then was known as Hyundai Electronics. It was succeeded by the Super Nintendo Entertainment System. The best-selling gaming console of its time, the NES helped revitalize the US video game industry following the video game crash of 1983. With the NES, Nintendo introduced a now-standard business model of licensing third-party developers, authorizing them to produce and distribute titles for Nintendo's platform. In 2009, the Nintendo Entertainment System was named the single greatest video game console in history by IGN, in a list of 25. It was judged the second greatest console behind the Sega Dreamcast in PC Magazine's "Top 10 Video Game Consoles of All Time". == History == === Development === Following a series of arcade game successes in the early 1980s, Nintendo made plans to create a cartridge-based console called the Famicom. Masayuki Uemura designed the system. Original plans called for an advanced 16-bit system which would function as a full-fledged computer with a keyboard and floppy disk drive, but Nintendo president Hiroshi Yamauchi rejected this and instead decided to go for a cheaper, more conventional cartridge-based game console as he felt that features such as keyboards and disks were intimidating to non-technophiles. A test model was constructed in October 1982 to verify the functionality of the hardware, after which work began on programming tools. Because 65xx CPUs had not been manufactured or sold in Japan up to that time, no cross-development software was available and it had to be produced from scratch. Early Famicom games were written on a system that ran on an NEC PC-8001 computer and LEDs on a grid were used with a digitizer to design graphics as no software design tools for this purpose existed at that time. The code name for the project was "GameCom", but Masayuki Uemura's wife proposed the name "Famicom", arguing that "In Japan, 'pasokon' is used to mean a personal computer, but it is neither a home or personal computer. Perhaps we could say it is a family computer." Meanwhile, Hiroshi Yamauchi decided that the console should use a red and white theme after seeing a billboard for DX Antenna which used those colors. Original plans called for the Famicom's cartridges to be the size of a cassette tape, but ultimately they ended up being twice as big. Careful design attention was paid to the cartridge connectors since loose and faulty connections often plagued arcade machines. As it necessitated taking 60 connection lines for the memory and expansion, Nintendo decided to produce their own connectors in-house rather than use ones from an outside supplier. The game pad controllers were more-or-less copied directly from the Game & Watch machines, although the Famicom design team originally wanted to use arcade-style joysticks, even taking apart ones from American game consoles to see how they worked. However, it was eventually decided that children might step on joysticks left on the floor and their durability was also questioned. Katsuyah Nakawaka attached a Game & Watch D-pad to the Famicom prototype and found that it was easy to use and had no discomfort. Ultimately though, they did install a 15-pin expansion port on the front of the console so that an arcade-style joystick could be used optionally. The controllers were hard-wired to the console with no connectors for cost reasons. Uemura added an eject lever to the cartridge slot which was not really necessary, but he felt that children could be entertained by pressing it. He also added a microphone to the second controller with the idea that it could be used to make players' voices sound through the TV speaker. === Release === The console was released on July 15, 1983 as the Family Computer (or Famicom for short) for ¥14,800 alongside three ports of Nintendo's successful arcade games Donkey Kong, Donkey Kong Jr. and Popeye. The Famicom was slow to gather momentum; a bad chip set caused the initial release of the system to crash. Following a product recall and a reissue with a new motherboard, the Famicom’s popularity soared, becoming the best-selling game console in Japan by the end of 1984. Encouraged by these successes, Nintendo soon turned its attention to the North American market. Nintendo entered into negotiations with Atari to release the Famicom under Atari’s name as the name Nintendo Advanced Video Gaming System. The deal was set to be finalized and signed at the Summer Consumer Electronics Show in June 1983. However, Atari discovered at that show that its competitor Coleco was illegally demonstrating its Coleco Adam computer with Nintendo's Donkey Kong game. This violation of Atari's exclusive license with Nintendo to publish the game for its own computer systems delayed the implementation of Nintendo's game console marketing contract with Atari. Atari's CEO Ray Kassar was fired the next month, so the deal went nowhere, and Nintendo decided to market its system on its own. Subsequent plans to market a Famicom console in North America featuring a keyboard, cassette data recorder, wireless joystick controller and a special BASIC cartridge under the name "Nintendo Advanced Video System" likewise never materialized. By the beginning of 1985, the Famicom had sold more than 2.5 million units in Japan and Nintendo soon announced plans to release it in North America as the Advanced Video Entertainment System (AVS) that same year. The American video game press was skeptical that the console could have any success in the region, with the March 1985 issue of Electronic Games magazine stating that "the videogame market in America has virtually disappeared" and that "this could be a miscalculation on Nintendo's part." At June 1985's Consumer Electronics Show (CES), Nintendo unveiled the American version of its Famicom. This is the system which would eventually be officially deployed as the Nintendo Entertainment System, or the colloquial "NES". Nintendo seeded these first systems to limited American test markets starting in New York City on October 18, 1985, following up with a full-fledged North American release of the console in February of the following year. Nintendo released 17 launch titles: 10-Yard Fight, Baseball, Clu Clu Land, Duck Hunt, Excitebike, Golf, Gyromite, Hogan’s Alley, Ice Climber, Kung Fu, Pinball, Soccer, Stack-Up, Tennis, Wild Gunman, Wrecking Crew, and Super Mario Bros. Some varieties of these launch games contained Famicom chips with an adapter inside the cartridge so they would play on North American consoles, which is why the title screen of Gyromite has the Famicom title "Robot Gyro" and the title screen of Stack-Up has the Famicom title "Robot Block". The system was originally targeted for release in the spring of 1985, but the release date was pushed back. After test-marketing in the New York City area in late fall, retailers had reportedly stated the system "failed miserably", while others stated that Nintendo had an excellent nine-week market test in New York last fall. Due to the moderate success launch in New York City, Nintendo tried a second time; the system was test-marketed further beginning in February 1986, with the nationwide release occurring in September 1986. The system's launch represented not only a new product, but also a reframing of the severely damaged home video game market segment as a whole. The video game market crash of 1983 had occurred in significant part due to a lack of consumer and retailer confidence in video games, which had in turn been due partially to confusion and misrepresentation in the marketing of video games. Prior to the NES, the packaging of many video games presented bombastic artwork which exaggerated the graphics of the actual game. In terms of product identity, a single game such as Pac-Man would appear in many versions on many different game consoles and computers, with large variations in graphics, sound, and general quality between the versions. By stark contrast, Nintendo's marketing strategy aimed to regain consumer and retailer confidence, by delivering a singular platform whose technology was not in need of heavy exaggeration and whose qualities were clearly defined. To differentiate Nintendo's new home platform from the early 1980s' common perception of a troubled and shallow video game market, the company freshened its product nomenclature and positioning, and it established a strict product approval and licensing policy. The overall system was referred to as an "Entertainment System" instead of a "video game system", which was centered upon a machine called a "Control Deck" instead of a "console", and which featured software cartridges called "Game Paks" instead of "video games". The 10NES lockout chip system acted as a lock-and-key coupling of each Game Pak and Control Deck, deterring the copying or production of NES games which had not first achieved Nintendo's licensed approval. The packaging of the launch lineup of NES games bore pictures of a very close representation of the actual onscreen graphics of the game, which were of sufficiently recognizable quality on their own. Symbols on the launch games' packaging clearly indicated the genre of the game, in order to reduce consumer confusion. A 'seal of quality' was printed on all appropriately licensed game and accessory packaging. The initial seal stated, "This seal is your assurance that Nintendo has approved and guaranteed the quality of this product". This text was later changed to "Official Nintendo Seal of Quality". Unlike with the Famicom, Nintendo of America marketed the console primarily to children, instituting a rather strict policy of censoring profanity, sexual, religious, or political content in games. The most famous case of this was Lucasfilm's attempts to port Maniac Mansion (a game with a considerable amount of material Nintendo had issues with) to the NES. NOA continued their censorship policy until 1994 with the advent of the Entertainment Software Rating Board system. The optional Robotic Operating Buddy, or R.O.B., was part of a marketing plan to portray the NES's technology as being novel and sophisticated when compared to previous game consoles, and to portray its position as being within reach of the better established toy market. While at first, the American public exhibited limited excitement for the console itself, peripherals such as the light gun and R.O.B. also attracted extensive attention. In Europe and Australia, the system was released to two separate marketing regions. One region consisted of most of mainland Europe (excluding Italy), and distribution there was handled by a number of different companies, with Nintendo responsible for 
36:0:Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning. Pattern recognition systems are in many cases trained from labeled "training" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform "most likely" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms. == Overview == Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances. A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled is the training data. Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in community ecology, the term "classification" is used to refer to what is commonly known as "clustering". The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), ordinal (consisting of one of a set of ordered items, e.g., "large", "medium" or "small"), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10). === Probabilistic classifiers === Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a "best" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms: They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.) Correspondingly, they can abstain when the confidence of choosing any particular output is too low. Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation. === How many feature variables are important? === Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given. The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of features the powerset consisting of all subsets of features need to be explored. The Branch-and-Bound algorithm does reduce this complexity but is intractable for medium to large values of the number of available features . For a large-scale comparison of feature-selection algorithms see . Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features. == Problem statement (supervised version) == Formally, the problem of supervised pattern recognition can be stated as follows: Given an unknown function (the ground truth) that maps input instances to output labels , along with training data assumed to represent accurate examples of the mapping, produce a function that approximates as closely as possible the correct mapping . (For example, if the problem is filtering spam, then is some representation of an email and is either "spam" or "non-spam"). In order for this to be a well-defined problem, "approximates as closely as possible" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function that assigns a specific value to "loss" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of . In practice, neither the distribution of nor the ground truth function are known exactly, but can be computed only empirically by collecting a large number of samples of and hand-labeling them using the correct value of (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a "typical" test set. For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form where the feature vector input is , and the function f is typically parameterized by some parameters . In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability is instead estimated and combined 
118:0:Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). A core body of research on Markov decision processes resulted from Ronald A. Howard's book published in 1960, Dynamic Programming and Markov Processes. They are used in a wide area of disciplines, including robotics, automated control, economics, and manufacturing. More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward . The probability that the process moves into its new state is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state depends on the current state and the decision maker's action . But given and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP process satisfies the Markov property. Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are the same (e.g., zero), a Markov decision process reduces to a Markov chain. == Definition == A Markov decision process is a 5-tuple , where is a finite set of states, is a finite set of actions (alternatively, is the finite set of actions available from state ), is the probability that action in state at time will lead to state at time , is the immediate reward (or expected immediate reward) received after transition to state from state , is the discount factor, which represents the difference in importance between future rewards and present rewards. (Note: The theory of Markov decision processes does not state that or are finite, but the basic algorithms below assume that they are finite.) == Problem == The core problem of MDPs is to find a "policy" for the decision maker: a function that specifies the action that the decision maker will choose when in state . Note that once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a Markov chain. The goal is to choose a policy that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon: (where we choose ) where is the discount factor and satisfies . (For example, when the discount rate is r.) is typically close to 1. Because of the Markov property, the optimal policy for this particular problem can indeed be written as a function of only, as assumed above. == Algorithms == MDPs can be solved by linear programming or dynamic programming. In what follows we present the latter approach. Suppose we know the state transition function and the reward function , and we wish to calculate the policy that maximizes the expected discounted reward. The standard family of algorithms to calculate this optimal policy requires storage for two arrays indexed by state: value , which contains real values, and policy which contains actions. At the end of the algorithm, will contain the solution and will contain the discounted sum of the rewards to be earned (on average) by following that solution from state . The algorithm has the following two kinds of steps, which are repeated in some order for all the states until no further changes take place. They are defined recursively as follows: Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution. === Notable variants === ==== Value iteration ==== In value iteration (Bellman 1957), which is also called backward induction, the function is not used; instead, the value of is calculated within whenever it is needed. Lloyd Shapley's 1953 paper on stochastic games included as a special case the value iteration method for MDPs, but this was recognized only later on. Substituting the calculation of into the calculation of gives the combined step: where is the iteration number. Value iteration starts at and as a guess of the value function. It then iterates, repeatedly computing for all states , until converges with the left-hand side equal to the right-hand side (which is the "Bellman equation" for this problem). ==== Policy iteration ==== In policy iteration (Howard 1960), step one is performed once, and then step two is repeated until it converges. Then step one is again performed once and so on. Instead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. This variant has the advantage that there is a definite stopping condition: when the array does not change in the course of applying step 1 to all states, the algorithm is completed. ==== Modified policy iteration ==== In modified policy iteration (van Nunen, 1976; Puterman and Shin 1978), step one is performed once, and then step two is repeated several times. Then step one is again performed once and so on. ==== Prioritized sweeping ==== In this variant, the steps are preferentially applied to states which are in some way important - whether based on the algorithm (there were large changes in or around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm). == Extensions and generalizations == A Markov decision process is a stochastic game with only one player. === Partial observability === The solution above assumes that the state is known when action is to be taken; otherwise cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP. A major advance in this area was provided by Burnetas and Katehakis in "Optimal adaptive policies for Markov decision processes". In this work a class of adaptive policies that possess uniformly maximum convergence rate properties for the total expected finite horizon reward, were constructed under the assumptions of finite state-action spaces and irreducibility of the transition law. These policies prescribe that the choice of actions, at each state and time period, should be based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. === Reinforcement learning === If the probabilities or rewards are unknown, the problem is one of reinforcement learning (Sutton and Barto, 1998). For this purpose it is useful to define a further function, which corresponds to taking the action and then continuing optimally (or according to whatever policy one currently has): While this function is also unknown, experience during learning is based on pairs (together with the outcome ); that is, "I was in state and I tried doing and happened"). Thus, one has an array and uses experience to update it directly. This is known as Q‑learning. Reinforcement learning can solve Markov decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration. In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states. === Category theoretic interpretation === Other than the rewards, a Markov decision process can be understood in terms of Category theory. Namely, let denote the free monoid with generating set A. Let Dist denote the Kleisli category of the Giry monad. Then a functor encodes both the set S of states and the probability function P. In this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result a context-dependent Markov decision process, because moving from one object to another in changes the set of available actions and the set of possible states. == Continuous-time Markov Decision Process == In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for Continuous-time Markov Decision Processes, decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov Decision Process, Continuous-time Markov Decision Process can better model the decision making process for a system that has continuous dynamics, i.e., the system dynamics is defined by partial differential equations (PDEs). === Definition === In order to discuss the continuous-time Markov Decision Process, we introduce two sets of notations: If the state space and action space are finite, : State space; : Action space; : , transition rate function; : , a reward function. If the state space and action space are continuous, : State space.; : Space of possible control; : , a transition rate function; : , a reward rate function such that , where is the reward function we discussed in previous case. === Problem === Like the Discrete-time Markov Decision Processes, in Continuous-time Markov Decision Process we want to find the optimal policy or control which could give us the optimal expected integrated reward: Where === Linear programming formulation === If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov Chain under a stationary policy. Under this assumption, although the decision maker can make a decision at any time at the current state, he could not benefit more by taking more than one action. It is better for him to take an action only at the time when system is transitioning from the current state to another state. Under some conditions,(for detail check Corollary 3.14 of Continuous-Time Markov Decision Processes), if our optimal value function is independent of state i, we will have the following inequality: If there exists a function , then will be the smallest g satisfying the above equation. In order to find , we could use the following linear programming model: Primal linear program(P-LP) Dual linear program(D-LP) 
34:827:In computer science, computational learning theory (or just learning theory) is a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms.
68:434:Blondie24 is an artificial intelligence checkers-playing computer program named after the screen name used by a team led by David B. Fogel. The purpose was to determine the effectiveness of an artificial intelligence checkers-playing computer program.
5:1699:MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research laboratory at the Massachusetts Institute of Technology formed by the 2003 merger of the Laboratory for Computer Science and the Artificial Intelligence Laboratory. Housed within the Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership.
147:234:A moral graph is a concept in graph theory, used to find the equivalent undirected form of a directed acyclic graph. It is a key step of the junction tree algorithm, used in belief propagation on graphical models.
20:533:A digital organism is a self-replicating computer program that mutates and evolves. Digital organisms are used as a tool to study the dynamics of Darwinian evolution, and to test or verify specific hypotheses or mathematical models of evolution. The study of digital organisms is closely related to the area of artificial life.
27:1554:In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into "spam" or "non-spam" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition.
2:209:The Association for the Advancement of Artificial Intelligence (AAAI) is an international, nonprofit, scientific society devoted to promote research in, and responsible use of, artificial intelligence. AAAI also aims to increase public understanding of artificial intelligence (AI), improve the teaching and training of AI practitioners, and provide guidance for research planners and funders concerning the importance and potential of current AI developments and future directions.
56:271:The promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coruña, in Spain. It evolves variable size feedforward artificial neural networks (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not.
64:1005:Bio-inspired computing, short for biologically inspired computing, is a field of study that loosely knits together subfields related to the topics of connectionism, social behaviour and emergence. It is often closely related to the field of artificial intelligence, as many of its pursuits can be linked to machine learning. It relies heavily on the fields of biology, computer science and mathematics. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of natural computation.
32:700:Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
162:0:Evolution is change in the heritable traits of biological populations over successive generations. Evolutionary processes give rise to diversity at every level of biological organisation, including the levels of species, individual organisms, and molecules. All life on Earth shares a common ancestor known as the last universal ancestor, which lived approximately 3.5–3.8 billion years ago, although a study in 2015 found "remains of biotic life" from 4.1 billion years ago in ancient rocks in Western Australia. According to one of the researchers, "If life arose relatively quickly on Earth ... then it could be common in the universe." Repeated formation of new species (speciation), change within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth are demonstrated by shared sets of morphological and biochemical traits, including shared DNA sequences. These shared traits are more similar among species that share a more recent common ancestor, and can be used to reconstruct a biological "tree of life" based on evolutionary relationships (phylogenetics), using both existing species and fossils. The fossil record includes a progression from early biogenic graphite, to microbial mat fossils, to fossilized multicellular organisms. Existing patterns of biodiversity have been shaped both by speciation and by extinction. More than 99 percent of all species that ever lived on Earth are estimated to be extinct. Estimates of Earth's current species range from 10 to 14 million, of which about 1.2 million have been documented. In the mid-19th century, Charles Darwin formulated the scientific theory of evolution by natural selection, published in his book On the Origin of Species (1859). Evolution by natural selection is a process demonstrated by the observation that more offspring are produced than can possibly survive, along with three facts about populations: 1) traits vary among individuals with respect to morphology, physiology, and behaviour (phenotypic variation), 2) different traits confer different rates of survival and reproduction (differential fitness), and 3) traits can be passed from generation to generation (heritability of fitness). Thus, in successive generations members of a population are replaced by progeny of parents better adapted to survive and reproduce in the biophysical environment in which natural selection takes place. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Natural selection is the only known cause of adaptation but not the only known cause of evolution. Other, nonadaptive causes of microevolution include mutation and genetic drift. In the early 20th century the modern evolutionary synthesis integrated classical genetics with Darwin's theory of evolution by natural selection through the discipline of population genetics. The importance of natural selection as a cause of evolution was accepted into other branches of biology. Moreover, previously held notions about evolution, such as orthogenesis, evolutionism, and other beliefs about innate "progress" within the largest-scale trends in evolution, became obsolete scientific theories. Scientists continue to study various aspects of evolutionary biology by forming and testing hypotheses, constructing mathematical models of theoretical biology and biological theories, using observational data, and performing experiments in both the field and the laboratory. In terms of practical application, an understanding of evolution has been instrumental to developments in numerous scientific and industrial fields, including agriculture, human and veterinary medicine, and the life sciences in general. Discoveries in evolutionary biology have made a significant impact not just in the traditional branches of biology but also in other academic disciplines, including biological anthropology, and evolutionary psychology. Evolutionary Computation, a sub-field of Artificial Intelligence, is the result of the application of Darwinian principles to problems in Computer Science. == History of evolutionary thought == The proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork De rerum natura (On the Nature of Things). In contrast to these materialistic views, Aristotle understood all natural things, not only living things, as being imperfect actualisations of different fixed natural possibilities, known as "forms," "ideas," or (in Latin translations) "species." This was part of his teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be. In the 17th century, the new method of modern science rejected Aristotle's approach. It sought explanations of natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences, the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, "species," to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. These species were designed by God, but showed differences caused by local conditions. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognized the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan. Other naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or "filament"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's "transmutation" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the Natural Theology or Evidences of the Existence and Attributes of the Deity (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin. The crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin in terms of variable populations. Partly influenced by An Essay on the Principle of Population (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a "struggle for existence" in which favorable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of "natural selection" from 1838 onwards and was writing up his "big book" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at a 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his "abstract" as On the Origin of Species explained natural selection in detail and in a way that led to an increasingly wide acceptance of concepts of evolution. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe. Precise mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cells structure. De Vries was also one of the researchers who made Mendel's work well-known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled. In the 1920s and 1930s a modern evolutionary synthesis connected natural selection, mutation theory, and Mendelian inheritance into a unified theory that applied generally to any branch of biology. The modern synthesis was able to explain patterns observed across species in populations, through fossil transitions in palaeontology, and even complex cellular mechanisms in developmental biology. The publication of the structure of DNA by James Watson and Francis Crick in 1953 demonstrated a physical basis for inheritance. Molecular biology improved our understanding of the relationship between genotype and phenotype. Advancements were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that "nothing in biology makes sense except in the light of evolution," because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet. Since then, the modern synthesis has been further extended to explain biological phenomena across the full and 
95:545:A roboticist is a person who designs, builds, programs, and experiments with robots. Since robotics is a highly interdisciplinary field, roboticists often have backgrounds in a number of disciplines including computer science, mechanical engineering, electrical engineering, physics, human–computer interaction and interaction design. Roboticists often work for university, industry, and government research labs, but may also work for startup companies and other entrepreneurial firms. Amateur Robotics is also a growing hobby all over the world.
126:0:Deep learning (deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-linear transformations. Deep learning is part of a broader family of machine learning methods based on learning representations of data. An observation (e.g., an image) can be represented in many ways such as a vector of intensity values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc. Some representations are better than others at simplifying the learning task (e.g., face recognition or facial expression recognition) from examples. One of the promises of deep learning is replacing handcrafted features with efficient algorithms for unsupervised or semi-supervised feature learning and hierarchical feature extraction. Research in this area attempts to make better representations and create models to learn these representations from large-scale unlabeled data. Some of the representations are inspired by advances in neuroscience and are loosely based on interpretation of information processing and communication patterns in a nervous system, such as neural coding which attempts to define a relationship between various stimuli and associated neuronal responses in the brain. Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Deep learning has been characterized as a buzzword, or a rebranding of neural networks. == Introduction == === Definitions === There are a number of ways that the field of deep learning has been characterized. For example, in 1986, Rina Dechter introduced the concepts of first order deep learning and second order deep learning in the context of constraint satisfaction. Later, deep learning was characterized as a class of machine learning algorithms that use a cascade of many layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output from the previous layer as input. The algorithms may be supervised or unsupervised and applications include pattern analysis (unsupervised) and classification (supervised). are based on the (unsupervised) learning of multiple levels of features or representations of the data. Higher level features are derived from lower level features to form a hierarchical representation. are part of the broader machine learning field of learning representations of data. learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts. These definitions have in common (1) multiple layers of nonlinear processing units and (2) the supervised or unsupervised learning of feature representations in each layer, with the layers forming a hierarchy from low-level to high-level features. The composition of a layer of nonlinear processing units used in a deep learning algorithm depends on the problem to be solved. Layers that have been used in deep learning include hidden layers of an artificial neural network and sets of complicated propositional formulas. They may also include latent variables organized layer-wise in deep generative models such as the nodes in Deep Belief Networks and Deep Boltzmann Machines. Deep learning algorithms transform their inputs through more layers than shallow learning algorithms. At each layer, the signal is transformed by a processing unit, like an artificial neuron, whose parameters are 'learned' through training. A chain of transformations from input to output is a credit assignment path (CAP). CAPs describe potentially causal connections between input and output and may vary in length. For a feedforward neural network, the depth of the CAPs, and thus the depth of the network, is the number of hidden layers plus one (the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP is potentially unlimited in length. There is no universally agreed upon threshold of depth dividing shallow learning from deep learning, but most researchers in the field agree that deep learning has multiple nonlinear layers (CAP > 2) and Schmidhuber considers CAP > 10 to be very deep learning. === Fundamental concepts === Deep learning algorithms are based on distributed representations. The underlying assumption behind distributed representations is that observed data are generated by the interactions of factors organized in layers. Deep learning adds the assumption that these layers of factors correspond to levels of abstraction or composition. Varying numbers of layers and layer sizes can be used to provide different amounts of abstraction. Deep learning exploits this idea of hierarchical explanatory factors where higher level, more abstract concepts are learned from the lower level ones. These architectures are often constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features are useful for learning. For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures which remove redundancy in representation. Many deep learning algorithms are applied to unsupervised learning tasks. This is an important benefit because unlabeled data are usually more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks. == Interpretations == Deep neural networks are generally interpreted in terms of: Universal approximation theorem or Probabilistic inference. === Universal approximation theorem interpretation === The universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. === Probabilistic interpretation === The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. See Deep belief network. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced and popularized by Geoff Hinton, Yoshua Bengio, Yann LeCun and Juergen Schmidhuber. == History == Ukrainian mathematicians Ivakhnenko and Lapa published the first general, working learning algorithm for supervised deep feedforward multilayer perceptrons in 1965. A paper from 1971 already described a deep network with 8 layers trained by the Group method of data handling algorithm which is still popular in the current millennium. These ideas were implemented in a computer identification system "Alpha", which demonstrated the learning process. Other Deep Learning working architectures, specifically those built from artificial neural networks (ANN), date back to the Neocognitron introduced by Kunihiko Fukushima in 1980. The ANNs themselves date back even further. The challenge was how to train networks with multiple layers. In 1989, Yann LeCun et al. were able to apply the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. Despite the success of applying the algorithm, the time to train the network on this dataset was approximately 3 days, making it impractical for general use. In 1993, Jürgen Schmidhuber's neural history compressor implemented as an unsupervised stack of recurrent neural networks (RNNs) solved a "Very Deep Learning" task that requires more than 1,000 subsequent layers in an RNN unfolded in time. In 1995, Brendan Frey demonstrated that it was possible to train a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, which was co-developed with Peter Dayan and Geoffrey Hinton. However, training took two days. Many factors contribute to the slow speed, one being the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter. While by 1991 such neural networks were used for recognizing isolated 2-D hand-written digits, recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Juyang Weng et al. suggested that a human brain does not use a monolithic 3-D object model, and in 1992 they published Cresceptron, a method for performing 3-D object recognition directly from cluttered scenes. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron automatically learned an open number of unsupervised features in each layer, where each feature is represented by a convolution kernel. Cresceptron also segmented each learned object from a cluttered scene through back-analysis through the network. Max-pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization. Despite these advantages, simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of the computational cost of ANNs at the time, and a great lack of understanding of how the brain autonomously wires its biological networks. In the long history of speech recognition, both shallow and deep learning (e.g., recurrent nets) of artificial neural networks have been explored for many years. But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. A number of key difficulties have been methodologically analyzed, including gradient diminishing and weak temporal correlation structure in the neural predictive models. Additional difficulties were the lack of big training data and weaker computing power in these early days. Thus, most speech recognition researchers who understood such barriers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI conducted research on deep neural networks in speech and speaker recognition. The speaker recognition team, led by Larry Heck, achieved the first significant success with deep neural networks in speech processing as demonstrated in the 1998 NIST (National Institute of Standards and Technology) Speaker Recognition evaluation and later published in the journal of Speech Communication. While SRI established success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of deep feedforward neural networks in speech recognition. Today, however, many aspects of speech recognition have been taken over by a deep learning method called Long short term memory (LSTM), a recurrent neural network published by Sepp Hochreiter & Jürgen 
52:882:Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball.
107:800:Fuzzy clustering (also referred to as soft clustering) is a form of clustering in which each data point can belong to more than one cluster or partition.
144:518:Variational message passing (VMP) is an approximate inference technique for continuous- or discrete-valued Bayesian networks, with conjugate-exponential parents, developed by John Winn. VMP was developed as a means of generalizing the approximate variational methods used by such techniques as Latent Dirichlet allocation and works by updating an approximate distribution at each node through messages in the node's Markov blanket.
94:234:Dario Floreano (San Daniele del Friuli, Italy, 1964) is director of the Laboratory of Intelligent Systems (LIS) at the École Polytechnique Fédérale de Lausanne in Switzerland. He is one of the pioneers in evolutionary robotics, a research field in which robots are evolved using artificial evolution.
31:297:In machine learning and pattern recognition, a feature is an individual measurable property of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of "feature" is related to that of explanatory variable used in statistical techniques such as linear regression.
112:309:The clustering illusion is the tendency to erroneously consider the inevitable "streaks" or "clusters" arising in small samples from random distributions to be non-random. The illusion is caused by a human tendency to underpredict the amount of variability likely to appear in a small sample of random or semi-random data.
99:0:Birch is a thinleaved deciduous hardwood tree of the genus Betula (/ˈbɛtjʊlə/), in the family Betulaceae, which also includes alders, hazels, and hornbeams, and is closely related to the beech/oak family, Fagaceae. The genus Betula contains 30 to 60 known taxa of which 11 are on the IUCN 2011 Green List of Threatened Species. They are a typically rather short-lived pioneer species widespread in the Northern Hemisphere, particularly in northern temperate and boreal climates. == Description == Birch species are generally small to medium-sized trees or shrubs, mostly of temperate climates. The simple leaves are alternate, singly or doubly serrate, feather-veined, petiolate and stipulate. They often appear in pairs, but these pairs are really borne on spur-like, two-leaved, lateral branchlets. The fruit is a small samara, although the wings may be obscure in some species. They differ from the alders (Alnus, other genus in the family) in that the female catkins are not woody and disintegrate at maturity, falling apart to release the seeds, unlike the woody, cone-like female alder catkins. The bark of all birches is characteristically marked with long, horizontal lenticels, and often separates into thin, papery plates, especially upon the paper birch. Its decided color gives the common names gray, white, black, silver and yellow birch to different species. The buds form early and are full grown by midsummer, all are lateral, no terminal bud is formed; the branch is prolonged by the upper lateral bud. The wood of all the species is close-grained with satiny texture, and capable of taking a fine polish; its fuel value is fair. === Flower and fruit === The flowers are monoecious, opening with or before the leaves and borne once fully grown these leaves are usually 3–6 millimetres (0.12–0.24 in) long on three-flowered clusters in the axils of the scales of drooping or erect catkins or aments. Staminate aments are pendulous, clustered or solitary in the axils of the last leaves of the branch of the year or near the ends of the short lateral branchlets of the year. They form in early autumn and remain rigid during the winter. The scales of the staminate aments when mature are broadly ovate, rounded, yellow or orange color below the middle, dark chestnut brown at apex. Each scale bears two bractlets and three sterile flowers, each flower consisting of a sessile, membranaceous, usually two-lobed, calyx. Each calyx bears four short filaments with one-celled anthers or strictly, two filaments divided into two branches, each bearing a half-anther. Anther cells open longitudinally. The pistillate aments are erect or pendulous, solitary; terminal on the two-leaved lateral spur-like branchlets of the year. The pistillate scales are oblong-ovate, three-lobed, pale yellow green often tinged with red, becoming brown at maturity. These scales bear two or three fertile flowers, each flower consisting of a naked ovary. The ovary is compressed, two-celled, and crowned with two slender styles; the ovule is solitary.Each scale bear a single small, winged nut that is oval, with two persistent stigmas at the apex. == Taxonomy == === Subdivision === Betula species are organised into five subgenera. Birches native to Europe and Asia include Betula albosinensis—Chinese red birch (northern + central China) Betula alnoides—alder-leaf birch (China, Himalayas, northern Indochina) Betula ashburneri (Bhutan, Tibet, Sichuan, Yunnan Provinces in China) Betula baschkirica (eastern European Russia) Betula bomiensis (Tibet) Betula calcicola (Sichuan + Yunnan Provinces in China) Betula celtiberica (Spain) Betula chichibuensis (Chichibu region of Japan) Betula chinensis—Chinese dwarf birch (China, Korea) Betula coriaceifolia (Uzbekistan) Betula corylifolia (Honshu Island in Japan) Betula costata (northeastern China, Korea, Primorye region of Russia) Betula cylindrostachya (Himalayas, southern China, Myanmar) Betula dahurica (eastern Siberia, Russian Far East, northeastern China, Mongolia, Korea, Japan) Betula delavayi - (Tibet, southern China) Betula ermanii—Erman's birch (eastern Siberia, Russian Far East, northeastern China, Korea, Japan) Betula falcata (Tajikistan) Betula fargesii (Chongqing + Hubei Provinces in China) Betula fruticosa (eastern Siberia, Russian Far East, northeastern China, Mongolia, Korea, Japan) Betula globispica (Honshu Island in Japan) Betula gmelinii (Siberia, Mongolia, northeastern China, Korea, Hokkaido Island in Japan) Betula grossa—Japanese cherry birch (Japan) Betula gynoterminalis (Yunnan Province in China) Betula honanensis - (Henan Province in China) Betula humilis or Betula kamtschatica— Kamchatka birch platyphylla (northern + central Europe, Siberia, Kazakhstan, Xinjiana, Mongolia, Korea) Betula insignis - (southern China) Betula karagandensis (Kazakhstan) Betula klokovii (Ukraine) Betula kotulae (Ukraine) Betula litvinovii (Turkey, Iran, Caucasus) Betula luminifera (China) Betula maximowiczii—monarch birch (Japan, Kuril Islands) Betula medwediewii—Caucasian birch (Turkey, Iran, Caucasus) Betula megrelica (Republic of Georgia) Betula microphylla (Siberia, Mongolia, Xinjiang, Kazakhstan, Kyrgyzstan, Uzbekistan) Betula nana—dwarf birch (northern + central Europe, Russia, Siberia, Greenland, Northwest Territories of Canada)) Betula pendula—silver birch (widespread in Europe and northern Asia; Morocco; naturalized in New Zealand and scattered locations in US + Canada) Betula platyphylla (Betula pendula var. platyphylla)—Siberian silver birch (Siberia, Russian Far East, Manchuria, Korea, Japan, Alaska, western Canada) Betula potamophila (Tajikistan) Betula potaninii (southern China) Betula psammophila (Kazakhstan) Betula pubescens—downy birch, also known as white, European white or hairy birch (Europe, Siberia, Greenland, Newfoundland; naturalized in scattered locations in US) Betula raddeana (Caucasus) Betula saksarensis (Khakassiya region of Siberia) Betula saviczii (Kazakhstan) Betula schmidtii (northeastern China, Korea, Japan, Primorye region of Russia) Betula sunanensis (Gansu Province of China) Betula szechuanica (Betula pendula var. szechuanica)—Sichuan birch (Tibet, southern China) Betula tianshanica (Kazakhstan, Kyrgyzstan, Tajikistan, Uzbekistan, Xinjiang, Mongolia) Betula utilis—Himalayan birch (Afghanistan, Central Asia, China, China, Tibet, Himalayas) Betula wuyiensis (Fujian Province of China) Betula zinserlingii (Kyrgyzstan) Note: many American texts have B. pendula and B. pubescens confused, though they are distinct species with different chromosome numbers. Birches native to North America include Betula alleghaniensis—yellow birch (B. lutea) (eastern Canada, Great Lakes, Northeastern US, Appalachians) Betula cordifolia—mountain paper birch (eastern Canada, Great Lakes, Northeastern US) Betula glandulosa—American dwarf birch (Siberia, Mongolia, Russian Far East, Alaska, Canada, Greenland, mountains of western US and New England, Adirondacks) Betula lenta—sweet birch, cherry birch, or black birch (Quebec, Ontario, eastern US) Betula michauxii—Newfoundland dwarf birch (Newfoundland, Labrador, Quebec, Nova Scotia) Betula minor—dwarf white birch (eastern Canada, mountains of northern New England and Adirondacks) Betula nana—dwarf birch or bog birch (also in northern Europe and Asia) Betula neoalaskana— Alaska paper birch also known as Alaska Birch or Resin Birch (Alaska and northern Canada) Betula nigra—river birch or black birch (eastern US) Betula occidentalis—water birch or red birch (B. fontinalis) (Alaska, Yukon, Northwest Territories, western Canada, western US) Betula papyrifera—paper birch, canoe birch or American white birch (Alaska, most of Canada, northern US) Betula populifolia—gray birch (eastern Canada, northeastern US) Betula pumila—swamp birch (Alaska, Canada, northern US) === Etymology === The common name birch comes from Old English birce, bierce, from Proto-Germanic *berk-jōn (cf. German Birke, West Frisian bjirk), an adjectival formation from *berkōn (cf. Dutch berk, Low German Bark, Danish birk, Norwegian bjørk), itself from the Proto-Indo-European root *bʰerHǵ- ~ bʰrHǵ-, which also gave Lithuanian béržas, Latvian Bērzs, Russian beréza, Ukrainian beréza, Albanian bredh ‘fir’, Ossetian bærz(æ), Sanskrit bhurja, Polish brzoza, Latin fraxinus ‘ash (tree)’. This root is presumably derived from *bʰreh₁ǵ- ‘to shine’, in reference to the birch's white bark. The Proto-Germanic rune berkanan is named after the birch. The generic name betula is from Latin, which is a diminutive borrowed from Gaulish betua (cf. Old Irish bethe, Welsh bedw). == Ecology == Birches often form even-aged stands on light, well-drained, particularly acidic soils. They are regarded as pioneer species, rapidly colonising open ground especially in secondary successional sequences following a disturbance or fire. Birches are early tree species to become established in primary successions, and can become a threat to heathland if the seedlings and saplings are not suppressed by grazing or periodic burning. Birches are generally lowland species, but some species, such as Betula nana, have a montane distribution. In the British Isles, there is some difference between the environments of Betula pendula and Betula pubescens, and some hybridization, though both are "opportunists in steady-state woodland systems". Mycorrhizal fungi, including sheathing (ecto)mycorrhizas, are found in some cases to be beneficial to tree growth. Birch foliage is used as a food plant by the larvae of a large number of lepidopteran (butterflies and moths) species. == Uses == Because of the hardness of Birch, it is better to shape it with power tools, as it is quite difficult to work it with hand tools. Birch wood is fine-grained and pale in colour, often with an attractive satin-like sheen. Ripple figuring may occur, increasing the value of the timber for veneer and furniture-making. The highly decorative Masur (or Karelian) birch, from Betula verrucosa var. carelica, has ripple textures combined with attractive dark streaks and lines. Birch wood is suitable for veneer, and birch plywood is among the strongest and dimensionally most stable plywoods, although it is unsuitable for exterior use. Birch plywood is made from laminations of birch veneer. It is light but strong, and has many other good properties. Birch plywood is used to make longboards (skateboard), giving it a strong yet flexible ride. It is also used (often in very thin grades with many laminations) for making model aircraft. Extracts of birch are used for flavoring or leather oil, and in cosmetics such as soap or shampoo. In the past, commercial oil of wintergreen (methyl salicylate) was made from the sweet birch (Betula lenta). Birch-tar or Russian oil extracted from birch bark is thermoplastic and waterproof; it was used as a glue on, for example, arrows, and also for medicinal purposes. Fragrant twigs of silver birch are used in saunas to relax the muscles. Birch is also associated with the feast of Pentecost in Germany, Central and Eastern Europe, and Russia, where its branches are used as decoration for churches and homes on this day. Birch leaves are used to make a diuretic tea and extracts for dyes and cosmetics. Ground birch bark, fermented in sea water, is used for seasoning the woolen, hemp or linen sails and hemp rope of traditional Norwegian boats. Birch twigs bound in a bundle, also called birch, were used for birching, a form of corporal punishment. Many Native Americans in the United States prized the birch for its bark, which because of its light weight, flexibility, and the ease with which it could be stripped from fallen trees, was often used for the construction of strong, waterproof but lightweight canoes, bowls, and wigwams. The Hughes H-4 Hercules was made mostly of birch wood, despite its better-known moniker, "The Spruce Goose". Birch plywood was specified by the BBC as the only wood that can be used in making the cabinets of the long-lived LS3/5A loudspeaker. Birch is used as firewood because of its high calorific value per unit weight and unit volume. It burns well, without popping, even when frozen and freshly hewn. The bark will burn very well even when wet because of the oils it contains. With care, it can be 
114:849:Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional data spaces are often encountered in areas such as medicine, where DNA microarray technology can produce a large number of measurements at once, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the vocabulary.
199:496:C-evo is a free turn-based strategy computer game whose source code – written in Delphi – has been put in the public domain by Steffen Gerlach, its programmer and designer. Other people have contributed separately downloadable alternative artificial intelligences for C-evo. Likewise, some of the graphics in the game are of external origin, but all are freeware.
3:0:A.I. Artificial Intelligence, also known as A.I., is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg was based on a screen story by Ian Watson and the 1969 short story Super-Toys Last All Summer Long by Brian Aldiss. The film was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. Set in a futuristic post-climate change society, A.I. tells the story of David (Osment), a childlike android uniquely programmed with the ability to love. Development of A.I. originally began with producer-director Stanley Kubrick in the early 1970s. Kubrick hired a series of writers up until the mid-1990s, including Brian Aldiss, Bob Shaw, Ian Watson, and Sara Maitland. The film languished in protracted development for years, partly because Kubrick felt computer-generated imagery was not advanced enough to create the David character, whom he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick's death in 1999. Spielberg remained close to Watson's film treatment for the screenplay. The film was greeted with generally positive reviews from critics, grossed approximately $235 million, and was nominated for two Academy Awards at the 74th Academy Awards for Best Visual Effects and Best Original Score (by John Williams). The film is dedicated to Stanley Kubrick. == Plot == In the late 21st century, global warming has flooded the coastlines, wiping out coastal cities (Amsterdam, Venice, New York) and drastically reducing the human population. There is a new class of robots called Mecha, advanced humanoids capable of emulating thoughts and emotions. David (Haley Joel Osment), a prototype model created by Cybertronics of New Jersey, is designed to resemble a human child and to display love for its human owners. They test their creation with one of their employees, Henry Swinton (Sam Robards), and his wife Monica (Frances O'Connor). The Swintons' son, Martin (Jake Thomas), had been placed in suspended animation until a cure could be found for his rare disease. Initially frightened of David, Monica eventually warms up enough to him to activate his imprinting protocol, which irreversibly causes David to have an enduring childlike love for her. He is also befriended by Teddy (Jack Angel), a robotic teddy bear, who takes it upon himself to care for David's well-being. A cure is found for Martin and he is brought home; as he recovers, it becomes clear he does not want a sibling and soon makes moves to cause issues for David. First, he attempts to make Teddy choose whom he likes more. He then makes David promise to do something and in return Martin will tell Monica that he loves his new "brother", making her love him more. The promise David makes is to go to Monica in the middle of the night and cut off a lock of her hair, which of course upsets the parents (not just through the act itself, but because David isn't supposed to go into their room at night). At a pool party, one of Martin's friends unintentionally activates David's self-protection programming by poking him with a knife. David grabs Martin, apparently for protection, but they both fall into the pool. David sinks to the bottom while still clinging to Martin. Martin is saved from drowning, but Henry mistakes David's fear during the pool incident as hate for Martin. Henry persuades Monica to return David to Cybertronics, where he will be destroyed. However, Monica cannot bring herself to do this and, instead, tearfully abandons David in the forest (with Teddy) to hide as an unregistered Mecha. David is captured for an anti-Mecha "Flesh Fair", an event where obsolete and unlicensed Mecha are destroyed in front of cheering crowds. David is nearly killed, but the crowd is swayed by his fear (since Mecha do not plea for their lives) into believing he is human and he escapes with Gigolo Joe (Jude Law), a male prostitute Mecha on the run after being framed for murder. The two set out to find the Blue Fairy, who David remembers from the story The Adventures of Pinocchio. He is convinced that the Blue Fairy will transform him into a human boy, allowing Monica to love him and take him home. Joe and David make their way to Rouge City, a Las Vegas-esque resort. Information from a holographic answer engine called "Dr. Know" (Robin Williams) eventually leads them to the top of Rockefeller Center in the flooded ruins of Manhattan. There, David meets an identical copy of himself and, believing he is not special, becomes filled with anger and destroys the copy Mecha. David then meets his human creator, Professor Allen Hobby (William Hurt), who excitedly tells David that finding him was a test, which has demonstrated the reality of his love and desire. However, David learns that he is the namesake and image of Professor Hobby's deceased son and that many copies of David, along with female versions, are already being manufactured. Sadly realizing that he is not unique, a disheartened David attempts to commit suicide by falling from a ledge into the ocean, but Joe rescues him with their stolen amphibicopter. David tells Joe he saw the Blue Fairy underwater and wants to go down to her. At that moment, Joe is captured by the authorities with the use of an electromagnet, but he sets the amphibicopter on submerge. David and Teddy take it to the fairy, which turns out to be a statue from a submerged attraction at Coney Island. Teddy and David become trapped when the Wonder Wheel falls on their vehicle. Believing the Blue Fairy to be real, David asks to be turned into a real boy, repeating his wish without an end, until the ocean freezes in another ice age and his internal power source drains away. Two thousand years later, humans are extinct and Manhattan is buried under several hundred feet of glacial ice. The now highly advanced Mecha have evolved into an intelligent, silicon-based form. On their project to study humans — believing it was the key to understanding the meaning of existence — they find David and Teddy and discover they are original Mecha who knew living humans, making the pair very special and unique. David is revived and walks to the frozen Blue Fairy statue, which cracks and collapses as he touches it. Having downloaded and comprehended his memories, the advanced Mecha use these to reconstruct the Swinton home and explain to David via an interactive image of the Blue Fairy (Meryl Streep) that it is impossible to make him human. However, at David's insistence, they recreate Monica from DNA in the lock of her hair, which Teddy had saved. One of the Mecha warns David that the clone can live for only a single day and that the process cannot be repeated. The next morning, David is reunited with Monica and spends the happiest day of his life with her and Teddy. Monica tells David that she loves him and has always loved him as she drifts to sleep for the last time. David lies down next to her, closes his eyes and goes "to that place where dreams are born." Teddy climbs onto the bed and watches as David and Monica lie peacefully together. == Cast == Haley Joel Osment as David, an innovative Mecha created by Cybertronics and programmed with the ability to love. He is adopted by Henry and Monica Swinton, but a sibling rivalry ensues once their son Martin comes out of suspended animation. Osment was Spielberg's first and only choice for the role. Osment avoided blinking his eyes to perfectly portray the character, and "programmed" himself with good posture for realism. Jude Law as Gigolo Joe, a male prostitute Mecha programmed with the ability to mimic love, like David, but in a different sense. To prepare for the role, Law studied the acting of Fred Astaire and Gene Kelly. Frances O'Connor as Monica Swinton, David's adopted mother who reads him The Adventures of Pinocchio. She is first displeased to have David in her home but soon starts loving him. Sam Robards as Henry Swinton, an employee at Cybertronics, husband of Monica and David's adopted father. Henry eventually sees David as dangerous to his family. Jake Thomas as Martin Swinton, Henry and Monica's first son, who was placed in suspended animation and David's adopted brother. When Martin comes back, he convinces David to cut off a lock of Monica's hair. William Hurt as Professor Allen Hobby, responsible for shepherding the creation of David. He resides in New York City, which is crippled by the effects of global warming but still functioning as Cybertronics' headquarters. David is modeled after Hobby's own son, also named David, who died at a young age. Brendan Gleeson as Lord Johnson-Johnson, the owner and master of ceremonies of the Flesh Fair. Ashley Scott as Gigolo Jane Voices Jack Angel as Teddy, David's android teddy bear. Ben Kingsley as a Mecha specialist. He is also uncredited as the narrator. Robin Williams as Dr. Know, a holographic answer engine. (Cameo) Meryl Streep as The Blue Fairy. (Cameo) Chris Rock as a Mecha comedian destroyed at the Flesh Fair. (Cameo) == Production == === Development === Kubrick began development on an adaptation of Super-Toys Last All Summer Long in the early 1970s, hiring the short story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick brought longtime friend Steven Spielberg on board to produce the film, along with Jan Harlan. Warner Bros. agreed to co-finance A.I. and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw served as writer very briefly, leaving after six weeks because of Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, "Not only did the bastard fire me, he hired my enemy [Watson] instead." Kubrick handed Watson The Adventures of Pinocchio for inspiration, calling A.I. "a picaresque robot version of Pinocchio". Three weeks later Watson gave Kubrick his first story treatment, and concluded his work on A.I. in May 1991 with another treatment, at 90 pages. Gigolo Joe was originally conceived as a GI Mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, "I guess we lost the kiddie market." In the meantime, Kubrick dropped A.I. to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character. However, after the release of Spielberg's Jurassic Park (with its innovative use of computer-generated imagery), it was announced in November 1993 that production would begin in 1994. Dennis Muren and Ned Gorman, who worked on Jurassic Park, 
146:781:A graphical model or probabilistic graphical model (PGM) is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.
90:264:Francesco Mondada (born 17 March 1967) is a Swiss professor in artificial intelligence and robotics. He got a Master's degree in Microengineering at the EPFL in 1991 and a Ph.D. degree in 1997. He is one of the creators of the Khepera and directed the design of the S-bot, the e-puck, the marXbot and the Thymio mobile robots. Together, these robots are mentioned in more than 7000 research articles. In particular the Khepera robot is a milestone in the field of bio-inspired and evolutionary robotics.
187:570:The Invincible (Polish: Niezwyciężony) is a science fiction novel written by Stanisław Lem and published in 1964.
188:0:The firefly algorithm (FA) is a metaheuristic algorithm, inspired by the flashing behaviour of fireflies. The primary purpose for a firefly's flash is to act as a signal system to attract other fireflies. Xin-She Yang formulated this firefly algorithm by assuming: All fireflies are unisexual, so that any individual firefly will be attracted to all other fireflies; Attractiveness is proportional to their brightness, and for any two fireflies, the less bright one will be attracted by (and thus move towards) the brighter one; however, the intensity (apparent brightness) decrease as their mutual distance increases; If there are no fireflies brighter than a given firefly, it will move randomly. The brightness should be associated with the objective function. Firefly algorithm is a nature-inspired metaheuristic optimization algorithm. == Algorithm description == The pseudo code can be summarized as: Begin 1) Objective function: ; 2) Generate an initial population of fireflies ;. 3) Formulate light intensity I so that it is associated with (for example, for maximization problems, or simply ;) 4) Define absorption coefficient γ While (t < MaxGeneration) for i = 1 : n (all n fireflies) for j = 1 : n (n fireflies) if (), move firefly i towards j; Vary attractiveness with distance r via ; Evaluate new solutions and update light intensity; end if end for j end for i Rank fireflies and find the current best; end while Post-processing the results and visualization; end The main update formula for any pair of two fireflies and is where is a parameter controlling the step size, while is a vector drawn from a Gaussian or other distribution. It can be shown that the limiting case corresponds to the standard Particle Swarm Optimization (PSO). In fact, if the inner loop (for j) is removed and the brightness is replaced by the current global best , then FA essentially becomes the standard PSO. == Implementation Guides == The should be related to the scales of design variables. Ideally, the term should be order one, which requires that should be linked with scales. For example, one possible choice is to use where is the average scale of the problem. In case of scales vary significantly, can be considered as a vector to suit different scales in different dimensions. Similarly, should also be linked with scales. For example, . It is worth pointing out the above description does not include the randomness reduction. In fact, in actual implementation by most researchers, the motion of the fireflies is gradually reduced by an annealing-like randomness reduction via where , though this value may depend on the number of iterations. In some difficult problem, it may be helpful if you increase at some stages, then reduce it when necessary. This non-monotonic variation of will enable the algorithm to escape any local optima when in the unlikely case it might get stuck if randomness is reduced too quickly. Parametric studies show that n (number of fireflies) should be about 15 to 40 for most problems. A python implementation is also available, though with limited functionalities. Recent studies shows that the firefly algorithm is very efficient, and could outperform other metaheuristic algorithms including particle swarm optimization. Most metaheuristic algorithms may have difficulty in dealing with stochastic test functions, and it seems that firefly algorithm can deal with stochastic test functions very efficiently. In addition, FA is also better for dealing with noisy optimization problems with ease of implementation. Chatterjee et al. showed that the firefly algorithm can be superior to particle swarm optimization in their applications, the effectiveness of the firefly algorithm was further tested in later studies. In addition, firefly algorithm can efficiently solve non-convex problems with complex nonlinear constraints. Further improvement on the performance is also possible with promising results. == Theoretical Analysis == Although much progress has been achieved FA-based algorithms since 2008, significant efforts are required to further improve the performance of FA: Theoretical analysis for convergence trajectory; Deriving the sufficient and necessary conditions for the selections of control coefficients; Efficient strategies or mechanisms for the selections of the control parameters; Non-homogeneous update rules for enhancing the search ability was proposed in ref. Further, classical variants of the algorithm have unexpected parameter settings and limited update laws, notably the homogeneous rule needs to be improved in order to do more search on different fitness landscape. Ref. analyzes the trajectory of a single firefly in the traditional algorithm and an adaptive variant, respectively. These analyses lead to a general model of the algorithms including a set of the boundary conditions for the parameters guaranteeing the convergence tendencies of the two algorithms. == Variants of Firefly Algorithm == A recent, comprehensive review showed that the firefly algorithm and its variants have been used in almost all areas of science There are more than twenty variants: === Adaptive Firefly Algorithm (AdaFa) === An adaptive variant of firefly algorithm, termed AdaFa, was proposed in ref. In AdaFa, the parameter selection and adaptation strategies are investigated. There are three strategies in AdaFa including (1) a distance-based light absorption coefficient; (2) a gray coefficient enhancing fireflies to share difference information from attractive ones efficiently; and (3) five different dynamic strategies for the randomization parameter. Promising selections of parameters in the strategies are analyzed to guarantee the efficient performance of AdaFa. === Discrete Firefly Algorithm (DFA) === A discrete version of Firefly Algorithm, namely, Discrete Firefly Algorithm (DFA) proposed recently by M. K. Sayadi, R. Ramezanian and N. Ghaffari-Nasab can efficiently solve NP-hard scheduling problems. DFA outperforms existing algorithms such as the ant colony algorithm. For image segmentation, the FA-based method is far more efficient to Otsu's method and recursive Otsu. Meanwhile, a good implementation of a discrete firefly algorithm for QAP problems has been carried out by Durkota. === Multiobjective FA === An important study of FA was carried out by Apostolopoulos and Vlachos, which provides a detailed background and analysis over a wide range of test problems including multobjective load dispatch problem. === Lagrangian FA === An interesting, Lagrangian firefly algorithm is proposed to solve power system optimization unit commitment problems. === Chaotic FA === A chaotic firefly algorithm (CFA) was developed and found to outperform the previously best-known solutions available. === Hybrid Algorithms === A hybrid intelligent scheme has been developed by combining the firefly algorithm with the ant colony optimization. === Firefly Algorithm Based Memetic Algorithm === A firefly algorithm (FA) based memetic algorithm (FA-MA) is proposed to appropriately determine the parameters of SVR forecasting model for electricity load forecasting. In the proposed FA-MA algorithm, the FA algorithm is applied to explore the solution space, and the pattern search is used to conduct individual learning and thus enhance the exploitation of FA. === Parallel Firefly Algorithm with Predation (pFAP) === An implementation for shared memory environments with the addition of a predation mechanism that helps the method to escape local optimum. === Modified Firefly Algorithm === Many variants and modifications are done to increase its performance. A particular example will be modified firefly algorithm by Tilahun and Ong ., in which the updating process of the brightest firefly is modified to keep the best result throughout the iterations. Another example is a modified firefly algorithm to solve univariate nonlinear equations having real as well as complex roots. == Applications == === Digital Image Compression and Image Processing === Very recently, an FF-LBG algorithm for vector quantization of digital image compression was based on the firefly algorithm, which proves to be faster than other algorithms such as PSO-LBG and HBMO-LBG (particle swarm optimization and honey-bee mating optimization; variations on the Linde–Buzo–Gray algorithm). For minimum cross entropy thresholding, firefly-based algorithm uses the least computation time Also, for gel electrophoresis images, FA-based method is very efficient. === Eigenvalue optimization === Eigenvalue optimization of isospectral systems has solved by FA and multiple optimum points have been found efficiently. === Nanoelectronic Integrated Circuit and System Design === The multiobjective firefly algorithm (MOFA) has been used for the design optimization of a 90 nm CMOS based operational amplifier (OP-AMP) which could perform simultaneous power minimization and slew rate maximization within 500 iterations. === Feature selection and fault detection === Feature selection can be also carried out successfully using firefly algorithm. Real-time fault identification in large systems becomes viable, based on the recent work on fault identification with binary adaptive firefly optimization. A hybrid filter-wrapper feature selection for load forecasting is proposed based on Firefly Algorithm. === Antenna Design === Firefly algorithms outperforms ABC for optimal design of linear array of isotropic sources and digital controllable array antenna. It has found applications in synthesis of satellite footprint patterns as well. === Structural Design === For mixed-variable problems, many optimization algorithms may struggle. However, firefly algorithm can efficiently solve optimization problems with mixed variables. === Scheduling and TSP === Firefly-based algorithms for scheduling task graphs and job shop scheduling requires less computing than all other metaheuristics. A binary firefly algorithm has been developed to tackle the knapsack cryptosystem efficiently Recently, an evolutionary discrete FA has been developed for solving travelling salesman problems Further improvement in performance can be obtained by using preferential directions in firefly movements. === Semantic Web Composition === A hybrid FA has been developed by Pop et al. for selecting optimal solution in semantic web service composition. === Chemical Phase equilibrium === For phase equilibrium calculations and stability analysis, FA was found to be the most reliable compared with other techniques. === Clustering === Performance study for clustering also suggested that firefly algorithm is very efficient. === Dynamic Problems === Firefly algorithm can solve optimization problems in dynamic environments very efficiently. === Rigid Image Registration Problems === Firefly algorithm can solve the rigid image registration problems more efficient than genetic algorithm, particle swarm optimization, and artificial bee colony === Protein Structure Prediction === Prediction of protein structures is NP-hard, and a recent study by Maher et al. shows that firefly-based methods can speed up the predictions. Firefly algorithm can solve two dimensional HP model. In their experiment, they took 14 sequences of different chain lengths from 18 to 100 as the dataset and compared the FA with standard genetic algorithm and immune genetic algorithm. The averaged energy convergence results show that FA achieves the lowest values. === Parameter Optimization of SVM === Firefly algorithm (FA)is applied to determine the paraemters of MSVR (Multiple-output support vector regression) in interval-valued stock price index forecasting. Meanwhile, a firefly algorithm (FA) based memetic algorithm (FA-MA) is proposed to appropriately determine the parameters of SVR forecasting model for electricity load forecasting. In the proposed FA-MA algorithm, the FA algorithm is applied to explore the solution space, and the pattern search is used to conduct individual learning and thus enhance the exploitation of FA. === IK-FA, Solving Inverse Kinematics using FA === FA, heuristic is used as 
47:0:Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. The algorithm for inducing Breiman's random forest was developed by Leo Breiman and Adele Cutler, and "Random Forests" is their trademark. The method combines Breiman's "bagging" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance. The selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg. == History == The general method of random decision forests was first proposed by Ho in 1995, who established that forests of trees splitting with oblique hyperplanes, if randomly restricted to be sensitive to only selected feature dimensions, can gain accuracy as they grow without suffering from overtraining. A subsequent work along the same lines concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level before accuracy being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination. The early development of Breiman's notion of random forests was influenced by the work of Amit and Geman who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown, and variation among the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree. Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Dietterich. The introduction of random forests proper was first made in a paper by Leo Breiman. This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular: Using out-of-bag error as an estimate of the generalization error. Measuring variable importance through permutation. The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation. == Algorithm == === Preliminaries: decision tree learning === Decision trees are a popular method for various machine learning tasks. Tree learning "come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining", say Hastie et al., because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate. In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, because they have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model. === Tree bagging === The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples: For b = 1, ..., B: Sample, with replacement, n training examples from X, Y; call these Xb, Yb. Train a decision or regression tree fb on Xb, Yb. After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x': or by taking the majority vote in the case of decision trees. This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets. The number of samples/trees, B, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees B can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample. The training and test error tend to level off after some number of trees have been fit. === From bagging to random forests === The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called "feature bagging". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho. Typically, for a classification problem with p features, √p (rounded down) features are used in each split. For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default. === Extensions === Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. These are trained using bagging and the random subspace method, like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity), for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample) == Properties == === Variable importance === Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest. The first step in measuring the variable importance in a data set is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training). To measure the importance of the -th feature after training, the values of the -th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the -th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences. Features which produce large values for this score are ranked as more important than features which produce small values. This method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations and growing unbiased trees can be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups. === Relationship to nearest neighbors === A relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set that make predictions for new points x' by looking at the "neighborhood" of the point, formalized by a weight function W: Here, is the non-negative weight of the i'th training point relative to the new point x'. For any particular x', the weights must sum to one. Weight functions are given as follows: In k-NN, the weights are if xi is one of the k points closest to x', and zero otherwise. In a tree, if xi is one of the k' points in the same leaf as x', and zero otherwise. Since a forest averages the predictions of a set of m trees with individual weight functions , its predictions are This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of x' in this interpretation are the points which fall in the same leaf as x' in at least one tree of the forest. In this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature. == Unsupervised learning with random forests == As part of their construction, RF predictors naturally lead to a dissimilarity measure between the observations. One can also define an RF dissimilarity measure between unlabeled data: the idea is to construct an RF predictor that distinguishes the “observed” data from suitably 
11:331:Artificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. They claimed to have found, in research, the "mechanisms of knowledge representation in the brain which is equivalent to finding artificial intelligence", before moving into the developmental phase.
200:202:Neural machine translation (NMT) is the approach to machine translation in which a large neural network is trained to maximize translation performance. It is a radical departure from the phrase-based statistical translation approaches, in which a translation system consists of subcomponents that are separately optimized.
83:473:A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims.
29:0:Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias). The parallel task in human and animal psychology is often referred to as concept learning. == Overview == In order to solve a given problem of supervised learning, one has to perform the following steps: Determine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting. Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements. Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output. Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support vector machines or decision trees. Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation. Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set. A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem). There are four major issues to consider in supervised learning: === Bias-variance tradeoff === A first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for . A learning algorithm has high variance for a particular input if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust). === Function complexity and amount of training data === The second issue is the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be learnable from a very large amount of training data and using a "flexible" learning algorithm with low bias and high variance. Good learning algorithms therefore automatically adjust the bias/variance tradeoff based on the amount of data available and the apparent complexity of the function to be learned. === Dimensionality of the input space === A third issue is the dimensionality of the input space. If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, high input dimensionality typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. === Noise in the output values === A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation that part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator. In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance. === Other factors to consider === Other factors to consider when choosing and applying a learning algorithm include the following: Heterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including Support Vector Machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data. Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization. Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, Support Vector Machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them. When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms. The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron). == How supervised learning algorithms work == Given a set of training examples of the form such that is the feature vector of the i-th example and is its label (i.e., class), a learning algorithm seeks a function , where is the input space and is the output space. The function is an element of some space of possible functions , usually called the hypothesis space. It is sometimes convenient to represent using a scoring function such that is defined as returning the value that gives the highest score: . Let denote the space of scoring functions. Although and can be any space of functions, many learning algorithms are probabilistic models where takes the form of a conditional probability model , or takes the form of a joint probability model . For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model. There are two basic approaches to choosing or : empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimize includes a penalty function that controls the bias/variance tradeoff. In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, . In order to measure how well a function fits the training data, a loss function is defined. For training example , the loss of predicting the value is . The risk of function is defined as the expected loss of . This can be estimated from the training data as . === Empirical risk minimization === In empirical risk minimization, the supervised learning algorithm seeks the function that minimizes . Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find . When is a conditional probability distribution and the loss function is the negative log 
98:930:In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.
25:0:Machine learning is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions. Machine learning is closely related to and often overlaps with computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is infeasible. Example applications include spam filtering, optical character recognition (OCR), search engines and computer vision. Machine learning is sometimes conflated with data mining, where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning. Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data. == Overview == Tom M. Mitchell provided a widely quoted, more formal definition: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E". This definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms, thus following Alan Turing's proposal in his paper "Computing Machinery and Intelligence" that the question "Can machines think?" be replaced with the question "Can machines do what we (as thinking entities) can do?" === Types of problems and tasks === Machine learning tasks are typically classified into three broad categories, depending on the nature of the learning "signal" or "feedback" available to a learning system. These are Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle), without a teacher explicitly telling it whether it has come close to its goal. Another example is learning to play a game by playing against an opponent. Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing. Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers, and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation. Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system: In classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are "spam" and "not spam". In regression, also a supervised problem, the outputs are continuous rather than discrete. In clustering, a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task. Density estimation finds the distribution of inputs in some space. Dimensionality reduction simplifies inputs by mapping them into a lower-dimensional space. Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics. == History and relationships to other fields == As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory. It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the Internet. Machine learning and data mining often employ the same methods and overlap significantly. They can be roughly distinguished as follows: Machine learning focuses on prediction, based on known properties learned from the training data. Data mining focuses on the discovery of (previously) unknown properties in the data. This is the analysis step of Knowledge Discovery in Databases. The two areas overlap in many ways: data mining uses many machine learning methods, but often with a slightly different goal in mind. On the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in Knowledge Discovery and Data Mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. === Relation to statistics === Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field. Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein 'algorithmic model' means more or less the machine learning algorithms like Random forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. == Theory == A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error. How well a model trained with existing examples predicts the output for unknown instances is called generalization. For best generalization, complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, we've underfitted. Then, we increase the complexity, the training error decreases. But if our hypothesis is too complex, we've overfitted. After then, we should find the hypothesis that has the minimum training error. In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. There are many similarities between machine learning theory and statistical inference, although they use different terms. == Approaches == === Decision tree learning === Decision tree learning uses a decision tree 
71:1136:A Cellular Evolutionary Algorithm (cEA) is a kind of evolutionary algorithm (EA) in which individuals cannot mate arbitrarily, but every one interacts with its closer neighbors on which a basic EA is applied (selection, variation, replacement).
101:770:In graph theory, a clustering coefficient is a measure of the degree to which nodes in a graph tend to cluster together. Evidence suggests that in most real-world networks, and in particular social networks, nodes tend to create tightly knit groups characterised by a relatively high density of ties; this likelihood tends to be greater than the average probability of a tie randomly established between two nodes (Holland and Leinhardt, 1971; Watts and Strogatz, 1998).
66:176:Encog is a machine learning framework available for Java, .Net, and C++.  Encog supports different learning algorithms such as Bayesian Networks, Hidden Markov Models and Support Vector Machines. However, its main strength lies in its neural network algorithms. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using many different techniques. Multithreading is used to allow optimal training performance on multicore machines. The C++ version of Encog can offload some processing to an OpenCL compatible GPU for further performance gains.
184:0:In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position but, is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions. PSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart describes many philosophical aspects of PSO and swarm intelligence. An extensive survey of PSO applications is made by Poli. Recently, a comprehensive review on theoretical and experimental works on PSO has been published by Bonyadi and Michalewicz. PSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. More specifically, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods. == Algorithm == A basic variant of the PSO algorithm works by having a population (called a swarm) of candidate solutions (called particles). These particles are moved around in the search-space according to a few simple formulae. The movements of the particles are guided by their own best known position in the search-space as well as the entire swarm's best known position. When improved positions are being discovered these will then come to guide the movements of the swarm. The process is repeated and by doing so it is hoped, but not guaranteed, that a satisfactory solution will eventually be discovered. Formally, let f: ℝn → ℝ be the cost function which must be minimized. The function takes a candidate solution as argument in the form of a vector of real numbers and produces a real number as output which indicates the objective function value of the given candidate solution. The gradient of f is not known. The goal is to find a solution a for which f(a) ≤ f(b) for all b in the search-space, which would mean a is the global minimum. Maximization can be performed by considering the function h = -f instead. Let S be the number of particles in the swarm, each having a position xi ∈ ℝn in the search-space and a velocity vi ∈ ℝn. Let pi be the best known position of particle i and let g be the best known position of the entire swarm. A basic PSO algorithm is then: For each particle i = 1, ..., S do: Initialize the particle's position with a uniformly distributed random vector: xi ~ U(blo, bup), where blo and bup are the lower and upper boundaries of the search-space. Initialize the particle's best known position to its initial position: pi ← xi If (f(pi) < f(g)) update the swarm's best known position: g ← pi Initialize the particle's velocity: vi ~ U(-|bup-blo|, |bup-blo|) Until a termination criterion is met (e.g. number of iterations performed, or a solution with adequate objective function value is found), repeat: For each particle i = 1, ..., S do: For each dimension d = 1, ..., n do: Pick random numbers: rp, rg ~ U(0,1) Update the particle's velocity: vi,d ← ω vi,d + φp rp (pi,d-xi,d) + φg rg (gd-xi,d) Update the particle's position: xi ← xi + vi If (f(xi) < f(pi)) do: Update the particle's best known position: pi ← xi If (f(pi) < f(g)) update the swarm's best known position: g ← pi Now g holds the best found solution. The parameters ω, φp, and φg are selected by the practitioner and control the behaviour and efficacy of the PSO method, see below. == Parameter selection == The choice of PSO parameters can have a large impact on optimization performance. Selecting PSO parameters that yield good performance has therefore been the subject of much research. The PSO parameters can also be tuned by using another overlaying optimizer, a concept known as meta-optimization. Parameters have also been tuned for various optimization scenarios. == Neighborhoods and Topologies == The basic PSO is easily trapped into a local minimum. This premature convergence can be avoided by not using the entire swarm's best known position g but just the best known position l of a sub-swarm "around" the particle that is moved. Such a sub-swarm can be a geometrical one - for example "the m nearest particles" - or, more often, a social one, i.e. a set of particles that is not depending on any distance. In such a case, the PSO variant is said to be local best (vs global best for the basic PSO). If we suppose there is an information link between each particle and its neighbours, the set of these links builds a graph, a communication network, that is called the topology of the PSO variant. A commonly used social topology is the ring, in which each particle has just two neighbours, but there are many others. The topology is not necessarily fixed, and can be adaptive (SPSO, stochastic star, TRIBES, Cyber Swarm, C-PSO). == Inner workings == There are several schools of thought as to why and how the PSO algorithm can perform optimization. A common belief amongst researchers is that the swarm behaviour varies between exploratory behaviour, that is, searching a broader region of the search-space, and exploitative behaviour, that is, a locally oriented search so as to get closer to a (possibly local) optimum. This school of thought has been prevalent since the inception of PSO. This school of thought contends that the PSO algorithm and its parameters must be chosen so as to properly balance between exploration and exploitation to avoid premature convergence to a local optimum yet still ensure a good rate of convergence to the optimum. This belief is the precursor of many PSO variants, see below. Another school of thought is that the behaviour of a PSO swarm is not well understood in terms of how it affects actual optimization performance, especially for higher-dimensional search-spaces and optimization problems that may be discontinuous, noisy, and time-varying. This school of thought merely tries to find PSO algorithms and parameters that cause good performance regardless of how the swarm behaviour can be interpreted in relation to e.g. exploration and exploitation. Such studies have led to the simplification of the PSO algorithm, see below. === Convergence === In relation to PSO the word convergence typically refers to two different definitions: Convergence of the sequence of solutions (aka, stability analysis, converging) in which all particles have converged to a point in the search-space, which may or may not be the optimum, Convergence to a local optimum where all personal bests p or, alternatively, the swarm's best known position g, approaches a local optimum of the problem, regardless of how the swarm behaves. Convergence of the sequence of solutions has been investigated for PSO. These analyses have resulted in guidelines for selecting PSO parameters that are believed to cause convergence to a point and prevent divergence of the swarm's particles (particles do not move unboundedly and will converge to somewhere). However, the analyses were criticized by Pedersen for being oversimplified as they assume the swarm has only one particle, that it does not use stochastic variables and that the points of attraction, that is, the particle's best known position p and the swarm's best known position g, remain constant throughout the optimization process. However, it was shown that these simplifications do not affect the boundaries found by these studies for parameter where the swarm is convergent. Convergence to a local optimum has been analyzed for PSO in and. It has been proven that PSO need some modification to guarantee to find a local optimum. This means that determining convergence capabilities of different PSO algorithms and parameters therefore still depends on empirical results. One attempt at addressing this issue is the development of an "orthogonal learning" strategy for an improved use of the information already existing in the relationship between p and g, so as to form a leading converging exemplar and to be effective with any PSO topology. The aims are to improve the performance of PSO overall, including faster global convergence, higher solution quality, and stronger robustness. However, such studies do not provide theoretical evidence to actually prove their claims. === Biases === As the basic PSO works dimension by dimension, the solution point is easier found when it lies on an axis of the search space, on a diagonal, and even easier if it is right on the centre. One approach is to modify the algorithm so that it is not any more sensitive to the system of coordinates. Note that some of these methods have a higher computational complexity (are in O(n^2) where n is the number of dimensions) that make the algorithm very slow for large scale optimization. The only currently existing PSO variant that is not sensitive to the rotation of the coordinates while is locally convergent has been proposed at 2014. The method has shown a very good performance on many benchmark problems while its rotation invariance and local convergence have been mathematically proven. == Variants == Numerous variants of even a basic PSO algorithm are possible. For example, there are different ways to initialize the particles and velocities (e.g. start with zero velocities instead), how to dampen the velocity, only update pi and g after the entire swarm has been updated, etc. Some of these choices and their possible performance impact have been discussed in the literature. A series of standard implementations have been created by leading researchers, "intended for use both as a baseline for performance testing of improvements to the technique, as well as to represent PSO to the wider optimization community. Having a well-known, strictly-defined standard algorithm provides a valuable point of comparison which can be used throughout the field of research to better test new advances." The latest is Standard PSO 2011 (SPSO-2011). === Hybridization === New and more sophisticated PSO variants are also continually being introduced in an attempt to improve optimization performance. There are certain trends in that research; one is to make a hybrid 
48:952:Adaptive resonance theory (ART) is a theory developed by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction.
13:1037:In computer science, evolutionary computation is a subfield of artificial intelligence (more particularly computational intelligence) that can be defined by the type of algorithms it is concerned with. These algorithms, called evolutionary algorithms, are based on adopting Darwinian principles, hence the name. Technically they belong to the family of trial and error problem solvers and can be considered global optimization methods with a metaheuristic or stochastic optimization character, distinguished by the use of a population of candidate solutions (rather than just iterating over one point in the search space). They are mostly applied for black box problems (no derivatives known), often in the context of expensive optimization.
40:543:In mathematics, in particular probability theory and related fields, the softmax function, or normalized exponential, is a generalization of the logistic function that "squashes" a K-dimensional vector  of arbitrary real values to a K-dimensional vector  of real values in the range (0, 1) that add up to 1. The function is given by
131:0:Speech recognition (SR) is the inter-disciplinary sub-field of computational linguistics which incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields to develop methodologies and technologies that enables the recognition and translation of spoken language into text by computers and computerized devices such as those categorized as smart technologies and robotics. It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT). Some SR systems use "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent". Speech recognition applications include voice user interfaces such as voice dialing (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed Direct Voice Input). The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process. From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the world-wide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Google, Microsoft, IBM, Baidu (China), Apple, Amazon, Nuance, IflyTek (China), many of which have publicized the core technology in their speech recognition systems as being based on deep learning. == History == As early as 1932, Bell Labs researchers like Harvey Fletcher were investigating the science of speech perception. In 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words. Unfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce's letter compared speech recognition to "schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon." Pierce defunded speech recognition research at Bell Labs. Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess. Also around this time Soviet researchers invented the dynamic time warping algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. Achieving speaker independence was a major unsolved goal of researchers during this time period. In 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN. IBM., Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter. Despite the fact that CMU's Harpy system met the goals established at the outset of the program, many of the predictions turned out to be nothing more than hype disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis. During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. At CMU, Raj Reddy's students James Baker and Janet Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model. Under Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominate speech recognition algorithm in the 1980s. IBM had a few competitors including Dragon Systems founded by James and Janet Baker in 1982. The 1980s also saw the introduction of the n-gram language model. Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams. During the same time, also CSELT was using HMM (especially, the diphonies) to recognize language like Italian. At the same time, CSELT led a series of European projects (Esprit I, II), and summarized the state-of-the-art in a book, later (2013) reprinted. Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. Using these computers it could take up to 100 minutes to decode just 30 seconds of speech. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models. Another one of Raj Reddy's former students, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Huang went on to found the speech recognition group at Microsoft in 1993. The 1990s saw the first introduction of commercially successful speech recognition technologies. By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. In 2000, Lernout & Hauspie acquired Dragon Systems and was an industry leader until an accounting scandal brought an end to the company in 2001. The L&H speech technology was bought by ScanSoft which became Nuance in 2005. In 2011 Nuance acquired Loquendo, the company spin-off from the former CSELT speech technology group in 2001. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri. === 21st Century === In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, Cambridge University, and a team composed of ISCI, SRI and University of Washington. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages. In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program. In the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks. Today, however, many aspects of speech recognition have been taken over by a deep learning method called Long short term memory (LSTM), a recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech. Around 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users. The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979." In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well. In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 80's, 90's and a few years into 2000. But these methods never 
166:0:Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric. PCA is sensitive to the relative scaling of the original variables. PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed (and named) by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Kosambi-Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of ), Eckart–Young theorem (Harman, 1960), or Schmidt–Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics. PCA is mostly used as a tool in exploratory data analysis and for making predictive models. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute. The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score). PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection or "shadow" of this object when viewed from its (in some sense; see below) most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced. PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. == Intuition == PCA can be thought of as fitting an n-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipse is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information. To find the axes of the ellipse, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data, and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then, we must orthogonalize the set of eigenvectors, and normalize each to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues. It is important to note that this procedure is sensitive to the scaling of the data, and that there is no consensus as to how to best scale the data to obtain optimal results. == Details == PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. Consider a data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor). Mathematically, the transformation is defined by a set of p-dimensional vectors of weights or loadings that map each row vector of X to a new vector of principal component scores , given by in such a way that the individual variables of t considered over the data set successively inherit the maximum possible variance from x, with each loading vector w constrained to be a unit vector. === First component === The first loading vector w(1) thus has to satisfy Equivalently, writing this in matrix form gives Since w(1) has been defined to be a unit vector, it equivalently also satisfies The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a symmetric matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector. With w(1) found, the first component of a data vector x(i) can then be given as a score t1(i) = x(i) ⋅ w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) ⋅ w(1)} w(1). === Further components === The kth component can be found by subtracting the first k − 1 principal components from X: and then finding the loading vector which extracts the maximum variance from this new data matrix It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the loading vectors are eigenvectors of XTX. The kth component of a data vector x(i) can therefore be given as a score tk(i) = x(i) ⋅ w(k) in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, {x(i) ⋅ w(k)} w(k), where w(k) is the kth eigenvector of XTX. The full principal components decomposition of X can therefore be given as where W is a p-by-p matrix whose columns are the eigenvectors of XTX === Covariances === XTX itself can be recognised as proportional to the empirical sample covariance matrix of the dataset X. The sample covariance Q between two of the different principal components over the dataset is given by: where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset. Another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix. In matrix form, the empirical covariance matrix for the original variables can be written The empirical covariance matrix between the principal components becomes where Λ is the diagonal matrix of eigenvalues λ(k) of XTX (λ(k) being equal to the sum of the squares over the dataset associated with each component k: λ(k) = Σi tk2(i) = Σi (x(i) ⋅ w(k))2) === Dimensionality reduction === The transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L loading vectors, gives the truncated transformation where the matrix TL now has n rows but only L columns. In other words, PCA learns a linear transformation where the columns of p × L matrix W form an orthogonal basis for the L features (the components of representation t) that are decorrelated. By construction, of all the transformed data matrices with only L columns, this score matrix maximises the variance in the original data that has been preserved, while minimising the total squared reconstruction error or . Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L = 2 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable. Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression. Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total 
59:517:Peter Bowker (born 1958) is a British playwright and screenwriter. He is best known for the television serials Blackpool (2004), a musical drama about a shady casino owner; Occupation (2009), which follows three military servicemen adjusting to civilian life after a tour of duty in Iraq; and Desperate Romantics (2009), a biographical drama about the Pre-Raphaelite Brotherhood. In 2007, he adapted Blackpool for CBS as Viva Laughlin.
179:185:Gerardo Beni is a professor of electrical engineering at University of California, Riverside who, with Jin Wang, is known as the originator of the term 'swarm intelligence'  in the context of cellular robotics and the concept of 'electrowetting' , with Susan Hackwood. He also devised, with Xuan-Li Xie, the Xie-Beni Index  for measuring the validity of fuzzy clustering.
75:520:Evolutionary music is the audio counterpart to Evolutionary art, whereby algorithmic music is created using an evolutionary algorithm. The process begins with a population of individuals which by some means or other produce audio (e.g. a piece, melody, or loop), which is either initialized randomly or based on human-generated music. Then through the repeated application of computational steps analogous to biological selection, recombination and mutation the aim is for the produced audio to become more musical. Evolutionary sound synthesis is a related technique for generating sounds or synthesizer instruments. Evolutionary music is typically generated using an interactive evolutionary algorithm where the fitness function is the user or audience, as it is difficult to capture the aesthetic qualities of music computationally. However, research into automated measures of musical quality is also active. Evolutionary computation techniques have also been applied to harmonization and accompaniment tasks. The most commonly used evolutionary computation techniques are genetic algorithms and genetic programming.
82:1505:Memetic algorithms (MA) represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MA are also referred to in the literature as Baldwinian evolutionary algorithms (EA), Lamarckian EAs, cultural algorithms, or genetic local search.
134:874:Cytotoxicity is the quality of being toxic to cells. Examples of toxic agents are an immune cell or some types of venom, e.g. from the puff adder (Bitis arietans) or brown recluse spider (Loxosceles reclusa).
109:670:In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.
91:0:Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence developmental robotics also provides feedback and novel hypotheses on theories of human and animal development. Developmental robotics is related to, but differs from, evolutionary robotics (ER). ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time. DevRob is also related to work done in the domains of robotics and artificial life. == Background == Can a robot learn like a child? Can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment? How can it discover its body and its relationships with the physical and social environment? How can its cognitive capacities continuously develop without the intervention of an engineer once it is "out of the factory"? What can it learn through natural social interactions with humans? These are the questions at the center of developmental robotics. Alan Turing, as well as a number of other pioneers of cybernetics, already formulated those questions and the general approach in 1950, but it is only since the end of the 20th century that they began to be investigated systematically. Because the concept of adaptive intelligent machine is central to developmental robotics, it has relationships with fields such as artificial intelligence, machine learning, cognitive robotics or computational neuroscience. Yet, while it may reuse some of the techniques elaborated in these fields, it differs from them from many perspectives. It differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems. It differs from traditional machine learning because it targets task- independent self-determined learning rather than task-specific inference over "spoon fed human-edited sensori data" (Weng et al., 2001). It differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves. It differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning. More generally, developmental robotics is uniquely characterized by the following three features: It targets task-independent architectures and learning mechanisms, i.e. the machine/robot has to be able to learn new tasks that are unknown by the engineer; It emphasizes open-ended development and lifelong learning, i.e. the capacity of an organism to acquire continuously novel skills. This should not be understood as a capacity for learning "anything" or even “everything”, but just that the set of skills that is acquired can be infinitely extended at least in some (not all) directions; The complexity of acquired knowledge and skills shall increase (and the increase be controlled) progressively. Developmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self- organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines, but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development. == Research directions == === Skill domains === Due to the general approach and methodology, developmental robotics projects typically focus on having robots develop the same types of skills as human infants. A first category that is importantly being investigated is the acquisition of sensorimotor skills. These include the discovery of one's own body, including its structure and dynamics such as hand–eye coordination, locomotion, and interaction with objects as well as tool use, with a particular focus on the discovery and learning of affordances. A second category of skills targeted by developmental robots are social and linguistic skills: the acquisition of simple social behavioural games such as turn-taking, coordinated interaction, lexicons, syntax and grammar, and the grounding of these linguistic skills into sensorimotor skills (sometimes referred as symbol grounding). In parallel, the acquisition of associated cognitive skills are being investigated such as the emergence of the self/non-self distinction, the development of attentional capabilities, of categorization systems and higher-level representations of affordances or social constructs, of the emergence of values, empathy, or theories of mind. === Mechanisms and constraints === The sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a life-time. Thus, mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity. There are several important families of these guiding mechanisms and constraints which are studied in developmental robotics, all inspired by human development: Motivational systems, generating internal reward signals that drive exploration and learning, which can be of two main types: extrinsic motivations push robots/organisms to maintain basic specific internal properties such as food and water level, physical integrity, or light (e.g. in phototropic systems); intrinsic motivations push robot to search for novelty, challenge, compression or learning progress per se, thus generating what is sometimes called curiosity-driven learning and exploration, or alternatively active learning and exploration; Social guidance: as humans learn a lot by interacting with their peers, developmental robotics investigates mechanisms which can allow robots to participate to human-like social interaction. By perceiving and interpreting social cues, this may allow robots both to learn from humans (through diverse means such as imitation, emulation, stimulus enhancement, demonstration, etc. ...) and to trigger natural human pedagogy. Thus, social acceptance of developmental robots is also investigated; Statistical inference biases and cumulative knowledge/skill reuse: biases characterizing both representations/encodings and inference mechanisms can typically allow considerable improvement of the efficiency of learning and are thus studied. Related to this, mechanisms allowing to infer new knowledge and acquire new skills by reusing previously learnt structures is also an essential field of study; The properties of embodiment, including geometry, materials, or innate motor primitives/synergies often encoded as dynamical systems, can considerably simplify the acquisition of sensorimotor or social skills, and is sometimes referred as morphological computation. The interaction of these constraints with other constraints is an important axis of investigation; Maturational constraints: In human infants, both the body and the neural system grow progressively, rather than being full-fledged already at birth. This implies for example that new degress of freedom, as well as increases of the volume and resolution of available sensorimotor signals, may appear as learning and development unfold. Transposing these mechanisms in developmental robots, and understanding how it may hinder or on the contrary ease the acquisition of novel complex skills is a central question in developmental robotics. === From bio-mimetic development to functional inspiration. === While most developmental robotics projects strongly interact with theories of animal and human development, the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots, as well as the abstraction levels of modeling, may vary a lot. While some projects aim at modeling precisely both the function and biological implementation (neural or morphological models), such as in neurorobotics, some other projects only focus on functional modeling of the mechanisms and constraints described above, and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields. == Open questions == As developmental robotics is a relatively novel research field and at the same time very ambitious, many fundamental open challenges remain to be solved. First of all, existing techniques are far from allowing real-world high-dimensional robots to learn an open- ended repertoire of increasingly complex skills over a life-time period. High-dimensional continuous sensorimotor spaces are a major obstacle to be solved. Lifelong cumulative learning is another one. Actually, no experiments lasting more than a few days have been set up so far, which contrasts severely with the time period needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms. Among the strategies to explore in order to progress towards this target, the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically. Indeed, they have so far mainly been studied in isolation. For example, the interaction of intrinsically motivated learning and socially guided learning, possibly constrained by maturation, is an essential issue to be investigated. Another important challenge is to allow robots to perceive, interpret and leverage the diversity of multimodal social cues provided by non-engineer humans during human-robot interaction. These capacities are so far mostly too limited to allow efficient general purpose teaching from humans. A fundamental scientific issue to be understood and resolved, which applied equally to human development, is how compositionality, functional hierarchies, primitives, and modularity, at all levels of sensorimotor and social structures, can be formed and leveraged during development. This is deeply linked with the problem of the emergence of symbols, sometimes referred as the "symbol grounding problem" when it comes to language acquisition. Actually, the very existence and need for symbols in the brain is actively questioned, and alternative concepts, still allowing for compositionality and functional hierarchies are being investigated. During biological epigenesis, morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills. The development of morphology poses obvious practical problems with robots, but it may be a crucial mechanism that should be further explored, at least in simulation, such as in morphogenetic robotics. Another open problem is the understanding of the relation between the key phenomena investigated by developmental robotics (e.g., hierarchical and modular sensorimotor systems, intrinsic/extrinsic/social motivations, and open-ended learning) and the underlying brain 
111:689:CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases that is more robust to outliers and identifies clusters having non-spherical shapes and size variances.
160:369:Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involve random objective functions or random constraints, for example. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.
38:0:Computational neurogenetic modeling (CNGM) is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes. These include neural network models and their integration with gene network models. This area brings together knowledge from various scientific disciplines, such as computer and information science, neuroscience and cognitive science, genetics and molecular biology, as well as engineering. == Levels of processing == === Molecular kinetics === Models of the kinetics of proteins and ion channels associated with neuron activity represent the lowest level of modeling in a computational neurogenetic model. The altered activity of proteins in some diseases, such as the amyloid beta protein in Alzheimer's disease, must be modeled at the molecular level to accurately predict the effect on cognition. Ion channels, which are vital to the propagation of action potentials, are another molecule that may be modeled to more accurately reflect biological processes. For instance, to accurately model synaptic plasticity (the strengthening or weakening of synapses) and memory, it is necessary to model the activity of the NMDA receptor (NMDAR). The speed at which the NMDA receptor lets Calcium ions into the cell in response to Glutamate is an important determinant of Long-term potentiation via the insertion of AMPA receptors (AMPAR) into the plasma membrane at the synapse of the postsynaptic cell (the cell that receives the neurotransmitters from the presynaptic cell). === Genetic regulatory network === In most models of neural systems neurons are the most basic unit modeled. In computational neurogenetic modeling, to better simulate processes that are responsible for synaptic activity and connectivity, the genes responsible are modeled for each neuron. A gene regulatory network, protein regulatory network, or gene/protein regulatory network, is the level of processing in a computational neurogenetic model that models the interactions of genes and proteins relevant to synaptic activity and general cell functions. Genes and proteins are modeled as individual nodes, and the interactions that influence a gene are modeled as excitatory (increases gene/protein expression) or inhibitory (decreases gene/protein expression) inputs that are weighted to reflect the effect a gene or protein is having on another gene or protein. Gene regulatory networks are typically designed using data from microarrays. Modeling of genes and proteins allows individual responses of neurons in an artificial neural network that mimic responses in biological nervous systems, such as division (adding new neurons to the artificial neural network), creation of proteins to expand their cell membrane and foster neurite outgrowth (and thus stronger connections with other neurons), up-regulate or down-regulate receptors at synapses (increasing or decreasing the weight (strength) of synaptic inputs), uptake more neurotransmitters, change into different types of neurons, or die due to necrosis or apoptosis. The creation and analysis of these networks can be divided into two sub-areas of research: the gene up-regulation that is involved in the normal functions of a neuron, such as growth, metabolism, and synapsing; and the effects of mutated genes on neurons and cognitive functions. === Artificial neural network === An artificial neural network generally refers to any computational model that mimics the central nervous system, with capabilities such as learning and pattern recognition. With regards to computational neurogenetic modeling, however, it is often used to refer to those specifically designed for biological accuracy rather than computational efficiency. Individual neurons are the basic unit of an artificial neural network, with each neuron acting as a node. Each node receives weighted signals from other nodes that are either excitatory or inhibitory. To determine the output, a transfer function (or activation function) evaluates the sum of the weighted signals and, in some artificial neural networks, their input rate. Signal weights are strengthened (long-term potentiation) or weakened (long-term depression) depending on how synchronous the presynaptic and postsynaptic activation rates are (Hebbian theory). The synaptic activity of individual neurons is modeled using equations to determine the temporal (and in some cases, spatial) summation of synaptic signals, membrane potential, threshold for action potential generation, the absolute and relative refractory period, and optionally ion receptor channel kinetics and Gaussian noise (to increase biological accuracy by incorporation of random elements). In addition to connectivity, some types of artificial neural networks, such as spiking neural networks, also model the distance between neurons, and its effect on the synaptic weight (the strength of a synaptic transmission). === Combining gene regulatory networks and artificial neural networks === For the parameters in the gene regulatory network to affect the neurons in the artificial neural network as intended there must be some connection between them. In an organizational context, each node (neuron) in the artificial neural network has its own gene regulatory network associated with it. The weights (and in some networks, frequencies of synaptic transmission to the node), and the resulting membrane potential of the node (including whether an action potential is produced or not), affect the expression of different genes in the gene regulatory network. Factors affecting connections between neurons, such as synaptic plasticity, can be modeled by inputting the values of synaptic activity-associated genes and proteins to a function that re-evaluates the weight of an input from a particular neuron in the artificial neural network. === Incorporation of other cell types === Other cell types besides neurons can be modeled as well. Glial cells, such as astroglia and microglia, as well as endothelial cells, could be included in an artificial neural network. This would enable modeling of diseases where pathological effects may occur from sources other than neurons, such as Alzheimer's disease. == Factors affecting choice of artificial neural network == While the term artificial neural network is usually used in computational neurogenetic modeling to refer to models of the central nervous system meant to possess biological accuracy, the general use of the term can be applied to many gene regulatory networks as well. === Time variance === Artificial neural networks, depending on type, may or may not take into account the timing of inputs. Those that do, such as spiking neural networks, fire only when the pooled inputs reach a membrane potential is reached. Because this mimics the firing of biological neurons, spiking neural networks are viewed as a more biologically accurate model of synaptic activity. === Growth and shrinkage === To accurately model the central nervous system, creation and death of neurons should be modeled as well. To accomplish this, constructive artificial neural networks that are able to grow or shrink to adapt to inputs are often used. Evolving connectionist systems are a subtype of constructive artificial neural networks (evolving in this case referring to changing the structure of its neural network rather than by mutation and natural selection). === Randomness === Both synaptic transmission and gene-protein interactions are stochastic in nature. To model biological nervous systems with greater fidelity some form of randomness is often introduced into the network. Artificial neural networks modified in this manner are often labeled as probabilistic versions of their neural network sub-type (e.g., pSNN). === Incorporation of fuzzy logic === Fuzzy logic is a system of reasoning that enables an artificial neural network to deal in non-binary and linguistic variables. Biological data is often unable to be processed using Boolean logic, and moreover accurate modeling of the capabilities of biological nervous systems requires fuzzy logic. Therefore, artificial neural networks that incorporate it, such as evolving fuzzy neural networks (EFuNN) or Dynamic Evolving Neural-Fuzzy Inference Systems (DENFIS), are often used in computational neurogenetic modeling. The use of fuzzy logic is especially relevant in gene regulatory networks, as the modeling of protein binding strength often requires non-binary variables. === Types of learning === Artificial Neural Networks designed to simulate of the human brain require an ability to learn a variety of tasks that is not required by those designed to accomplish a specific task. Supervised learning is a mechanism by which an artificial neural network can learn by receiving a number of inputs with a correct output already known. An example of an artificial neural network that uses supervised learning is a multilayer perceptron (MLP). In unsupervised learning, an artificial neural network is trained using only inputs. Unsupervised learning is the learning mechanism by which a type of artificial neural network known as a self-organizing map (SOM) learns. Some types of artificial neural network, such as evolving connectionist systems, can learn in both a supervised and unsupervised manner. == Improvement == Both gene regulatory networks and artificial neural networks have two main strategies for improving their accuracy. In both cases the output of the network is measured against known biological data using some function, and subsequent improvements are made by altering the structure of the network. A common test of accuracy for artificial neural networks is to compare some parameter of the model to data acquired from biological neural systems, such as from an EEG. In the case of EEG recordings, the local field potential (LFP) of the artificial neural network is taken and compared to EEG data acquired from human patients. The relative intensity ratio (RIRs) and fast Fourier transform (FFT) of the EEG are compared with those generated by the artificial neural networks to determine the accuracy of the model. === Genetic algorithm === Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model, evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene regulatory network is: first, create a population; next, to create offspring via a crossover operation and evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator; finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. === Evolving systems === Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient. == Potential applications 
157:927:In applied mathematics, multimodal optimization deals with optimization tasks that involve finding all or most of the multiple (at least locally optimal) solutions of a problem, as opposed to a single best solution. Evolutionary multimodal optimization is a branch of Evolutionary computation, which is closely related to Machine learning. Wong provides a short survey, wherein the chapter of Shir and the book of Preuss cover the topic in more detail.
49:329:In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice-versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).
24:781:NeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures ("complexifying").
55:384:This article is on evolutionary acquisition of artificial neural topologies, not of natural ones.
102:1379:In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:
58:953:The College of Information Sciences and Technology, also known as IST, was opened in 1999 in response to the rapidly growing need in almost every field for education, leadership, and research in information sciences and technology. The college's goal is to educate a new generation of leaders, researchers, educators, technologists and scholars that are equipped to address the challenges presented by the complex interplay of information, technology, and people in the Digital Age.
108:0:A globular cluster is a spherical collection of stars that orbits a galactic core as a satellite. Globular clusters are very tightly bound by gravity, which gives them their spherical shapes and relatively high stellar densities toward their centers. The name of this category of star cluster is derived from the Latin globulus—a small sphere. A globular cluster is sometimes known more simply as a globular. Globular clusters, which are found in the halo of a galaxy, contain considerably more stars and are much older than the less dense galactic, or open clusters, which are found in the disk. Globular clusters are fairly common; there are about 150 to 158 currently known globular clusters in the Milky Way, with perhaps 10 to 20 more still undiscovered. These globular clusters orbit the Galaxy at radii of 40 kiloparsecs (130,000 light-years) or more. Larger galaxies can have more: Andromeda, for instance, may have as many as 500. Some giant elliptical galaxies, particularly those at the centers of galaxy clusters, such as M87, have as many as 13,000 globular clusters. Every galaxy of sufficient mass in the Local Group has an associated group of globular clusters, and almost every large galaxy surveyed has been found to possess a system of globular clusters. The Sagittarius Dwarf galaxy and the disputed Canis Major Dwarf galaxy appear to be in the process of donating their associated globular clusters (such as Palomar 12) to the Milky Way. This demonstrates how many of this galaxy's globular clusters might have been acquired in the past. Although it appears that globular clusters contain some of the first stars to be produced in the galaxy, their origins and their role in galactic evolution are still unclear. It does appear clear that globular clusters are significantly different from dwarf elliptical galaxies and were formed as part of the star formation of the parent galaxy rather than as a separate galaxy. == Observation history == The first globular cluster discovered was M22 in 1665 by Abraham Ihle, a German amateur astronomer. However, given the small aperture of early telescopes, individual stars within a globular cluster were not resolved until Charles Messier observed M4. The first eight globular clusters discovered are shown in the table. Subsequently, Abbé Lacaille would list NGC 104, NGC 4833, M55, M69, and NGC 6397 in his 1751–52 catalogue. The M before a number refers to the catalogue of Charles Messier, while NGC is from the New General Catalogue by John Dreyer. When William Herschel began his comprehensive survey of the sky using large telescopes in 1782 there were 34 known globular clusters. Herschel discovered another 36 himself and was the first to resolve virtually all of them into stars. He coined the term "globular cluster" in his Catalogue of a Second Thousand New Nebulae and Clusters of Stars published in 1789. The number of globular clusters discovered continued to increase, reaching 83 in 1915, 93 in 1930 and 97 by 1947. A total of 152 globular clusters have now been discovered in the Milky Way galaxy, out of an estimated total of 180 ± 20. These additional, undiscovered globular clusters are believed to be hidden behind the gas and dust of the Milky Way. Beginning in 1914, Harlow Shapley began a series of studies of globular clusters, published in about 40 scientific papers. He examined the RR Lyrae variables in the clusters (which he assumed were cepheid variables) and would use their period–luminosity relationship for distance estimates. Later, it was found that RR Lyrae variables are fainter than cepheid variables, which caused Shapley to overestimate the distance to the clusters. Of the globular clusters within the Milky Way, the majority are found in the vicinity of the galactic core, and the large majority lie on the side of the celestial sky centered on the core. In 1918, this strongly asymmetrical distribution was used by Harlow Shapley to make a determination of the overall dimensions of the galaxy. By assuming a roughly spherical distribution of globular clusters around the galaxy's center, he used the positions of the clusters to estimate the position of the sun relative to the galactic center. While his distance estimate was in significant error, it did demonstrate that the dimensions of the galaxy were much greater than had been previously thought. His error was because dust in the Milky Way diminished the amount of light from a globular cluster that reached the earth, thus making it appear farther away. Shapley's estimate was, however, within the same order of magnitude as the currently accepted value. Shapley's measurements also indicated that the Sun was relatively far from the center of the galaxy, contrary to what had previously been inferred from the apparently nearly even distribution of ordinary stars. In reality, ordinary stars lie within the galaxy's disk and are thus often obscured by gas and dust, whereas globular clusters lie outside the disk and can be seen at much further distances. === Classification of globulars === Shapley was subsequently assisted in his studies of clusters by Henrietta Swope and Helen Battles Sawyer (later Hogg). In 1927–29, Harlow Shapley and Helen Sawyer began categorizing clusters according to the degree of concentration the system has toward the core. The most concentrated clusters were identified as Class I, with successively diminishing concentrations ranging to Class XII. This became known as the Shapley–Sawyer Concentration Class (it is sometimes given with numbers [Class 1–12] rather than Roman numerals.) In 2015, a new type of globular cluster was proposed on the basis of observational data, the dark globular clusters. == Formation == The formation of globular clusters remains a poorly understood phenomenon and it remains uncertain whether the stars in a globular cluster form in a single generation or are spawned across multiple generations over a period of several hundred million years. In many globular clusters, most of the stars are at approximately the same stage in stellar evolution, suggesting that they formed at about the same time. However, the star formation history varies from cluster to cluster, with some clusters showing distinct populations of stars. An example of this is the globular clusters in the Large Magellanic Cloud (LMC) that exhibit a bimodal population. During their youth, these LMC clusters may have encountered giant molecular clouds that triggered a second round of star formation. This star-forming period is relatively brief, compared to the age of many globular clusters. It has also been proposed that the reason for this multiplicity in stellar populations could have a dynamical origin. In the Antennae galaxy, for example, the Hubble Space Telescope has observed clusters of clusters, regions in the galaxy that span hundreds of parsecs, where many of the clusters will eventually collide and merge. Many of them present a significant range in ages, hence possibly metallicities, and their merger could plausibly lead to clusters with a bimodal or even multiple distribution of populations. Observations of globular clusters show that these stellar formations arise primarily in regions of efficient star formation, and where the interstellar medium is at a higher density than in normal star-forming regions. Globular cluster formation is prevalent in starburst regions and in interacting galaxies. Research indicates a correlation between the mass of a central supermassive black holes (SMBH) and the extent of the globular cluster systems of elliptical and lenticular galaxies. The mass of the SMBH in such a galaxy is often close to the combined mass of the galaxy's globular clusters. No known globular clusters display active star formation, which is consistent with the view that globular clusters are typically the oldest objects in the Galaxy, and were among the first collections of stars to form. Very large regions of star formation known as super star clusters, such as Westerlund 1 in the Milky Way, may be the precursors of globular clusters. == Composition == Globular clusters are generally composed of hundreds of thousands of low-metal, old stars. The type of stars found in a globular cluster are similar to those in the bulge of a spiral galaxy but confined to a volume of only a few million cubic parsecs. They are free of gas and dust and it is presumed that all of the gas and dust was long ago turned into stars. Globular clusters can contain a high density of stars; on average about 0.4 stars per cubic parsec, increasing to 100 or 1000 stars per cubic parsec in the core of the cluster. The typical distance between stars in a globular cluster is about 1 light year, but at its core, the separation is comparable to the size of the Solar System (100 to 1000 times closer than stars near the Solar System). However, they are not thought to be favorable locations for the survival of planetary systems. Planetary orbits are dynamically unstable within the cores of dense clusters because of the perturbations of passing stars. A planet orbiting at 1 astronomical unit around a star that is within the core of a dense cluster such as 47 Tucanae would only survive on the order of 108 years. There is a planetary system orbiting a pulsar (PSR B1620−26) that belongs to the globular cluster M4, but these planets likely formed after the event that created the pulsar. Some globular clusters, like Omega Centauri in the Milky Way and G1 in M31, are extraordinarily massive, with several million solar masses (M☉) and multiple stellar populations. Both can be regarded as evidence that supermassive globular clusters are in fact the cores of dwarf galaxies that are consumed by the larger galaxies. About a quarter of the globular cluster population in the Milky Way may have been accreted along with their host dwarf galaxy. Several globular clusters (like M15) have extremely massive cores which may harbor black holes, although simulations suggest that a less massive black hole or central concentration of neutron stars or massive white dwarfs explain observations equally well. === Metallic content === Globular clusters normally consist of Population II stars, which have a low proportion of elements other than hydrogen and helium when compared to Population I stars such as the Sun. Astronomers refer to these heavier elements as metals and to the proportions of these elements as the metallicity. These elements are produced by stellar nucleosynthesis and then are recycled into the interstellar medium, where they enter the next generation of stars. Hence the proportion of metals can be an indication of the age of a star, with older stars typically having a lower metallicity. The Dutch astronomer Pieter Oosterhoff noticed that there appear to be two populations of globular clusters, which became known as Oosterhoff groups. The second group has a slightly longer period of RR Lyrae variable stars. Both groups have weak lines of metallic elements. But the lines in the stars of Oosterhoff type I 
17:885:In evolutionary computation, a human-based genetic algorithm (HBGA) is a genetic algorithm that allows humans to contribute solution suggestions to the evolutionary process. For this purpose, a HBGA has human interfaces for initialization, mutation, and recombinant crossover. As well, it may have interfaces for selective evaluation. In short, a HBGA outsources the operations of a typical genetic algorithm to humans.
106:0:Clustering is the assignment of objects into groups (called clusters) so that objects from the same cluster are more similar to each other than objects from different clusters. Often similarity is assessed according to a distance measure. Clustering is a common technique for statistical data analysis, which is used in many fields, including machine learning, data mining, pattern recognition, image analysis and bioinformatics. Consensus clustering has emerged as an important elaboration of the classical clustering problem. Consensus clustering, also called aggregation of clustering (or partitions), refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Consensus clustering is thus the problem of reconciling clustering information about the same data set coming from different sources or from different runs of the same algorithm. When cast as an optimization problem, consensus clustering is known as median partition, and has been shown to be NP-complete. Consensus clustering for unsupervised learning is analogous to ensemble learning in supervised learning. == Issues with existing clustering techniques == Current clustering techniques do not address all the requirements adequately. Dealing with large number of dimensions and large number of data items can be problematic because of time complexity; Effectiveness of the method depends on the definition of "distance" (for distance based clustering) If an obvious distance measure doesn’t exist we must "define" it, which is not always easy, especially in multidimensional spaces. The result of the clustering algorithm (that in many cases can be arbitrary itself) can be interpreted in different ways. == Justification for using consensus clustering == There are potential shortcomings for all existing clustering techniques. This may cause interpretation of results to become difficult, especially when there is no knowledge about the number of clusters. Clustering methods are also very sensitive to the initial clustering settings, which can cause non-significant data to be amplified in non-reiterative methods. An extremely important issue in cluster analysis is the validation of the clustering results, that is, how to gain confidence about the significance of the clusters provided by the clustering technique (cluster numbers and cluster assignments). Lacking an external objective criterion (the equivalent of a known class label in supervised analysis), this validation becomes somewhat elusive. Iterative descent clustering methods, such as the SOM and K-means clustering circumvent some of the shortcomings of Hierarchical clustering by providing for univocally defined clusters and cluster boundaries. Consensus clustering provides a method that represents the consensus across multiple runs of a clustering algorithm, to determine the number of clusters in the data, and to assess the stability of the discovered clusters. The method can also be used to represent the consensus over multiple runs of a clustering algorithm with random restart (such as K-means, model-based Bayesian clustering, SOM, etc.), so as to account for its sensitivity to the initial conditions. It can provide data for a visualization tool to inspect cluster number, membership, and boundaries. However, they lack the intuitive and visual appeal of Hierarchical clustering dendrograms, and the number of clusters must be chosen a priori. == Over-interpretation potential of consensus clustering == Consensus clustering can be a powerful tool for identifying clusters, but it needs to be applied with caution. It has been shown that consensus clustering is able to claim apparent stability of chance partitioning of null datasets drawn from a unimodal distribution, and thus has the potential to lead to over-interpretation of cluster stability in a real study. If clusters are not well separated, consensus clustering could lead one to conclude apparent structure when there is none, or declare cluster stability when it is subtle. To reduce the false positive potential in clustering samples (observations), Şenbabaoğlu et al recommends (1) doing a formal test of cluster strength using simulated unimodal data with the same feature-space correlation structure as in the empirical data, (2) not relying solely on the consensus matrix heatmap to declare the existence of clusters, or to estimate optimal K, (3) applying the proportion of ambiguous clustering (PAC) as a simple yet powerful method to infer optimal K. PAC: In the CDF curve of a consensus matrix, the lower left portion represents sample pairs rarely clustered together, the upper right portion represents those almost always clustered together, whereas the middle segment represent those with ambiguous assignments in different clustering runs. The "proportion of ambiguous clustering" (PAC) measure quantifies this middle segment; and is defined as the fraction of sample pairs with consensus indices falling in the interval (u1, u2) ∈ [0, 1] where u1 is a value close to 0 and u2 is a value close to 1 (for instance u1=0.1 and u2=0.9). A low value of PAC indicates a flat middle segment, and a low rate of discordant assignments across permuted clustering runs. We can therefore infer the optimal number of clusters by the K value having the lowest PAC. In simulated datasets with known number of clusters, consensus clustering+PAC has been shown to perform better than several other commonly used methods such as consensus clustering+Δ(K), CLEST, GAP, and silhouette width. == Related work == 1. Clustering ensemble (Strehl and Ghosh): They considered various formulations for the problem, most of which reduce the problem to a hyper-graph partitioning problem. In one of their formulations they considered the same graph as in the correlation clustering problem. The solution they proposed is to compute the best k-partition of the graph, which does not take into account the penalty for merging two nodes that are far apart. 2. Clustering aggregation (Fern and Brodley): They applied the clustering aggregation idea to a collection of soft clusterings they obtained by random projections. They used an agglomerative algorithm and did not penalize for merging dissimilar nodes. 3. Fred and Jain: They proposed to use a single linkage algorithm to combine multiple runs of the k-means algorithm. 4. Dana Cristofor and Dan Simovici: They observed the connection between clustering aggregation and clustering of categorical data. They proposed information theoretic distance measures, and they propose genetic algorithms for finding the best aggregation solution. 5. Topchy et al.: They defined clustering aggregation as a maximum likelihood estimation problem, and they proposed an EM algorithm for finding the consensus clustering. 6. Abu-Jamous et al.: They proposed their binarization of consensus partition matrices (Bi-CoPaM) method to enhance ensemble clustering in two major aspects. The first is to consider clustering the same set of objects by various clustering methods as well as by considering their features measured in multiple datasets; this seems perfectly relevant in the context of microarray gene expression clustering, which is the context they initially proposed the method in. The second aspect is the format of the final result; based on the consistency of inclusion of a data object in the same cluster by the multiple single clustering results, they allowed any single data object to have any of the three eventualities; to be exclusively assigned to one and only one cluster, to be unassigned from all clusters, or to be simultaneously assigned to multiple clusters at the same time. They made it possible to produce, in a perfectly tunable way, wide overlapping clusters, tight specific clusters, as well as complementary clusters. Therefore, they proposed their work as a new paradigm of clustering rather than merely a new ensemble clustering method. == Hard ensemble clustering == This approach by Strehl and Ghosh introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. They discuss three approaches towards solving this problem to obtain high quality consensus functions. Their techniques have low computational costs and this makes it feasible to evaluate each of the techniques discussed below and arrive at the best solution by comparing the results against the objective function. === Efficient consensus functions === 1. Cluster-based similarity partitioning algorithm (CSPA) In CSPA the similarity between two data-points is defined to be directly proportional to number of constituent clusterings of the ensemble in which they are clustered together. The intuition is that the more similar two data-points are the higher is the chance that constituent clusterings will place them in the same cluster. CSPA is the simplest heuristic, but its computational and storage complexity are both quadratic in n. The following two methods are computationally less expensive: 2. Hyper-graph partitioning algorithm (HGPA) The HGPA algorithm takes a very different approach to finding the consensus clustering than the previous method. The cluster ensemble problem is formulated as partitioning the hypergraph by cutting a minimal number of hyperedges. They make use of hMETIS which is a hypergraph partitioning package system. 3. Meta-clustering algorithm (MCLA) The meta-cLustering algorithm (MCLA) is based on clustering clusters. First, it tries to solve the cluster correspondence problem and then uses voting to place data-points into the final consensus clusters. The cluster correspondence problem is solved by grouping the clusters identified in the individual clusterings of the ensemble. The clustering is performed using METIS and Spectral clustering. == Soft clustering ensembles == Punera and Ghosh extended the idea of hard clustering ensembles to the soft clustering scenario. Each instance in a soft ensemble is represented by a concatenation of r posterior membership probability distributions obtained from the constituent clustering algorithms. We can define a distance measure between two instances using the Kullback–Leibler (KL) divergence, which calculates the “distance” between two probability distributions. 1. sCSPA sCSPA extends CSPA by calculating a similarity matrix. Each object is visualized as a point in dimensional space, with each dimension corresponding to probability of its belonging to a cluster. This technique first transforms the objects into a label-space and then interprets the dot product between the vectors representing the objects as their similarity. 2. sMCLA sMCLA extends MCLA by accepting soft clusterings as input. sMCLA’s working can be divided into the following steps: Construct Soft Meta-Graph of Clusters Group the Clusters into Meta-Clusters Collapse Meta-Clusters using Weighting Compete for Objects 3. sHBGF HBGF represents the ensemble as a bipartite graph with clusters and instances as nodes, and edges between the instances and the clusters they belong to. This approach can be trivially adapted to consider soft ensembles since the graph partitioning algorithm METIS accepts weights on the edges of the graph to be partitioned. In sHBGF, the graph has n + t vertices, where t is the total number of underlying clusters. 4. Bayesian consensus clustering (BCC) BCC defines a fully Bayesian model for soft consensus clustering in which multiple source clusterings, defined by different input data or different probability models, are assumed to adhere loosely to a consensus clustering. The full posterior for the separate clusterings, and the consensus clustering, are inferred simultaneously via Gibbs sampling. == 
175:703:OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision, originally developed by Intel's research center in Nizhny Novgorod (Russia), later supported by Willow Garage and now maintained by Itseez. The library is cross-platform and free for use under the open-source BSD license.
46:278:The Generalized Hebbian Algorithm (GHA), also known in the literature as Sanger's rule, is a linear feedforward neural network model for unsupervised learning with applications primarily in principal components analysis. First defined in 1989, it is similar to Oja's rule in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons.
