When you see Sulley’s soft, blue fur in Monster’s University, you’d be forgiven for thinking you can reach out and touch it. It’s hard to imagine that only a few decades ago, animators were hand-inking plates for animated films and cartoons, or relying on stop-animation to create realistic effects. Now, the adventures of Sulley and Mikey can be realised in full, jaw-dropping 3D.

The birth of 3D animation was a long, drawn-out process and has cost billions and years to develop. James Cameron notably waited 20 years until 3D animation was sufficiently advanced before starting filming of his 2009 Avatar film. 3D is big business now and is constantly being advanced with every new released.

So how did CGI get to where it is today?

 

The early days

The first computer animated film was basic to say the least, but paved the way for what would become one of the most successful animation methods. Charles Csuri and James Shaffer’s simplistic Hummingbird was created in 1967 and featured a pre-programmed drawing of a hummingbird. An early computer generated more than 30,000 images for the animation.



Next up was Kitty in 1968, created by a group of Russian mathematicians. The enormous BESM-4 computer, which had only 45 bits of internal memory and was built using transistors, was used to create the animation. The bulky technology printed hundreds of frames, which were then converted into film, by use of a specially-created program.



It wasn’t until 1972 that what many consider as being the first ‘true’ computer animated film was created, by none other than one of the founders of GCI giant Pixar, Ed Catmull.

A Computer Animated Hand demonstrated the capabilities of computer animation by rendering a hand entirely in 3D graphics. The short film showed exactly how the effect was achieved – it was the acorn from which great oaks would grow.



 

On the big screen

The first ever instance of CGI in feature-length films came along in 1973, in Yul Bryner flick Westworld. Pixelated POV shots were created with CGI – by colour-separating each frame of footage and scanning it to be converted into the pixel effect. Colour was then added to complete the effect.



In the sequel, the 1976 film Futureworld, more CGI imagery was used. 2D compositing was employed to materialise characters over certain backgrounds. The previously-mentioned A Computer Animated Hand would make an appearance on the big screen, with the same technology being used for a face.



After this, wire-frame 3D animation was used in Star Wars Episode IV: A New Hope in 1977 and The Black Hole and Alien in 1979.

In 1982, Disney’s Tron would extensively use 3D CGI – for more than 15 minutes – to create special effects. The Light Cycle sequence would go on to become one of the most famous CGI scenes in animation history.



The 80s would see an increasing use of CGI, but it wasn’t until the 90s that CGI animation started to really come into its own.

 

The golden age of CGI

Terminator 2: Judgement Day in 1991 showcased exactly what CGI was capable of with pioneering multiple morphing effects for its partially computer-generated main character of the liquid metal, shape shifting T-1000.



Pixar, a subsidiary of Lucasfilm, presented its ground-breaking form of entirely GCI-rendered animation with the John Lasseter-fronted short The Adventures of Andre and Wally B, but it wasn’t until 1988 that the animation company started to make a buzz for its short Tin Toy.

The use of the cell shading and rendering in that short would be continued into the flagship CGI feature, Toy Story in 1995. Having been approached by Disney after the success of Tin Toy, Toy Story would put Pixar firmly on the map.



Thanks to technological advancements at the time and to the methods tried and tested in the films previously mentioned, Pixar was able to create a whole new world with character, depth and charm. Using a complex system of model articulation and motion control coding, each character required its own set of motion controls. Woody himself required 723.

Each shot went through a series of computer-run teams, including a render process completed by a set of 117 Sun Microsystems that had to run 24 hours a day to complete the film. The animation was finished at an average of around three minutes per week.

On the back of Toy Story’s success, a sudden surge in 3D-rendering software boomed. The next decade would see a wealth of progressively more complex CGI films appear – including Pixar’s second CGI film A Bug’s Life in 1998, Final Fantasy: the Spirits Within in 2001, The Polar Express in 2004, Chicken Little in 2005 and motion-capture CGI film Beowulf.

James Cameron’s decade-in-the-making magnum opus Avatar used an advanced version of previous CGI and motion capture techniques, at one point employing nearly 900 people to do so. In order to render the film, animators required 2,000 Hewlett-Packard servers, sporting 35,000 processor cores and 104 terabytes of RAM.

That’s a long way from the 45 bits of memory that created Kitty.

 

Where next?

With the promise of more CGI films in the near future, including The Smurfs 2 and Cloudy With a Chance of Meatballs 2 and 3D viewing technology taking huge leaps and bounds with every feature, there’s no sign that CGI animation has had its day.

From explosions to lighting effects, the use of computers to create film effects and animation offers so much more scope for the creation of brilliant cinematography. Who knows what kind of amazing things we can expect to see on the big screen next?
