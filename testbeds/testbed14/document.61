Natural language generation

This paper has focused on natural language understanding or interpretation. A natural language processor in the full sense should be able to not only interpret or understand natural language sentences, but also generate them and participate in a conversation (a natural dialogue) with another natural language processor, whether human or computer. But initiating a conversation is more than just being directed to understand it:

The big question to answer in defining a conversational agent is, why should the agent ever speak? What motivates it to say anything or to attempt to comprehend the utterances that are said to it?

A very simple NLP that engaged in conversation might do so because it was programmed to do so, in the manner of a database question-answering agent. Given an input question, the agent would interpret it, search for the answer in the database, and generate output providing the answer. But of course, Allen notes, this type of simple agent wouldn't be considered very intelligent or conversational. This type of agent would have no chance of passing the Turing test, for example, because it wouldn't be flexible and wouldn't seem at all able to generate an independent response or initiate a line of dialogue.

Allen notes that we can give some intelligence to a system if we can act in a way that more closely matches what motivates human behavior. People act to achieve goals. A way to model this is use the concepts of perceptions (of the world).

To get the knowledge base earlier mentioned to function as the beliefs of the agent, it's best to divide up the knowledge base into belief spaces. Each proposition could be a belief. Two spaces would be useful for a conversation, one for the agent's beliefs and the other to represent its beliefs about the other agent's beliefs. In particular, the agent must be able to recognize the other agent's intentions, and for this, plan recognition can be used. Allen discusses the notion of speech acts in discussing a notion of a discourse plan that would be able to control a dialogue.

The computer is going to act in a deterministic fashion in accordance with its program, so that it must be programmed when to initiate a conversation, for example. It's not going to be all that far off, then, from the simple database program alluded to earlier. Of course, some randomizing function could be built into the program, so that it can "choose" from among several alternatives in responding to or initiating dialogue. Given that with respect to the subject of human free will and choice we do not seem to have resolved the debate between libertarianism, compatibilism, and hard determinism, the fact that even a very sophisticated natural language processing computer will probably act in a deterministic fashion may not preclude it from engaging in conversation in the same way a human does.

Can computers understand natural languages yet?

Early efforts at NLP include the National Research Council attempt in the late forties or early fifties to develop a system that could translate among human languages. Full fluency was predicted within five years. The theory behind this optimism stemmed from the success of code-breaking efforts during World War II, which led people to believe that human languages were just different coding systems for the same meaning. Application of the appropriate transformational rules should enable conversion from one language to another. The method used an automatic dictionary lookup and application of grammar rules to rearrange the word equivalents obtained from the dictionary. There was some awareness that ambiguities and idioms might present problems, requiring the involvement of some manual editing. The mathematician Warren Weaver of the Rockefeller Foundation thought it might be necessary to first translate into an intermediate language (whether there really was such a thing underlying natural languages or it had to be created). But efforts met with no real success. For example, from the mid-fifties came the following translation of "In recent times, Boolean algebra has been successfully employed in the analysis of relay networks of the series-parallel type." The program listed alternatives when it was uncertain of the translation.

(In, At, Into, For, On) (last, latter, new, latest, lowest, worst) (time, tense) for analysis and synthesis relay-contact electrical (circuit, diagram, scheme) parallel-(series, successive, consecutive, consistent) (connections, junction, combination) (with, from) (success, luck) (to be utilized, to be taken advantage of) apparatus Boolean algebra.

In 1966, after spending $20 million, the NRC's Automated Language Processing Advisory Committee recommended no further funding for the project. Instead, they thought, the focus of funding should shift to the study of language understanding.

We will not delve into progress in speech recognition. Perhaps the next oft-cited step in the other aspects of natural language processing was ELIZA, developed by Joseph Weizenbaum in the sixties. ELIZA could take text input, use pattern matching and substitution of "me" and "you" where needed, insert some canned leading phrases such as "Tell me more aboutï¿½" and "Why do you think so," and produce output that appeared to the user to be coming from an nondirective Rogerian therapist. This program could give the appearance of doing natural language processing, but its syntactic, semantic, and pragmatic analyses were primitive or virtually non-existent, so it was really just a clever party game, which seems to have been close to Weizenbaum's original intent anyway. Because it acted like a "client-centered" therapist, ELIZA could spit back at you anything you gave it that it couldn't process. In terms of breakthroughs in NLP, it appears to me to be not all that significant, except maybe as a commentary on the replacability of therapists using the client-centered methods of Carl Rogers.

In the seventies Roger Schank developed MARGIE, which reduced all English verbs to eleven semantic primitives (such as ATRANS, or Abstract Transfer, and PTRANS, or Physical Transfer). This sort of reduction enable MARGIE to make inferences about the implications of information it was given, because it would know what sorts of things would happen depending on the semantic primitive involved in the input sentence. This was developed further into the notion of Scripts, which we mentioned above. The idea was that the computer could be given background information (a SCRIPT) about what sorts of things happened in typical everyday scenarios, and it would then infer information not explicitly provided. Humans do this all the time. MARGIE gave way to SAM (Script Applier Mechanism), which was able to translate limited sentences from a variety of languages (English, Chinese, Russian, Dutch, and Spanish).

In the late seventies, Scripts resulted in PAM, for Plan Applier Mechanism, from the work of Schank, Abelson, and Wilensky. PAM interpreted stories in terms of the goals of the different participants involved. Later followed a variety of variants by students of these guys, including FRUMP, which was used to summarize news stories by UPI. Still, adequate consistent translation was lacking, as when FRUMP read a story about how a political assassination had shaken America and summarized the story as about an earthquake.

At the present time, a number of natural language processing programs have been developed, both by university research centers (on AI or computational linguistics) or by private companies. Most of these have very restricted domains, that is, they can only handle conversations about limited topics. Suffice it to say that with respect to a natural language processing system that can converse with a human about any topic likely to come up in conversation, we are not there yet.

One problem is that it is tedious to try to get into the computer a large lexicon, and maintain and update this lexicon. Structuring the rules of a natural language grammar is also a great task. But there is also the problem of general world knowledge. Much of the problem stems from the lack of common sense knowledge on the part of the computer. Maybe Doug Lenat's CYC database, if it is every "finished," will help. But it seems to me a few reasonably competent philosophers could quickly find common sense knowledge not encoded into the database.

What we need, it seems to me, is a way for the computer to learn common sense knowledge the way we do, by experiencing the world. Some researchers believe this too, and so work continues on the topic of machine learning.

More current information about the state of the art on the above topics can be obtained by searching the World Wide Web using keywords such as natural language processing, natural language understanding, machine learning, computational linguistics, etc. Many universities in the US doing research on NLP have information and links, for example, the University of Texas does at http://www.cs.utexas.edu\users\ml.
