What does natural language processing include?

Broadly construed, natural language processing (with respect to the interpretation side) is considered to involve at least the following subtopics5:

Signal processing
Syntactic analysis
Semantic analysis
Pragmatics
For the most part there is agreement on what fits into each of these categories. I use the term "pragmatics" here broadly; some authors use it more narrowly and add categories that I consider to be within pragmatics broadly.

Signal processing takes spoken words as input and turns it into text. Syntactic analysis gets at the structure or grammar of the sentences. Semantic analysis deals with the meaning of words and sentences, the ways that words and sentences refer to elements in the world. "Meaning" in these discussions is usually associated with semantics, but in other contexts I've seen syntax associated with "syntactic meaning." Pragmatics concerns how the meaning of a sentence depends on its function in everyday life, that is, the larger context of the conversation and so forth, and so it too seems concerned with meaning. So it might be best not to think of "meaning" issues as restricted to just semantics.

In this paper Iï¿½ll mostly ignore the topic of signal processing. Computers most often take text input directly, whether at the keyboard or read from a file or other source, rather than interpret spoken language. There are some sophisticated systems, and even some less costly ones anybody can buy, that process spoken words more or less successfully to translate them into text form.

I will discuss some of the basics of the other topics. James Allen has the second edition of what is considered the standard work here, Natural Language Understanding, and I draw from that source frequently.

Before proceeding I should note, however, two other processes that have to occur for natural language processing: tokenization and machine level processing. Some authors mention tokenization as another component of natural language processing, and it seems to me that you could consider it as part of signal processing, between signal processing and syntactic analysis, or perhaps less easily even as part of syntactic analysis.7 Tokenization is the conversion of an input signal into parts (tokens) so that the computer can process it. For example, even when a machine receives text from the keyboard, the text enters the machine as stream of ASCII characters, which must be classified into individual units, such as words or letter/number characters.6 The computer doesn't know how to consider the ASCII characters types into it unless it is told to expect characters, integers, floating point numbers, a string, etc. These are data types. When you type a letter on the keyboard, for example, the effect is to transmit an ASCII character code for the particular letter typed. "A," for example, is 41 (this would be transmitted in binary). But the computer must take this ASCII character and join it to others to interpret them together as a word, or an integer, or some other type of unit; in short, a token. Insofar as some authors talk of signal processing as the conversion of speech into text, and the text is recognized as words, some tokenization might already be involved. However, one might instead think of the signal processing as purely a conversion of audio to some kind of textual stream, with no processing of the stream as words or individual characters per se, i.e., no data typing on the part of the computer. In this case, tokenization would seem to be an additional component of natural language processing and not just part of signal processing. In this way some authors write of tokenization as the first step of parsing and preceding syntactic analysis. In this sense tokenization is needed not just for natural language processing but for any language processing on the part of the computer. Or if one has a really broad notion of syntactic analysis, one might even see this as including the tokenization, since recognizing words as opposed to numbers already involves the grammar of a language.

Another topic that is usually left out of the picture in discussing natural language processing in computers is machine level processing: the fact that, as the signal is processed, and syntactic, semantic, and pragmatics analysis are being accomplished, there is the continual translation into machine language, or even further, voltage signals. This will occur for the natural language sentence that is being analyzed, but also for all of the processing commands that the computer is following as it analyzes. How this occurs will involve the particular programming language in which the natural language processing program is written. But this code generation occurs as the program is compiled/interpreted and run.

In discussions of natural language processing by computers, it is just presupposed that machine level processing is going on as the language processing occurs, and it is not considered as a topic in natural language processing per se. But I think that if the attempt is being made to understand how natural language processing occurs in humans, with the goal of using this understanding to build a digital computer system that accomplishes satisfactory natural language processing, then the topic of how the actual low-level processing actually occurs in humans and digital computers may not be irrelevant. If humans process natural languages by using parallel, distributed processing at the basic level, and this is not just an implementation of sentential AI at a higher level but rather involves a different kind of representation or no representation at all, then it may or may not be possible to build an adequate natural language processor that uses solely the kind of sequential processing of current digital computers. It seems to me that it could turn out that how the computer actually works at the lowest level may be a relevant issue for natural language processing after all. As it stands, the usual kind of discussion that occurs about natural language processing in computers seems pretty much geared to a sentential AI interpretation. The usual goal is to process the natural language sentences into some sort of knowledge representation that is most easily interpreted as corresponding to an internal meaning representation or proposition in humans. The machines and programs used for the natural language processing simulations or programs are usually geared to sequential processing on traditional digital computers, so it is understandable why this should be so.

So that leaves syntactic analysis, semantic analysis, and pragmatics as the heart of most discussions of natural language processing. We tend to assume that a computer could be given the dictionary meanings of words, along with the type of speech for each and the rules of grammar, and by using this information process a natural language sentence. It should be a purely mechanical process of searching a database and classifying the sentence parts. But it is not at all this simple, mainly due to the problem of ambiguity at many levels. To interpret natural language sentences correctly, the system will not only have to parse the sentences grammatically but also associate the words with things in the world and with the general context of utterance of the sentence. While "parsing" is usually associated with syntactic analysis, parsing sentences correctly may even require clearing up any semantic and pragmatic issues that stand in the way of a correct syntactic analysis (for example, you may need to know context in order to decide whether "impact" is a noun or a verb; in recent years it has become increasingly used as a verb). And then once one has the syntactic analysis, semantics and pragmatics are needed to understand the full meaning of the sentence.

Thus there occurs an appreciation for the fact that understanding a natural language is much more complicated than just sorting out the words into parts of speech and looking up their definitions. Allen mentions the varied types of knowledge relevant to natural language understanding8:
