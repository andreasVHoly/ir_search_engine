In machine learning, computer algorithms (learners) attempt to automatically distil knowledge from example data. This knowledge can be used to make predictions about novel data in the future and to provide insight into the nature of the target concept(s). The example data typically consists of a number of input patterns or examples of the concepts to be learned. Each example is described by a vector of measurements or features along with a label which denotes the category or class the example belongs to. Machine learning systems typically attempt to discover regularities and relationships between features and classes in a learning or training phase. A second phase called classification uses the model induced during learning to place new examples into appropriate classes. Many factors affect the success of machine learning on a given task. The representation and quality of the example data is first and foremost. If there is much irrelevant and redundant information present or the data is noisy and unreliable, then knowledge discovery during the training phase is more difficult. Feature subset selection is the process of identifying and removing as much of the irrelevant and redundant information as possible. This reduces the dimensionality of the data and allows learning algorithms to operate faster and more effectively. In some cases, accuracy on future classification can be improved; in others, the result is a more compact, easily interpreted representation of the target concept. This paper presents a new approach to feature selection for machine learning that uses a correlation based heuristic to evaluate the merit of features. The effectiveness of the feature selector is evaluated by applying it to data as a pre-processing step for three common machine learning algorithms.
