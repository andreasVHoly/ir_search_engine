Machine Learning and Its Applications to Biology
Adi L Tarca,
Vincent J Carey,
Xue-wen Chen,
Roberto Romero,
Sorin Drăghici 
The term machine learning refers to a set of topics dealing with the creation and evaluation of algorithms that facilitate pattern recognition, classification, and prediction, based on models derived from existing data. Two facets of mechanization should be acknowledged when considering machine learning in broad terms. Firstly, it is intended that the classification and prediction tasks can be accomplished by a suitably programmed computing machine. That is, the product of machine learning is a classifier that can be feasibly used on available hardware. Secondly, it is intended that the creation of the classifier should itself be highly mechanized, and should not involve too much human input. This second facet is inevitably vague, but the basic objective is that the use of automatic algorithm construction methods can minimize the possibility that human biases could affect the selection and performance of the algorithm. Both the creation of the algorithm and its operation to classify objects or predict events are to be based on concrete, observable data.
The history of relations between biology and the field of machine learning is long and complex. An early technique [1] for machine learning called the perceptron constituted an attempt to model actual neuronal behavior, and the field of artificial neural network (ANN) design emerged from this attempt. Early work on the analysis of translation initiation sequences [2] employed the perceptron to define criteria for start sites in Escherichia coli. Further artificial neural network architectures such as the adaptive resonance theory (ART) [3] and neocognitron [4] were inspired from the organization of the visual nervous system. In the intervening years, the flexibility of machine learning techniques has grown along with mathematical frameworks for measuring their reliability, and it is natural to hope that machine learning methods will improve the efficiency of discovery and understanding in the mounting volume and complexity of biological data.
This tutorial is structured in four main components. Firstly, a brief section reviews definitions and mathematical prerequisites. Secondly, the field of supervised learning is described. Thirdly, methods of unsupervised learning are reviewed. Finally, a section reviews methods and examples as implemented in the open source data analysis and visualization language R (http://www.r-project.org).
Main Concepts and Definitions
Two main paradigms exist in the field of machine learning: supervised and unsupervisedlearning. Both have potential applications in biology.
In supervised learning, objects in a given collection are classified using a set of attributes, or features. The result of the classification process is a set of rules that prescribe assignments of objects to classes based solely on values of features. In a biological context, examples ofobject-to-class mappings are tissue gene expression profiles to disease group, and protein sequences to their secondary structures. The features in these examples are the expression levels of individual genes measured in the tissue samples and the presence/absence of a given amino acid symbol at a given position in the protein sequence, respectively. The goal in supervised learning is to design a system able to accurately predict the class membership of new objects based on the available features. Besides predicting a categorical characteristic such as class label, (similar to classical discriminant analysis), supervised techniques can be applied as well to predict a continuous characteristic of the objects (similar to regression analysis). In any application of supervised learning, it would be useful for the classification algorithm to return a value of “doubt” (indicating that it is not clear which one of several possible classes the object should be assigned to) or “outlier” (indicating that the object is so unlike any previously observed object that the suitability of any decision on class membership is questionable).
In contrast to the supervised framework, in unsupervised learning, no predefined class labels are available for the objects under study. In this case, the goal is to explore the data and discover similarities between objects. Similarities are used to define groups of objects, referred to as clusters. In other words, unsupervised learning is intended to unveil natural groupings in the data. Thus, the two paradigms may informally be contrasted as follows: in supervised learning, the data come with class labels, and we learn how to associate labeled data with classes; in unsupervised learning, all the data are unlabeled, and the learning procedure consists of both defining the labels and associating objects with them.
In some applications, such as protein structure classification, only a few labeled samples (protein sequences with known structure class) are available, while many other samples (sequences) with unknown class are available as well. In such cases, semi-supervisedtechniques can be applied to obtain a better classifier than could be obtained if only the labeled samples were used [5]. This is possible, for instance, by making the “cluster assumption,” i.e., that class labels can be reliably transferred from labeled to unlabeled objects that are “nearby” in feature space.
Life science applications of unsupervised and/or supervised machine learning techniques abound in the literature. For instance, gene expression data was successfully used to classify patients in different clinical groups and to identify new disease groups [6–9], while genetic code allowed prediction of the protein secondary structure [10]. Continuous variable prediction with machine learning algorithms was used to estimate bias in cDNA microarray data [11].
To support precise characterization of both supervised and unsupervised machine learning methods, we have adopted certain mathematical notations and concepts. In the next sections, we employ vector notation (x denotes an ordered p-tuple of numbers for some integer p), matrix notation (X denotes a rectangular array of numbers, where xij will denote the number in the ith row and jth column of X), conditional probability densities, and sufficient matrix algebra to define the multivariate normal density. Necessary formal background in algebra and probability can be found elsewhere [12].
Supervised Learning
General concepts.
Let us consider the general case in which we want to classify a collection of objects i = 1, . . ., ninto K predefined classes. For instance, if one wants to distinguish between different types of tumors based on gene expression values, then K would represent the number of known existing tumor types. Without loss of generality, data on features can be organized in an n × pmatrix X = (xij), where xij represents the measured value of the variable (feature) j in the object (sample) i. Every row of the matrix X is therefore a vector xi with p features to which a class label yi is associated, y = 1,2,. . .,c,. . .,K. In such multiclass classification problems, a classifierC(x) may be viewed as a collection of K discriminant functions gc(x) such that the object with feature vector x will be assigned to the class c for which gc(x) is maximized over the class labels c ∈ {1,. . .,K}. The feature space X is thus partitioned by the classifier C(x) into K disjoint subsets.
There are two main approaches to the identification of the discriminant functions gc(x) [13]. The first assumes knowledge of the underlying class-conditional probability density functions (the probability density function of x for a given class) and assigns gc(x) = f(p(x | y = c)), where f is a monotonic increasing function, for example the logarithmic function. Intuitively, the resulting classifier will classify an object x in the class in which it has the highest membership probability. In practice, p(x | y = c) is unknown, and therefore needs to be estimated from a set of correctly classified samples named training or design set. Parametric and nonparametric methods for density estimation can be used for this end. From the parametric category, we will discuss linear and quadratic discriminants, while from the nonparametric one, we will describe the k-nearest neighbor (k-NN) decision rule. The second approach is to use data to estimate the class boundaries directly, without explicit calculation of the probability density functions. Examples of algorithms in this category include decision trees, neural networks, and support vector machines (SVM).
Error estimation.
Suppose the classifier C(x) was trained to classify input vectors x into two distinct classes, 1 and 2. The classification result on a collection of input objects xi, i = 1,. . .,n can be summarized in a confusion matrix. The confusion matrix contrasts the predicted class labels of the objects with the true (given) class labels yi. An example confusion matrix computed for 100 objects is:


Conclusion
Modern biology can benefit from the advancements made in the area of machine learning. Caution should be taken when judging the superiority of some machine learning approaches over other categories of methods. It is argued [39] that the success or failure of machine learning approaches on a given problem is sometimes a matter of the quality indices used to evaluate the results, and these may vary strongly with the expertise of the user. Of special concern with supervised applications is that all steps involved in the classifier design (selection of input variables, model training, etc.) should be cross-validated to obtain an unbiased estimate for classifier accuracy. For instance, selecting the features using all available data and subsequently cross-validating the classifier training will produce an optimistically biased error estimate. Because of inadequate validation schemes, many studies published in the literature as successful have been shown to be overoptimistic [40]. It should be clear from the narrative examples used in this tutorial that choice, tuning, and diagnosis of machine learning applications are far from mechanical. 


