Survey and critique of techniques
for extracting rules from trained
artificial neural networks

Robert Andrews, Joachim Diederich and Alan B Tickle

 
The rapid and successful proliferation of applications
incorporating artificial neural network (ANN) technology
in fields as diverse as commerce, science, industry
and medicine offers a clear testament to the capability
of the ANN paradigm. Of the three salient characteristics
of ANNs which underpin this success, the first is the
comparatively direct and straightforward manner in
which artificial neural networks acquire information/
knowledge about a given problem domain through a
training phase. This process is quite distinct from the
more complicated knowledge engineering/acquisition
process for symbolic AI systems. The second characteristic
is the compact (albeit completely numerical) form in which the acquired information/knowledge is stored
within the trained ANN, and the comparative ease and
speed with which this 'knowledge' can be accessed and
used. The third characteristic is the robustness of an
ANN solution in the presence of noise in the input data.
In addition to these characteristics, one of the most
important advantages of trained artificial neural
networks is the high degree of accuracy reported when
an ANN solution is used to generalise over a set of
previously unseen examples from the problem domain
[l].
However, the success of the ANN paradigm is
achieved at a cost. This is an inherent inability to
explain in a comprehensible form the process through
which a given decision or output generated by an ANN
has been reached. For artificial neural networks to gain
an even wider degree of user acceptance and to enhance
their overall utility as learning and generalisation tools,
it is highly desirable if not essential that an explanation
capability should become an integral part of the functionality
of a trained ANN. Such a requirement is
mandatory if, for example, the ANN is to be used in
what are termed safety critical applications such as
airlines and power stations. In these cases, it is imperative
that a system user should be able to validate the
output of the artificial neural network under all possible
input conditions. Further, the system user should be
provided with the capability to determine the set of
conditions under which an output unit within an ANN
is active and when it is not, thereby providing some
degree of transparency of the ANN solution.
Apart from the direct contribution to enhancing the
overall utility of artificial neural networks, the addition
of an explanation capability is also seen as having the
potential to contribute to the understanding of how
symbolic and connectionist approaches to artificial
intelligence (AI) can be profitably integrated. It also provides a vehicle for traversing the boundary between
the connectionist and symbolic approaches.
This paper is a survey and critique of the more significant
techniques developed to date to extract rules from
trained artificial neural networks and hence provide the
requisite explanation capability.
For the purposes of this discussion, the focus will be
restricted to rule extraction from general feed-forward
multilayered artificial neural network architectures utilising
a supervised learning regime such as back propagation.
Hence techniques for rule extraction from
specialised artificial neural network learning architectures
such as KBANN, developed by Towell and Shavlik
[2] and Cascade, developed by Fahlman and Lebiere [3],
are included in the review, whereas techniques for rule
extraction from architectures based on unsupervised or
reinforcement learning are excluded.
At this stage, it is also worth commenting on two
related areas in the development of rule extraction techniques
which do not form part of this initial survey,
although both are important in their own right. The
first is the extraction of symbolic grammatical rules
from trained artificial neural networks, and, in particular,
recurrent neural networks [4,5,6]. In this area of
research the focus is on having an ANN simulate a
deterministic finite state automation (FSA) which
recognises regular grammars. The second is the work of
Gallant [7] on extracting explanations of individual
problem instances from trained artificial neural
networks, as distinct from complete sets of rules. 
