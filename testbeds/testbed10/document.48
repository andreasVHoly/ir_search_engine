We have performed a thorough evaluation of simple methods for the construction of neural network ensembles. In particular, we considered algorithms that can be implemented with an independent (parallel) training of the ensemble members, and introduced a framework that suggests naturally the SimAnn algorithm as the optimal one. Taking as the ensemble prediction the simple average of the ANN outputs, we have shown that SECA and SimAnn are the best performers in the large majority of cases. These include syn-thetic data with different noise levels and training set sizes, and also real-world databases. We have also shown that these methods resolve very differently the compromise between accuracy and diversity through their particular search strategies. The greedy method that we termed SECA seeks at every stage for a new member that is at least partially anticorrelated with the previous-stage ensemble estimator. This is achieved by applying a late-stopping method in the learning phase of individual networks, leading to a controlled level of overtraining of the ensemble members. In principle this algorithm retains the simplicity of independent network training, although, if necessary, it can avoid the computational burden of saving intermediate networks in this phase since it can be implemented in a sequential way. In this implementation the method is a stepwise construction of the ensemble, where each network is selected at a time and only its parameters have to be saved. We showed, by comparison with several other algorithms in the literature, that this strategy is effective, as exemplified by the results in Tables 1–4. The SimAnn algorithm, first proposed in this work, uses simulated annealing to minimize the error on unseen data with respect to the number of training epochs for each individual ensemble member. This method is also very effective, being competitive with SECA on most databases. Furthermore, the implementation of the minimization step at the end of the ANNs training process is, in practice, not very time consuming from a computational point of view, being only a fraction of the time required to train the networks. We also discussed a known problem with stepwise selection procedures like SECA, and proposed a modification of this algorithm to overcome it. The modified algorithm, which we called W-SECA, weights the predictions of ensemble members depending on their individual performances. We showed that it improves the results obtained with SECA in practically all cases. Moreover, since weighting is in general beneficial for all the methods considered, we investigated whether this procedure overrides the differences between ensemble construction algorithms. We found that the weighted versions of SECA and SimAnn (W-SECA and W-SimAnn) are again the best performers, indicating the intrinsic efficiency of these construction methods. Finally, we have also performed a comparison of W-SECA and W-SimAnn with several other regression methods, including methods based on SVMs and regularized boosting. For this we used published results in the literature corresponding to the Mackey-Glass equation. Again in this case we found that the algorithms here proposed are among the top performers in almost all situations considered (Tables 7a, 7b and 9). Given this competitive behavior of weighted bagging-like algorithms, one is tempted to speculate that, for regression, the success of boosting ideas might not be mainly related to the modification of resampling probabilities but to the final error weighting of ensemble members. We want to comment on the performance improvement obtained with the aggregation algorithms discussed in this work. We found that in general SECA and SimAnn, either in their weighted or non-weighted versions, produce better results than other algorithms in the literature (Bagging, NeuralBAG, Epoch). Although this holds true in several cases with more than 95% of statistical significance, the performance improvement obtained depends largely on the problem considered. For instance, with respect to Bagging, the most common algorithm, one finds the following (compare Tables 3 and 7): For the Friedman databases the improvement can be very low with high noise (1% or less), to very large (up to 200%) in some noise-free cases. For databases with fixed noise level (real-world data and Ikeda map), the improvement ranges from less than 1% (Abalone) to nearly 12% (Ikeda). The answer to the question as to whether these performances justify the use of the algorithms here proposed instead of Bagging would depend, then, on the concrete application, particularly on how critical it is. However, even for non-critical ones there is always a chance that using W-SECA or W-SimAnn one might obtain fairly large improvements. In any case, the best justification is perhaps the fact that not much additional computational time is required to implement these algorithms. Before closing, we want to comment on a recent work[24] partially related to the present one. In Ref. [24], the authors use genetic algorithms (GA) to select a suitable subset of all the trained nets to build the ensemble. For this, they train a number of ANNs to the optimal validation point like in Bagging, and then assign to these networks an importance weight through a GA strategy. Finally, only those networks that have weights larger than a given threshold are kept in the ensemble. In the algorithm—that they termed GASEN— the predictions of the retained ANNs are combined by simple average, which leads to good generalization capabilities when compared to Bagging and Boosting. This strategy can be readily implemented within our SimAnn algorithm by simply allowing an appropriate random change in the number of aggregated ANNs, in addition to the stochastic search of optimal training epochs. Notice that this procedure would extend the GASEN optimization to ANNs trained an arbitrary number of epochs (instead of searching only among those at the optimal validation point), which might be important since some degree of single-network overtraining is known to improve the ensemble performance. This combined approach, which would presumably bring the best of SimAnn and GASEN into a single algorithm, is, however, beyond the scope of this work. In addition to the above proposal, as future work we are also considering extending the methods here proposed to classification problems and comparing their performances with those of boosting strategies.
