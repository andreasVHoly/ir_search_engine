The process of converting data sets with continuous attributes into input
data sets with discrete attributes, called discretization, was studied in many
papers; see, e.g., [1-3, 5, 7, 10, 11, 13, 15]. We will assume that input data
sets contain examples, characterized by attribute and decision values. A concept is defined as a set of all examples that have the same value w for decision d. A data set may be either consistent or inconsistent. We need a measure of consistency for inconsistent data sets. Our measure, called a level of consistency, is based on rough set theory, a tool to deal with uncertainty, introduced by Z. Pawlak in [12]. Let U denote the set of all examples of the data set. Let P denote a nonempty subset of the set of all variables, i.e., attributes and a decision. Obviously, set p defines an equivalence relation ~o on U, where two examples e and e' from U belong to the same equivalence class of ~ if and only if both e and e' are characterized by the same values of each variable from P. The set of all equivalence classes of ~0, i.e., a partition on U, will be denoted P*. Equivalence classes of ~o are called elementary sets of P. Any finite union of elementary sets of P is called a definable set in P. Let X be any subset of U. In general, X is not a definable set in P. However, the set X may be approximated by two definable sets in P. The first one is called a lower approximation of X in P, denoted by _PX and defined as follows: 
