 The instance-based learning paradigm This section outlines the learning task; presents a framework for instance-based learning algorithms; defines the problems of noise, uncertain relevance, and novelty in this context; and characterizes why these problems impact on the performance of primitive instance-based learning algorithms. Subsequent sections describe methods for solving these problems and evidence that these methods work well. 2.1. LEARNING TASK AND PERFORMANCE DIMENSIONS The task studied in this article is supervised learning or learning from examples. More specifically, we focus on the incremental, empirical variant of this task in which the only input is a training sequence of instances provided by the environment. Each instance is represented by a set of attribute-value pairs. Attributes can have nominal, Boolean or numeric values.? The set of attributes that are used to describe the instances define an instance space, which determines the set of instances that may be input to the learning algorithm. IBL algorithms have two outputs. First, they output a set of concept descriptions (or concepts), one for each concept represented by at least one instance in the training sequence. A concept’s description maps each instance to a graded estimate of membership for that concept. IBL algorithms also output a cfussificution for each instance, which is a graded estimate of the membership of an instance in a class. The following five dimensions will be used to judge the performance of our supervised learning algorithms: 1. Generality: the class of concepts that are describable by the representation and learnable by the algorithm. We showed that IBL algorithms can PAC-learn (Valiant, 1984) any concept describable as a union of bounded regions in the instance space (Aha, Kibler & Albert, 1991). 2. Accuracy: the concept descriptions’ classification accuracy when mapping instance space to concept space. 2.2. METHODOLOGY AND FRAMEWORK FOR IBL ALGORITHMS Instance-based learning algorithms classify instances solely with respect to a set of previously presented instances. They assume that similar instances will have similar classifications. This leads to their local bias for classifying novel instances according to their most similar neighbor’s classification. IBL algorithms also assume that, without prior knowledge, attributes will have equal relevances for classification decisions. This bias is enforced by normalizing each attribute to have the same range of possible values. An instance-based concept description contains a set of stored instances. Most IBL algorithms also include performance feedback information in their concept descriptions, such as a record of the classification performance for each saved instance (i.e. their number of correct and incorrect classification attempts) and an estimate of the relevance of each attribute for assisting in classification decisions. IBL algorithms differ from most other supervised learning methods in that they do not store explicit abstractions (in their concept descriptions) such as decision trees or rules. Most learning algorithms derive generalizations from instances when they are presented and use simple matching procedures to classify subsequently presented instances. In effect, they incorporate the purpose of the generalizations at presentation time. IBL algorithms perform comparatively little work at presentation time. However, their workload is higher when presented with subsequent instances for classification, at which time they compute the similarities of their saved instances with the newly presented instance. This trade-off of workload at presentation and classification time eliminates the need for IBL algorithms to store rigid generalizations in concept descriptions, which can require large updating costs in response to correcting erroneous classification behavior. All IBL algorithms are specified by a framework of three components: 1. Similarity Function: this computes the numeric similarity between a newly presented instance i and an instance in a concept description. 2. Classification Function : this inputs the computed similarities of the newly presented instance and all of the instances in a target concept’s description. It yields a classification (i.e. an estimate of membership) for i in that target concept. Performance feedback information, if available, is used to assist in predicting the classification. 3. Concept Description Updater: this maintains records on classification performance and decides which instances to include in each concept description. Inputs include i, the similarity results, the classification results, and a current concept description. It yields the modified concept description.  The simplest IBL algorithm is named IBl (Table 1). Its similarity function is Similarity@, y) = - ~ygc-z where x and y are instances in an n-dimensional instance space. IBl is identical to the nearest neighbor algorithm except for two characteristics. First, it linearly normalizes all instances’ attribute va1ues.t This ensures that each attribute has the same (maximal) affect on the similarity function. Second, it uses a simple method for tolerating missing attribute values; if either of an attribute a’s value is missing during a similarity computation, it is assumed to be maximally different from the other value. IBl predicts that an instance’s class is the same as that of its most similar neighbor in the concept description. Finally, IBl saves all processed training instances in its concept description. IBl recorded high classification accuracies across a variety of applications, some of which are summarized in Table 3. Table 2 briefly characterizes the domains and the experiments. The results for C4, Quinlan’s (1987) extension of ID3 that prunes, are also presented in Table 3 for comparison purposes. The first five results falsely suggest that IBl is a relatively robust classifier (in comparison with C4). We see a clearer picture of IBl’s capabilities in the last two results, in which its accuracy is relatively poor. This occurred because instances in these data sets are described by several irrelevant attributes. IB2 (Table 1) is identical to IBl except that its concept description updater saves only misclassified instances. Table 3 shows that IB2 significantly reduced IBl’s storage requirements but sacrificed accuracy slightly. IB2’s poorer classification accuracy is due to its sensitivity to noise in the training set’s instances. Like IBl, it is also sensitive to irrelevant attributes. Fortunately, we will show that both of these problems can be remedied by suitable extensions of IB2. There can be several definitions of noise, uncertainty and novelty in the supervised learning task. We define these terms from our incremental learning perspective. We define noise as incorrect attribute value information. Noise in training instances presents problems for the instance-based approach because IB2 saves misclassified instances. IB2 will save noisy instances because they will regularly be misclassified, especially if the noisy instances are distant from concept boundaries. The saved noisy instances will then be used by IB2 to misclassify subsequent, similar instances. Noise results in higher storage requirements and lower classification accuracy. Section 3 presents IB3, an extension of IB2 that applies a filter to its saved instances, thus disallowing noisy instances from influencing classification decisions. Uncertain relevance can refer to several properties of the training data. We define this problem as that of learning the relative relevances of the attributes used to describe training instances. Attribute relevances vary among concepts; an attribute may be highly relevant to one concept and completely irrelevant to another. IB3 and its predecessors assume that all attributes are equally relevant for describing concepts. Therefore, IB3’s performance (e.g. learning rate, storage requirements) degrades quickly with increasing numbers of irrelevant attributes. Section 4 describes IB4, an extension of IB3 that learns a separate set of attribute weights for each concept. These weights are then used in IB4’s similarity function-a Euclidean weighted-distance measure of the similarity of two instances. Multiple sets of weights are used because similarity is concept-dependent; the similarity of two instances varies depending on the target concept. For example, the similarity of two expert bridge players, one male and one female, is generally higher for the concept “ability to play bridge” than it is for the concept “ability to give birth”. IB4 decreases the affect of irrelevant attributes on classification decisions. It subsequently is more tolerant of irrelevant attributes. We define the problem of novelty as the problem of learning when novel attributes are used to help describe instances. IB4, like its predecessors, assumes that all the attributes used to describe training instances are known before training begins. However, in several learning tasks, the set of describing attributes is not known beforehand. For example, while a person is usually given the rules when learning to play a new game, they must also learn useful strategies based on their game experience to be sucessful players. This depends on their ability to learn and use new attributes to describe their game situations. Section 5 introduces IB5, an extension of IB4 that tolerates the introduction of novel attributes during training. To simulate this capability during training, IB4 simply assumes that the values for the (as yet) unused attribute are missing. During this time, IB4 fixates the expected relevance of the attribute for classifying instances. IB5 instead updates an attribute’s weight only when its value is known for both of the instances involved in a classification attempt (i.e. the instance being classified and the instance chosen to classify it). IB5 can therefore learn the relevance of novel attributes more quickly than IB4. We show that this results in improved learning rate. 
