In this paper we will give an overview of a large family of symbolic rule learning algorithms, the so-called separate-and-conquer or covering algorithms. All members of this family share the same top-level loop: basically, a separate-and-conquer algorithm searches for a rule that explains a part of its training instances, separates these examples, and recursively conquers the remaining examples by learning more rules until no examples remain. This ensures that each instance of the original training set is covered by at least one rule. It is well-known that learning algorithms need an appropriate bias for making an “inductive leap”. Mitchell (1980) defined bias as any basis for choosing one generalization over another, other than strict consistency with the observed training instances. A learning algorithm can thus be characterized with the biases it employs. While the basic top-level loop is invariant for all algorithms of the separateand-conquer family, their method for learning single rules can vary considerably for different members of this family. We will characterize separate-andconquer algorithms along three dimensions: Language Bias: The search space for a learning algorithm is defined by its hypothesis language. Certain concepts may not be expressible or may have an awkward representation in certain hypothesis languages. An appropriate choice of the hypothesis language thus constitutes an important form of bias. Search Bias: The term search bias refers to the way the hypothesis space is searched. It includes the search algorithm (hill-climbing, beam search, etc.), its search strategy (top-down or bottom-up), and the search heuristics that are used to evaluate the found hypotheses. Overfitting Avoidance Bias: Many algorithms employ heuristics for avoiding overfitting of noisy data. They may prefer simpler rules to more complex rules, even when the accuracy of the simpler rules on the training data is lower, in the hope that their accuracy on unseen data will be higher. Such a bias for simpler rules has recently been termed overfitting avoidance bias (Schaffer 1993; Wolpert, 1993).1 We will start with the description of a generic separate-and-conquer algorithm that can be instantiated to various existing (and new) learning algorithms by specifying different biases. 2. The Separate-and-Conquer Strategy The separate-and-conquer strategy has its origins in the AQ family of algorithms (Michalski 1969) under the name covering strategy. The term separateand-conquer has been coined by Pagallo and Haussler (1990) because of the way of developing a theory that characterizes this learning strategy: learn a rule that covers a part of the given training set (the separate part) and recursively learn another rule that covers some of the remaining examples (the conquer part) until no examples remain. The terminological choice is a matter of personal taste, both terms can be found in the literature. We will use the term separate-and-conquer learning. Separate-and-conquer algorithms have been developed for a variety of different learning tasks. Figure 1 shows a collection of well-known algorithms grouped by the types of concepts they learn. The classical separate-andconquer algorithms induce rule setsfor attribute-value based concept learning problems. Variants generalize this approach to inducing ordered rule sets (also called decision lists) for multi-class problems. Problems with continuous class variables can be solved by learning regression rules. Research in the field of inductive logic programming (Bergadano and Gunetti 1995; Muggleton 1992; De Raedt 1995) has developed a variety of separate-and-conquer algorithms that can solve the above tasks in a richer representation language by inducing SEPARATE-AND-C logic programs for classification or for predicting output values in functional relations. In this paper we will mainly concentrate on concept learning tasks, as they seem to be the most common application of separate-and-conquer algorithms in the literature. We will start with a brief formalization of this learning problem (section 2.1), proceed to formalize a generic separate-and-conquer algorithm that can address this problem (section 2.2) and briefly discuss important variants that handle related learning problems (section 2.3).
