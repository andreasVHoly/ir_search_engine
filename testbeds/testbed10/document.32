The topic of learning in multi-agent systems, or multi-agent learning (MAL henceforth), has a long history in game theory, almost as long as the history of game theory itself.2 As early as 1951, fictitious play [10] was proposed as a learning algorithm for computing equilibria in games and there have been proposals for how to evaluate the success of learning rules going back to [23] and [5]. Since that time hundreds, if not thousands, of articles have been published on the topic, and at least two books ([20] and [54]). In Artificial Intelligence (AI) the history of single-agent learning is as rich if not richer, with thousands of articles, many books, and some very compelling applications in a variety of fields (for some examples see [29,40], or [50]). While it is only in recent years that AI has branched into the multi-agent aspects of learning, it has done so with today this is no longer possible. The leading conferences routinely feature articles on MAL, as do the journals.3 While the AI literature maintains a certain flavor that distinguishes it from the game theoretic literature, the commonalities are greater than the differences. Indeed, alongside the area of mechanism design, and perhaps the computational questions surrounding solution concepts such as the Nash equilibrium, MAL is today arguably one of the most fertile interaction grounds between computer science and game theory. The MAL research in both fields has produced some inspiring results. We will not repeat them here, since we cannot be comprehensive in this article, but nothing we say subsequently should be interpreted as belittling the achievements in the area. Yet alongside these successes there are some indications that it could be useful to take a step back and ask a few basic questions about the area of MAL. One surface indication is the presence of quite a number of frustrating dead ends. For example, the AI literature attempting to extend Bellman-style single-agent reinforcement learning techniques (in particular, Q-learning [53]) to the multi-agent setting, has fared well in zero-sum repeated games (e.g., [36] and [38]) as well as common-payoff (or ‘team’) repeated games (e.g., [14,31,52]), but less well in general-sum stochastic games (e.g., [21,26,37]) (for the reader unfamiliar with this line of work, we cover it briefly in Section 4). Indeed, upon close examination, it becomes clear that the very foundations of MAL could benefit from explicit discussion. What exact question or questions is MAL addressing? What are the yardsticks by which to measure answers to these questions? The present article focuses on these foundational questions. To start with the punch line, following an extensive look at the literature we have reached two conclusions: • There are several different agendas being pursued in the MAL literature. They are often left implicit and conflated; the result is that it is hard to evaluate and compare results. • We ourselves can identify and make sense of five distinct research agendas. Not all work in the field falls into one of the five agendas we identify. This is not necessarily a critique of work that doesn’t; it simply means that one must identify yet other well-motivated and well defined problems addressed by that work. We expect that as a result of our throwing down the gauntlet additional such problems will be defined, but also that some past work will be re-evaluated and reconstructed. Certainly we hope that future work will always be conducted and evaluated against well-defined criteria, guided by this article and the discussion engendered by it among our colleagues in AI and game theory. In general we view this article not as a final statement but as the start of a discussion. In order to get to the punch line outlined above, we proceed as follows. In the next section we define the formal setting on which we focus. In Section 3 we illustrate why the question of learning in multi-agent settings is inherently more complex than in the single-agent setting, and why it places a stress on basic game theoretic notions. In Section 4 we provide some concrete examples of MAL approaches from both game theory and AI. This is anything but a comprehensive coverage of the area, and the selection is not a value judgment. Our intention is to anchor the discussion in something concrete for the benefit of the reader who is not familiar with the area, and—within the formal confines we discuss in Section 2—the examples span the space of MAL reasonably well. In Section 5 we identify five different agendas that we see (usually) implicit in the literature, and which we argue should be made explicit and teased apart. We end in Section 6 with a summary of the main points made in this article. A final remark is in order. The reader may find some of the material in the next three sections basic or obvious; different readers will probably find different parts so. We don’t mean to insult anyone’s intelligence, but we err on the side of explicitness for two reasons. First, this article is addressed to at least two different communities with somewhat different backgrounds. Second, our goal is to contribute to the clarification of foundational issues; we don’t want to be guilty of vagueness ourselves We will couch our discussion in the formal setting of stochastic games (aka Markov games). Most of the MAL literature adopts this setting, and indeed most of it focuses on the even more narrow class of repeated games. Furthermore, stochastic games also generalize Markov Decision Problems (MDPs), the setting from which much of the relevant learning literature in AI originates. These are defined as follows. A stochastic game can be represented as a tuple: (N , S,A, R,T ) . N is a set of agents indexed 1,...,n. S is a set of n-agent stage games. A = A1,...,An, with Ai the set of actions (or pure strategies) of agent i (note that we assume the agent has the same strategy space in all games; this is a notational convenience, but not a substantive restriction). R = R1,...,Rn, with Ri : S × A → R giving the immediate reward function of agent i for stage game S. T : S × A → Π (S) is a stochastic transition function, specifying the probability of the next stage game to be played based on the game just played and the actions taken in it. We also need to define a way for each agent to aggregate the set of immediate rewards received in each state. For finitely repeated games we can simply use the sum or average, while for infinite games the most common approaches are to use either the limit average or the sum of discounted awards ∞ t=1 δt rt , where rt is the reward received at time t. A repeated game is a stochastic game with only one stage game, while an MDP is a stochastic game with only one agent. While most of the MAL literature lives happily in this setting, we would be remiss not to acknowledge the literature that does not. Certainly one could discuss learning in the context of extensive-form games of incomplete and/or imperfect information (cf. [28]). We don’t dwell on those since it would distract from the main discussion, and since the lessons we draw from our setting will apply there as well. Although we will not specifically include them, we also intend our comments to apply at a general level to large population games and evolutionary models, and particularly replicator dynamics (RD) [47] and evolutionary stable strategies (ESS) [49]. These are defined as follows. The replicator dynamic model assumes a population of homogeneous agents each of which continuously plays a two-player game against every other agent. Formally the setting can be expressed as a tuple (A, P0,R). A is the set of possible pure strategies/actions for the agents indexed 1,...,m. P0 is the initial distribution of agents across possible strategies, m i=1 P0(i) = 1. R :A×A → R is the immediate reward function for each agent with R(a, a ) giving the reward for an agent playing strategy a against another agent playing strategy a . The population then changes proportions according to how the reward for each strategy compares to the average reward: dt (Pt(a)) = Pt(a)[ut(a)−u∗ t ], where ut(a) = a Pt(a )R(a, a ) and u∗ t = a Pt(a)ut(a). A strategy a is then defined to be an evolutionary stable strategy if and only if for some  > 0 and for all other strategies a , R(a, (1 − )a + a ) > R(a , (1 − )a + a ). As the names suggest, one way to interpret these settings is as building on population genetics, that is, as representing a large population undergoing frequent pairwise interactions. An alternative interpretation however is as a repeated game between two agents, with the distribution of strategies in the population representing the agent’s mixed strategy (in the homogeneous definition above the two agents have the same mixed strategy, but there exist more general defi- nitions with more than two agents and with non-identical strategies). The second interpretation reduces the setting to the one we discuss. The first bears more discussion, and we do it briefly in Section 4. And so we stay with the framework of stochastic games. What is there to learn in these games? Here we need to be explicit about some aspects of stochastic games that were glossed over so far. Do the agents know the stochastic game, including the stage games and the transition probabilities? If not, do they at least know the specific game being played at each stage, or only the actions available to them? What do they see after each stage game has been played—only their own rewards, or also the actions played by the other agent(s)? Do they perhaps magically see the other agent(s)’ mixed strategy in the stage game? And so on. In general, games may be known or not, play may be observable or not, and so on. We will focus on known, fully observable games, where the other agent’s strategy (or agents’ strategies) is not known a priori (though in some case there is a prior distribution over it). In our restricted setting there two possible things to learn. First, the agent can learn the opponent’s (or opponents’) strategy (or strategies), so that the agent can then devise a best (or at least a good) response. Alternatively, the agent can learn a strategy of his own that does well against the opponents, without explicitly learning the opponent’s strategy. The first is sometimes called model-based learning, and the second modelfree learning. 368 Y. Shoham et al. 
