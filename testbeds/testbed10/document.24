Top-down induction of decision trees (TDIDT) [28] is the best known and most successful machine learning technique. It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rulebased competitors (e.g., AQ [21], CN2 [6]), which are based on covering strategies (cf. [4]). Within attribute-value learning (or propositional concept-learning) TDIDT is more popular than the covering approach. Yet, within first-order approaches to concept-learning, only a few learning systems have made use of decision tree techniques. The main reason why divide-and-conquer approaches are not yet so popular within first-order learning, lies in the discrepancies between the clausal representation employed within inductive logic programming and the structure underlying a decision tree. The main contributions of this paper are threefold. First, we introduce a logical representation for relational decision trees and show how to correctly translate firstorder logical decision trees into logic programs. The resulting programs contain invented predicates and contain both universal and existential quantifiers through the use of negation. We show that this makes logical decision trees more expressive than the programs typically induced by ILP systems, and equally expressive as first-order decision lists. Second, in our framework each single example is a Prolog program, which means that we are learning from interpretations (cf. [7,9]). This learning setting has the classical TDIDT setting (using attribute value representations) as a special case. This makes it possible to elegantly upgrade attribute-value learners to the first-order logic context. As an illustration, we implemented TILDE, an upgrade of Quinlan’s C4.5 [27]. Thirdly, we report on a number of experiments on large data sets with TILDE which clearly show that TILDE is competitive both in terms of efficiency and accuracy with stateof-the-art inductive logic programming systems such as PROGOL [23] and FOIL [29]. This text is organized as follows. In Section 2, we briefly discuss the ILP setting that will be used. In Section 3, we introduce first-order logical decision trees and discuss their properties. We show in Section 4 how TDIDT can be upgraded to first-order learning, and empirically evaluate our algorithm in Section 5. Finally, in Section 6 we conclude and touch upon related work. We assume familiarity with Prolog (see, e.g., [5]) and standard logic programming terminology (e.g., [ 1 Xl). In our framework, each example is a Prolog program that encodes its specific properties. Furthermore, each example is classified into one of a finite set of possible classes. One may also specify background knowledge in the form of a Prolog program. More formally, the problem specification is: Given: a set of classes C, a set of classified examples E and a background theory B, Find: a hypothesis H (a Prolog program), such that for all e E E, H A e A B /= c, and H A e A B w c’, where c is the class of the example e and c’ E C - (c). This setting is known under the label learning frum interpretations 2 [7,9]. Notice that within this setting, one always learns first-order definitions of propositional predicates (the classes), An implicit assumption is that the class of an example depends on that example only, not on any other examples. This is a reasonable assumption for many classification problems, though not for all; it precludes e.g. recursive concept definitions. Example 1. An engineer has to check a set of machines. A machine consists of several parts that may be in need of replacement. Some of these can be replaced by the engineer, others only by the manufacturer of the machine. If a machine contains worn parts that cannot be replaced by the engineer, it has to be sent back to the manufacturer (class sendback). If all the worn parts can be replaced, it is to be fixed (fix). If there are no worn parts, nothing needs to be done (ok). Given the following set of examples (each example corresponds to one machine) and background knowledge: 
