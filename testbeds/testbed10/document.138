Effective Data Mining Using Neural Networks 



Hongjun Lu, Member, IEEE Computer Society,
Rudy Setiono, and Huan Liu, Member, IEEE 


INTRODUCTION
ONE of the data mining problems is classification. Various classification
algorithms have been designed to tackle the problem by
researchers in different fields such as mathematical programming,
machine learning, and statistics. Recently, there is a surge of
data mining research in the database community. The classification
problem is re-examined in the context of large databases. Unlike
researchers in other fields, database researchers pay more
attention to the issues related to the volume of data. They are also
concerned with the effective use of the available database techniques,
such as efficient data retrieval mechanisms. With such
concerns, most algorithms proposed are basically based on decision
trees. The general impression is that the neural networks are
not well suited for data mining. The major criticisms include the
following:
1) Neural networks learn the classification rules by many
passes over the training data set so that the learning time of
a neural network is usually long.
2) A neural network is usually a layered graph with the output
of one node feeding into one or many other nodes in the
next layer. The classification process is buried in both the
structure of the graph and the weights assigned to the links
between the nodes. Articulating the classification rules becomes
a difficult problem.
3) For the same reason, available domain knowledge is rather
difficult to be incorporated to a neural network.
On the other hand, the use of neural networks in classification
is not uncommon in machine learning community [5]. In some
cases, neural networks give a lower classification error rate than
the decision trees but require longer learning time [71, [81 In this
paper, we present our results from applying neural networks to mine classification rules for large databases [4] with the focus on
articulating the classification rules represented by neural networks.
The contributions of our study include the following:
Different from previous research work that excludes the
neural network based approaches entirely, we argue that
those approaches should have their position in data mining
because of its merits such as low classification error rates
and robustness to noise.
With our rule extraction algorithms, symbolic classification
rules can be extracted from a neural network. The rules
usually have a comparable classification error rate to those
generated by the decision tree based methods. For a data set
with a strong relationship among attributes, the rules extracted
are generally more concise.
A data mining system based on neural networks is developed.
The system successfully solves a number of classification
problems in the literature.
Our neural network based data mining approach consists of
three major phases:
Network construction and training. This phase constructs and
trains a three layer neural network based on the number of
attributes and number of classes and chosen input coding
method.
Network pruning. The pruning phase aims at removing redundant
links and units without increasing the classification
error rate of the network. A small number of units and links
left in the network after pruning enable us to extract concise
and comprehensible rules.
Rule extraction. This phase extracts the classification rules
from the pruned network. The rules generated are in the
form of "if (a, Bv,) and (x, Bv,) and ... and (x, Bv,) then Cy
where a,s are the attributes of an input tuple, v,~ are constants,
& are relational operators (=, <, 2, <>), and Ci is one
of the class labels.
Due to space limitation, in this paper we omit the discussion of
the first two phases. Details of these phases can be found in our
earlier work 191, [lo]. We shall elaborate in this paper the third
phase. Section 2 describes our algorithms to extract classification
rules from a neural network and uses an example to illustrate how
the rules are generated using the proposed approach. Section 3
presents some experimental results obtained. Finally, Section 4
concludes the paper. 
