Evolutionary extreme learning machine

Qin-Yu Zhu, A.K. Qin, P.N. Suganthan E-mail the corresponding author, Guang-Bin Huang, , E-mail the corresponding author

1. Introduction
Conventional gradient-based learning algorithms, such as back-propagation (BP) and it variant Levenberg–Marquardt (LM) method, have been extensively used in the training of multilayer feedforward neural networks. Although reasonable performance can be obtained when the networks are trained by BP, these gradient-based learning algorithms are still relatively slow in learning. These learning algorithms may also easily get stuck in a local minimum. Moreover, the activation functions used in these gradient-based tuning methods need to be differentiable.

A novel learning algorithm for single-hidden-layer feedforward neural networks (SLFNs) called extreme learning machine (ELM) [1] and [2] was proposed recently. In ELM, the input weights (linking the input layer to the hidden layer) and hidden biases are randomly chosen, and the output weights (linking the hidden layer to the output layer) are analytically determined by using Moore–Penrose (MP) generalized inverse. ELM not only learns much faster with higher generalization performance than the traditional gradient-based learning algorithms but also avoids many difficulties faced by gradient-based learning methods such as stopping criteria, learning rate, learning epochs, and local minima. However, it is also found that ELM tends to require more hidden neurons than conventional tuning-based algorithms in many cases.

Since evolutionary algorithms (EAs) are widely used as a global searching method for optimization, the hybrids of EA and analytical methods should be promising for network training. In the method proposed by Ghosh and Verma [3] namely GALS, the input weights are tuned by EA and the output weights are iteratively tuned/updated using the QR factorization method. In this paper, instead of iterative optimization of the output weights, a novel hybrid approach taking advantages of both ELM and the differential evolution (DE) [4] is proposed. DE is known for its ability and efficiency to locate global optimum over other EAs (cf. [4]). In the proposed algorithm, a modified DE is used to search for the optimal input weights and hidden biases, while the MP generalized inverse is used to analytically calculate the output weights.

2. Extreme learning machine
Extreme learning machine (ELM) was proposed in Huang, et al. [1]. Suppose we are training SLFNs with K   hidden neurons and activation function g(x) to learn N   distinct samples (xi,ti), where xi=[xi1,xi2,…,xin]T∈Rn and ti=[ti1,ti2,…,tim]T∈Rm. In ELM, the input weights and hidden biases are randomly generated instead of tuned. By doing so, the nonlinear system has been converted to a linear system:

equation(1)
Hβ=T,
Turn MathJax on

where H={hij} (i=1,…,N and j=1,…,K) is the hidden-layer output matrix, hij=g(wj·xi+bj) denotes the output of j  th hidden neuron with respect to xi; wj=[wj1,wj2,…,wjn]T is the weight vector connecting j  th hidden neuron and input neurons, and bj denotes the bias of j  th hidden neuron; wj·xi denotes the inner product of wj and xi; β=[β1,β2,…,βK]T is the matrix of output weights and βj=[βj1,βj2,…,βjm]T (j=1,…,K) denotes the weight vector connecting the j  th hidden neuron and output neurons; T=[t1,t2,…,tN]T is the matrix of targets (desired output).



where H† is the MP generalized inverse of matrix H. The minimum norm LS solution is unique and has the smallest norm among all the LS solutions. As analyzed by Huang, et al. [1], ELM using such MP inverse method tends to obtain good generalization performance with dramatically increased learning speed.

6. Conclusions
In this paper, we proposed a novel learning algorithm named evolutionary extreme learning machine (E-ELM) which makes use of the advantages of both ELM and DE. It uses the fast minimum norm least-square scheme to analytically determine the output weights instead of tuning, and a modified form of DE is used to optimize the input weights and hidden biases. Unlike the gradient-based methods, the proposed E-ELM does not require the activation functions to be differentiable, implying that E-ELM can be used to train SLFNs with many nonlinear hidden units such as threshold units which are claimed to be easier for hardware implementation. Experimental results show that E-ELM generally achieves higher generalization performance than other algorithms including BP, GALS and the original ELM. The results also indicate that GALS is relatively slow and need more memory due to the large storage and computational complexity involved in computing the QR factorization. While doing simulations of GALS, the networks are easily overtrained because GALS only uses the training RMSE as the fitness, whereas E-ELM considers both validation RMSE and norm of weights during searching to achieve a better generalization performance. Hence, GALS is sensitive to the population size and the number of learning epochs. Compared with conventional gradient-based BP algorithms and GALS, E-ELM has faster learning speed and higher testing accuracy and compared with the ELM algorithm, E-ELM can obtain much more compact network architecture which would increase the response speed and be helpful in fast response (to unknown testing data) applications.
