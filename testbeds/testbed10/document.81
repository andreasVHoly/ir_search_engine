Challenges in real-life emotion annotation and machine learning based detection Laurence

 Devillers*, Laurence Vidrascu, Lori Lamel

The work presented in this paper addresses both experimental and theoretical issues in the study of emotion in natural un-acted spoken data. The brain is the seat of many emotions at the same time, even if at any given moment there is one dominant emotion. We explore issues such as how to annotate real-life non-basic spoken emotions, how to define a typology of blended emotions which tries to take into account the known effects of masking by self-control, and how to detect emotions with machine learning techniques. In this paper, the widely used terms of emotion or emotional state are used without distinction from the more generic term affective state which may be viewed as more adequate from the psychological theory point of view. A large amount of work has been reported in the neurobiology literature on how the human brain recognizes emotional states (e.g. Damasio, 1994; Ledoux, 1989). There are also numerous psychological studies and theories on perception and production models of emotion, among others, the appraisal theory (Scherer, 1999). During the last decade, new results have appeared identifying a close relationship of emotion to knowledge, to brain activity, and to consciousness (e.g. Damasio, 1994; Taylor, 1997). New imaging techniques of the brain are also promising for enabling a new understanding of the neuronal substrate serving complex natural emotional processes. The Handbook of Affective Sciences (Davidson, Scherer, & Goldsmith, 2003) reports on the recent considerable advances in understanding how brain processes shape emotions and how the brain is changed by human emotion using a wide range of inquiry methods, such as neuroimaging techniques or laboratory paradigms designed to assess the cognitive and social constituents of emotion. Cognitive scientists study the nature of affect and emotion from a psychological point of view, mostly building computer models that help to understand memorization, perception and other psychological processes. Within the broad area of cognitive science, the dominant paradigm is the information processing approach. This paradigm has been successfully applied to cognitive processes at different levels such as basic vision, spoken language or higher level thoughts. Most of the previous works on emotion have been conducted on induced or recalled data with archetypal emotions. Everyday emotions in real-life context are still rarely studied. Recently, databases involving people in various natural emotional states in response to real situations have been acquired, which will hopefully allow the development of more sophisticated systems for recognizing the relevant emotional states in such data. Our approach, grounded in findings from cognitive science and neuroscience is to develop tools for representing and modeling real-life emotions in natural contexts. Automatic detection systems based on different types of machine learning architectures, such as localized or distributed connectionist systems, may enable a deeper understanding of the perception of emotion by identifying relevant cues for emotion detection in natural emotional states. The pluridisciplinarity of the emotion research field is certainly the most powerful means to improve knowledge on the nature of emotions. Emotional behavior has been, since the early days, an important field of research for psychology, philosophy and neuroscience, but more recently it is a growing field of research for computer science. This can be clearly seen by the recent activities in academic laboratories (e.g. Affective Computing Research at MIT (Picard, 1997)), industrial research laboratories (e.g. AT&T, SpeechWorks-Scansoft), and the special interest shown by funding agencies via the promotion of the FP6 trans-European Network of Excellence HUMAINE ‘Human–machine interaction network on emotions’(http:// www.emotion-research.fr) and the European projects PFSTAR ‘Preparing future multisensorial interaction research’ (http://pfstar.itc.it/), AMI ‘Augmented Multi-party Interaction’ (http://amiprotocol.sourceforge.net/) and CHIL ‘Computers in the Human Interaction Loop’ (http://chil. server.de/serlevts/is/101/) all of which include activities on emotion research, as well as the growing number of workshops and special sessions on emotion (on affective dialog, Embodied Conversational Agents (ECAs), etc.) and articles appearing in the scientific and popular press. There has been increasing interest in emotion analysis to improve the capabilities of current speech technologies such as speech synthesis, recognition, and spoken dialog systems. In the context of human–machine interaction, the study of emotion has generally been aimed at the generation of Embodied Conversational Agents and at the automatic extraction of emotional behavior related features for dialog systems. Detecting emotion or underlying attitude can help orient the evolution of a human–computer interaction via dynamic modification of the dialog strategy. Emotion is conveyed by several multimodal cues: speech, gesture, face and physiological signals. In this paper, we focus on the speech signal. Emotional speech is more likely to occur in unstructured human-to-human interactions than in restricted contexts. Three types of corpora are generally used for emotion analysis and detection: acted, induced, and natural real-life corpora. People, often even actors, are bad at faking emotions on demand. However acted corpora are the easiest to obtain and to exploit, with only variations at the prosodic level since the linguistic (semantic and lexical) content has been controlled. Induced corpora often are obtained using Wizard of Oz (WOz) techniques and provide much more natural data. In Batliner, Fisher, Huber, Spilker, and Noth (2000), for instance, the authors make use of a WOz corpus to develop a detection model incorporating several layers of information, i.e. prosodic, dialog acts, repetitions, etc. Similar work using both human–human (DHH) and human–computer (DHM) dialogs and focusing on emotion detection was carried out in (Ang, Dhillon, Krupski, Shriberg, & Stolcke, 2002; Lee, Narayanan, & Pieraccini, 2002). However, as shown in Batliner et al. (2000), the closer we get to the real-life context of interaction, the more difficult the detection of reliable emotion markers is. It is quite difficult to obtain natural audio–video material because the camera is rarely invisible. Many researchers have made use of media material such as TV programs as natural corpora, focusing on interviews or reports that exhibit the most spontaneous situations. Others have used meetings or lectures, such as in the CHIL and AMI projects, as audio–visual material for studying emotional behaviors (Burger, Maclaren, & Yue, 2002; Wreded & Shriberg, 2003). Batliner et al. (2004) recorded corpora of children playing with an AIBO robot to incite for emotional reactions. When the focus is on real-life speech signals, call centers can provide interesting solutions for recording people in various natural emotional states since the recordings can be made imperceptivity. Among the natural corpora for emotion detection, we can mention the ‘Lifelog’ corpus consisting of everyday interactions between a female speaker and her family and friends described in (Campbell, 2004); the ‘Interviews corpus’ also known as the Belfast database (Douglas-Cowie, Campbell, Cowie, & Roach, 2003); the ‘EmoTV’ corpus—a set of TV interviews in French recorded in the HUMAINE Noe (Abrilian, Devillers, Buisine, & Martin, 2005); call center data (Lee et al., 2002); and medical dialogs (Craggs & Wood, 2004). Evidently, the types of emotions found in the corpus are heavily dependent on the task and situation/context. The research presented in this paper focuses on the detection of emotional behavior in real-life speech corpora recorded in call centers. Two corpora of real agent client dialogs recorded in French, one recorded at a Stock Exchange Customer service center and the other at a Medical Emergency call center, are studied. In both corpora, callers manifest real-life emotions whereas the agent plays a predetermined, moderating role. The first corpus was recorded for other purposes within the framework of the IST FP-5 Amities Automated Multilingual Interaction with Information and Services project at a French Stock Exchange Customer Service Centre (http://www.dcs.shef. ac.uk/nlp/amities/). The second corpus is studied in the context of collaboration with a French Medical Emergency call center. Emotion manifestations in the second corpus are much more frequent and intense than in the first one. LIMSI also participates in HUMAINE. One of the main challenges we address is the categorization and annotation of real-life emotions, requiring the definition of a pertinent and limited set of labels and an appropriated annotation scheme. Some of the problems we face have to do with the dynamic and constantly changing expression of emotions. Emotion manifestations are context-dependent, and are also highly person-dependent. Unambiguous emotions are apparent in only a small portion of any real corpus, therefore the relevant emotion data are too infrequent to provide a basis for consistent annotation and modeling using fine-grained emotion labels. A major difficulty of using natural corpora is that the expression of emotion is much more complex than in acted speech for the above listed reasons. Furthermore, interlabeler agreement and annotation label confidences are important issues to address. There are many reviews on the representation of emotions. For a recent review, the reader is referred to Cowie and Cornelius (2003). Here are just briefly described the three types of theories generally used to represent the emotions: appraisal dimensions, abstract dimensions and most commonly verbal categories.
