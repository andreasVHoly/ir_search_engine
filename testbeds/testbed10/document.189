Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups


Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury

Most current speech recognition systems use
hidden Markov models (HMMs) to deal with
the temporal variability of speech and
Gaussian mixture models (GMMs) to determine
how well each state of each HMM fits a
frame or a short window of frames of coefficients that represents
the acoustic input. An alternative way to evaluate the fit
is to use a feed-forward neural network that takes several
frames of coefficients as input and produces posterior probabilities
over HMM states as output. Deep neural networks
(DNNs) that have many hidden layers and are trained using
new methods have been shown to outperform GMMs on a variety
of speech recognition benchmarks, sometimes by a large
margin. This article provides an overview of this progress and
represents the shared views of four research groups that have
had recent successes in using DNNs for acoustic modeling in
speech recognition.

INTRODUCTION
New machine learning algorithms can lead to significant
advances in automatic speech recognition (ASR). The biggest  single advance occurred nearly
four decades ago with the introduction
of the expectation-maximization
(EM) algorithm for
training HMMs (see [1] and [2]
for informative historical reviews
of the introduction of HMMs).
With the EM algorithm, it be -
came possible to develop speech
recognition systems for realworld
tasks using the richness of GMMs [3] to represent the
relationship between HMM states and the acoustic input. In
these systems the acoustic input is typically represented by concatenating
Mel-frequency cepstral coefficients (MFCCs) or perceptual
linear predictive coefficients (PLPs) [4] computed from
the raw waveform and their first- and second-order temporal
differences [5]. This nonadaptive but highly engineered preprocessing
of the waveform is designed to discard the large amount
of information in waveforms that is considered to be irrelevant
for discrimination and to express the remaining information in
a form that facilitates discrimination with GMM-HMMs.
GMMs have a number of advantages that make them suitable
for modeling the probability distributions over vectors of
input features that are associated with each state of an HMM.
With enough components, they can model probability distributions
to any required level of accuracy, and they are fairly
easy to fit to data using the EM algorithm. A huge amount of
research has gone into finding ways of constraining GMMs to
increase their evaluation speed and to optimize the tradeoff
between their flexibility and the amount of training data
required to avoid serious overfitting [6].
The recognition accuracy of a GMM-HMM system can be
further improved if it is discriminatively fine-tuned after it has
been generatively trained to maximize its probability of generating
the observed data, especially if the discriminative objective
function used for training is closely related to the error
rate on phones, words, or sentences [7]. The accuracy can also
be improved by augmenting (or concatenating) the input features
(e.g., MFCCs) with “tandem” or bottleneck features generated
using neural networks [8], [69]. GMMs are so successful
that it is difficult for any new method to outperform them for
acoustic modeling. Despite all their advantages, GMMs have a serious shortcoming—they
are statistically inefficient for modeling data
that lie on or near a nonlinear manifold in the data space. For
example, modeling the set of points that lie very close to the
surface of a sphere only requires a few parameters using an
appropriate model class, but it requires a very large number of
diagonal Gaussians or a fairly large number of full-covariance
Gaussians. Speech is produced by modulating a relatively
small number of parameters of a dynamical system [10], [11],
and this implies that its true underlying structure is much
lower-dimensional than is immediately apparent in a window
that contains hundreds of coefficients. We believe, therefore,
that other types of model may work better than GMMs for acoustic modeling if they can
more effectively exploit information
embedded in a large window
of frames.
