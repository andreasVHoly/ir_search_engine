The Receiver Operating Characteristic (ROC) curve has long been used, in conjunction with the Neyman-Pearson method, in signal detection theory. (1'2) As such, it is a good way of visualising a classifier's performance in order to select a suitable operating point, or decision threshold. However, when comparing a number of different classification schemes it is often desirable to obtain a single figure as a measure of the classifier's performance. Often this figure is a cross-validated estimate of the classifier's overall accuracy [probability of a correct response, P(C)]. In this paper we discuss the use of the area under the ROC curve (AUC) as a measure of a classifier's performance. This paper addresses the generic problem of how to accurately evaluate the performance of a system that learns by being shown labelled examples. As a case study, we look at the performance of six different classification schemes on six "real word" medical data sets. These data sets are chosen to characterize those typically found in medical diagnostics, they have primarily continuous input attributes and have overlapping output classes. When comparing the performance of the classification schemes, Analysis of Variance (ANOVA) is used to test the statistical significance of any differences in the accuracy and AUC measures. Duncan's multiple range (3) test is then used to obtain the significant subgroups for both these performance measures. Results are presented in the form of ROC curves and ranked estimates of each classification scheme's overall accuracy and AUC. Discussion is then focused both on the performance of the different classification schemes and on the methodology used to compare them. The paper is structured in the following way: Section 2 details some commonly used performance measures and describes the use of the ROC curve and, in particular, AUC as a performance measure; Section 3 briefly describes the six data sets to be used in the experimental study; Section 4 details the implementations of the six learning algorithms used and describes how they are modified so that the decision threshold can be varied and a ROC curve produced; Section 5 describes the experimental methodology used, outlines three types of experimental bias, and describes how this bias can be avoided; Section 6 gives specific details of the performance measures and Section 7 the statistical techniques used to compare these measures. Section 8 presents a summary of the results, which are then discussed in detail in the remaining sections of the paper. 
