Game theory (von Neumann & Morgenstern, function reinforcement learning to Markov games to 1947) provides a powerful set of conceptual tools for create agents that learn from experience how to best reasoning about behavior in multiagent environ- interact with other agents. This paper presents severments. Markov games (van der Wal, 1981), or al value-function reinforcement-learning algorithms stochastic games (Owen, 1982; Shapley, 1953), are a and what is known about how they behave when formalization of temporally extended agent inter- learning simultaneously in different types of games. action. Section 2 describes single-agent environments and Reinforcement learning (Kaelbling, Littman & the basic Q-learning algorithm, which converges to Moore, 1996; Sutton & Barto, 1998) is the problem an optimal value function and optimal behavior in of an agent learning to behave from experience. One this type of environment. Section 3 examines multiwell studied approach to reinforcement learning is are coordination equilibria. Section 6 presents some discounted rewards is also bounded. From an appliconcluding thoughts. cations perspective, the discount factor can be thought of as the probability that the agent will be allowed to continue gathering reward after the 2. Single-agent environments current step, or, from an economic perspective, as an inverse interest rate on reward (Puterman, 1994). Markov decision processes (MDPs) (Bellman, A policy is a description of the behavior of an 1957; Howard, 1960) are a descriptive model of agent. A stationary policy, p:6 → P(! ), specifies, single-agent environments. In the MDP framework, for each state, a probability distribution over actions it is assumed that, although there may be a great deal to be taken. Notationally, p(s, a) is the probability of uncertainty about the effects of an agent’s actions, assigned to action a in state s. A deterministic policy there is never any uncertainty about the agent’s is one that assigns probability 1 to some action in current state — it has complete and perfect perceptu- each state. Every MDP has a deterministic stationary al abilities. optimal policy (Bertsekas, 1987). A policy p for an agent can be evaluated by 2.1. Markov decision processes computing the long-run value the agent can expect to p gain. Let Q (s, a) be the expected discounted future Mathematically, a Markov decision process is a reward to the agent for starting in state s and tuple k6, !, T, R, bl, where executing action a for one step, then continuing according to policy p. This can be defined by a set • 6 is a finite set of states of the environment; of simultaneous linear equations, one for each state • ! is a finite set of actions available to the agent; s: • T: 6 3 ! → P(6 ) is the transition function, p Q (s, a) 5 R(s, a) 1 b O T(s, a, s9) giving for each state and agent action, a probabili- s9[6 ty distribution over states (T(s, a, s9) is the p 3 O p(s9, a9)Q (s9, a9). probability of ending in state s9, given that the a9[! agent starts in state s and takes action a); p • R The function Q is called the Q-function for p. : 6 3 ! → R is the reward function, giving the expected immediate reward gained by the agent Given an initial state s, the agent should execute a p for taking each action in each state (R(s, a) is the policy p that maximizes oa p(s, a)Q (s, a). Howard expected reward for taking action a (1960) showed that there exists a stationary de- in state s); and terministic policy p* that is optimal for every • 0 # b , 1 is a discount factor. starting state. The Q-function for this policy, written Q*, is defined by the set of equations This paper considers only models with finite state Q*(s, a) 5 R(s, a) 1 b O T(s, a, s9)max Q*(s9, a9), and action spaces. s9[6 a9[! In an MDP, agents should act in such a way as to (1) maximize some measure of their long-run reward received. Under the discounted objective, which is and the greedy policy that assigns probability one to the focus of this paper, the discount factor 0 # b , 1 action argmax Q*(s, a) in state s is optimal a controls how much effect future rewards have on the (Puterman, 1994). decisions at each moment, with small values of b The presence of the maximization operator in Eq. emphasizing near-term gain and larger values giving (1) means the system of equations is not linear. significant weight to later situations. Concretely, a Methods such as value iteration, policy iteration, t reward of r received t steps in the future is worth b r linear programming, and modified policy iteration to the agent now. Mathematically, the discount factor can be used to solve the equations (Puterman, 1994). has the desirable property that if all immediate Barto, Sutton and Watkins (1989) argue that rewards are bounded, then the infinite sum of the MDPs are an appropriate model for studying re-
