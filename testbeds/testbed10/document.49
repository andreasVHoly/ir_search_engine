Artificial neural networks (ANNs) are hugely popular in research on medical decision support systems (see Baxt's review of clinical applications of ANNs [2]). This is despite signi®cant practical problems with their application. A practical problem that has come to prominence recently is that ANNs are unstable predictors. That is to say that small changes in the training data set may produce very different models [4,5,7] and consequently different performance on unseen data. Breiman suggests that these different models may result from the training of the ANN getting caught in different local minima in the error surface [5]. In this paper, we show that this instability means that estimations of the generalisation performance of an ANN for a particular task may vary considerably depending on the training data used. We argue that, because of this, estimations of the generalisation performance of a single ANN should be based on a k-fold cross-validation. Finally we describe how this instability problem can be ®xed by building an ensemble of ANNs and aggregating the results of the networks in the ensemble to produce reliable predictions. This paper begins with an overview of machine learning techniques in general and neural networks in particular in the next section. In Section 3 a detailed study of the generalisation performance of an ANN for predicting outcome in In-Vitro Fertilisation (IVF) treatment is presented. This study clearly shows how the instability problem with ANNs results in high-variance estimates of generalisation error. The paper concludes with a description of the ensemble solution and a demonstration of how it improves performance in narrow domain medical ®elds like IVF treatment outcome prediction.

2. Learning from examples The objective in using machine learning (ML) in medicine is to have the ML system learn to model a relationship that is represented explicitly in a set of historic data. The objective might be to learn the diagnosis associated with particular symptom combinations or likely treatment outcomes in particular situations. These are classi®cation problems where the learning system learns to classify new examples into outcome categories or diagnostic classes. Alternatively the objective may be a regression task where the outcome to be predicted is a numeric value rather than a category, e.g. predicating chemical concentrations or determining dosage levels. There are many factors that in¯uence the performance of such decision support systems applied to medicine. They have been proven to work best in `narrow domain' medical ®elds where the cognitive span is narrow, an abstraction is available and hence prediction is amenable to structured queries [3]. Also the degree of representation of the biological/physiological process of the disease or medical event by those variables used in the ML algorithm in¯uences the performance. In any ML system the competence of the systems will improve with the amount of training data available. This competence (or accuracy) will follow a learning curve like that shown in Fig. 1. Indeed the technical use of this term shares much in common with the vernacular use. Up to a certain point additional training data will produce appreciable increases in accuracy. However, beyond the knee point in the graph additional data produces little increase in accuracy. At the knee point the learning system has seen a useful cross-section of data samples that represent a good coverage of the problem domain. While this knee point will be clearly evident in a study such as that described in [20] it is very dif®cult to determine a priori the amount of training data that is required in a particular problem domain to give good coverage. The amount of data required for good coverage re¯ects the complexity of the input±output relationship being modelled (e.g. symptoms and diagnoses), the predictivness of the input features and the amount of noise in the data. All the nuances of straightforward problems may be represented in less than 100 examples but more complex problems may require several hundred or thousands of examples to provide good coverage. It is usually the case in studies of the use of ML in medicine that training data is scarce compared with the complexity of the problem being modelled. In that case the learning process is operating to the left of the knee point shown in Fig. 1. Researchers often report that they expect the accuracy of their systems to increase with more data. A particular problem in developing ML systems with relatively small amounts of training data is that the training process may over-®t the training data. An example of this involving a Neural Net is shown in Fig. 2 (a plot of error rather than accuracy). It can be seen that as the training of the network proceeds the error on the training data continues to drop but after 200 epochs (200 presentations of all the training data) the error on unseen test data begins to rise. After this point the network is over®tting to peculiarities in the training data and is loosing generalisation accuracy. The standard solution to this problem is to hold out some of the available data from training and stop training when error on this validation set starts to rise. Unhappily, over®tting is more of a problem when data is scarce and using precious data in a validation set can still be afforded. In situations where an abundance of training data is available, all the details of the problem will be well represented in the training data and over®tting is unlikely to be observed.
elds indicate negative examples. In turn, we will plot the results of all learning algorithms in the same way: # indicates that the learning algorithm classi
es the entity as a positive member, and a blank as a non-member. However, an additional square will indicate misclassi
cations, i.e. if the classi
cation obtained by the algorithm is wrong.

