InValiant’s probably approximately correct (PAC) learning model, a successful learning algorithm must be able to achieve any arbitrarily low error rate given random examples drawn from any fixed probability distribution. In an early paper, Kearns and Valiant [13] proposed the notion of a weak learning algorithm which need only achieve some error rate bounded away from 1 2 , and posed the question of whether weak and strong learning are equivalent for efficient (polynomial time) learning algorithms. Soon afterward, in a celebrated result Schapire gave a positive answer to this question [16]. Schapire gave an efficient boosting algorithm which, given access to any weak learning algorithm, uses the weak learner to generate a hypothesis with arbitrarily low error. Since Schapire’s initial result boosting has become one of the biggest successes of computational learning theory; boosting algorithms have been intensively studied from a theoretical perspective and are widely used in practice. The standard PAC learning model assumes that all examples received by the learner are labeled correctly, i.e. the data has no noise. An important question, which was asked by Schapire in his original paper [16] and by several subsequent researchers [2], is whether it is possible to efficiently perform boosting in the presence of noise. Since real data is frequently noisy, this question is of significant practical as well as theoretical interest. In this paper, we give a detailed study of boosting in the presence of random classification noise. In the random classification noise model, the binary label of each example which the learner receives is independently flipped from the true label f (x) with probability  for some fixed 0 <  < 1 2 ; the value  is referred to as the noise rate. Random classification noise is the most standard and widely studied noise model in learning theory. We give both positive and negative results for boosting in this model as described below.
