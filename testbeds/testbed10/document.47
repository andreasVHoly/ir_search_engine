Neural network ensemble is a learning paradigm where a collection of a finite number of neural networks is trained for the same task [42]. It originates from Hansen and Salamon’s work [20], which shows that the generalization ability of a neural network system can be significantly improved through ensembling a number of neural networks, i.e., training many neural networks and then combining their predictions. Since this technology behaves remarkably well, recently it has become a very hot topic in both neural networks and machine learning communities [40], and has already been successfully applied to diversified areas such as face recognition [16,22], optical character recognition [9,19,30], scientific image analysis [5], medical diagnosis [6,47], seismic signals classification [41], etc. In general, a neural network ensemble is constructed in two steps, i.e., training a number of component neural networks and then combining the component predictions. As for training component neural networks, the most prevailing approaches are Bagging and Boosting. Bagging is proposed by Breiman [3] based on bootstrap sampling [10]. It generates several training sets from the original training set and then trains a component neural network from each of those training sets. Boosting is proposed by Schapire [39] and improved by Freund et al. [11,12]. It generates a series of component neural networks whose training sets are determined by the performance of former ones. Training instances that are wrongly predicted by former networks will play more important roles in the training of later networks. There are also many other approaches for training the component neural networks. Examples are as follows. Hampshire and Waibel [17] utilize different object functions to train distinct component neural networks. Cherkauer [5] trains component networks with different number of hidden units. Maclin and Shavlik [29] initialize component networks at different points in the weight space. Krogh and Vedelsby [28] employ cross-validation to create component networks. Opitz and Shavlik [34] exploit genetic algorithm to train diverse knowledge based component networks. Yao and Liu [46] regard all the individuals in an evolved population of neural networks as component networks. As for combining the predictions of component neural networks, the most prevailing approaches are plurality voting or majority voting [20] for classification tasks, and simple averaging [33] or weighted averaging [35] for regression tasks. There are also many other approaches for combining predictions. Examples are as follows. Wolpert [45] utilizes learning systems to combine component predictions. Merz and Pazzani [31] employs principal component regression to determine the appropriate constraint for the weights of the component networks in combining their predictions. Jimenez [24] uses dynamic weights determined by the confidence of the component networks to combine the predictions. Ueda [43] exploits optimal linear weights to combine component predictions based on statistical pattern recognition theory. Note that there are some approaches using a number of neural networks to accomplish a task in the style of divide-and-conquer[23,25]. However, in those approaches, the neural networks are in fact trained for different subtasks instead of for the same task, which makes those approaches usually be categorized into mixture of experts instead of ensembles, and the discussion of them is beyond the scope of this paper.

It is worth mentioning that when a number of neural networks are available, at present most ensemble approaches employ all of those networks to constitute an ensemble. Yet the goodness of such a process has not been formally proved. In this paper, from the viewpoint of prediction, i.e., regression and classification, the relationship between the ensemble and its component neural networks is analyzed, which reveals that ensembling many of the available neural networks may be better than ensembling all of those networks. Then, in order to show that those “many” neural networks can be effectively selected from a number of available neural networks, an approach named GASEN (Genetic Algorithm based Selective ENsemble) is presented. This approach selects some neural networks to constitute an ensemble according to some evolved weights that could characterize the fitness of including the networks in the ensemble. An empirical study on twenty big data sets show that in most cases, the performance of the neural network ensembles generated by GASEN outperform those generated by some popular ensemble approaches such as Bagging and Boosting in that GASEN utilizes far less component neural networks but achieves stronger generalization ability. Moreover, this paper employs the biasvariance decomposition to analyze the empirical results, which shows that the success of GASEN may owe to its ability of significantly reducing the bias along with the variance. The rest of this paper is organized as follows. In Section 2, the relationship between the ensemble and its component neural networks is analyzed. In Section 3, GASEN is presented. In Section 4, a large empirical study is reported. In Section 5, the bias-variance decomposition of the error is provided. Finally in Section 6, contributions of this paper are summarized and several issues for future works are indicated.

At present, most neural network ensemble approaches utilize all the available neural networks to constitute an ensemble. However, the goodness of such a process has not yet been formally proved. In this paper, the relationship between the ensemble and its component neural networks is analyzed, which reveals that it may be a better choice to ensemble many instead of all the available neural networks. This theory may be useful in designing powerful ensemble approaches. Then, in order to show the feasibility of the theory, an ensemble approach named GASEN is presented. A large empirical study shows that GASEN is superior to both Bagging and Boosting in both regression and classification because it utilizes far less component neural networks but achieves stronger generalization ability. Note that although GASEN has obtained impressive performance in our empirical study, we believe that there are approaches that could do better than GASEN along the way that GASEN goes, i.e., ensembling many instead of all available neural networks under certain circumstances. The reason is that GASEN has not been finely tuned because its aim is only to show the feasibility of our theory. In other words, the aim of GASEN is just to show that the networks appropriate for constituting the ensemble could be effectively selected from a collection of available neural networks. So, its performance might at least be improved through utilizing better fitness functions, coding schemes, or genetic operators. In the future we hope to use some other large-scale data sets such as NIST to test GASEN and tune its performance, and then apply it to real-world applications. Moreover, it is worth mention that finding stronger ensemble approaches based on the recognition that many could be better than all is an interesting issue for future works. In order to explore the reason of the success of GASEN, the bias-variance decomposition is employed in this paper to analyze the empirical results. It seems that the success of GASEN mainly lies in that GASEN could reduce the bias as well as the variance. We guess that GASEN can reduce the bias because it efficiently utilizes the training data in that it employs a validation set bootstrap sampled from the training set, and it can reduce the variance because it combines multiple version of the same learning approach. Rigorous theoretical analysis may be necessary to justify those guesses, which is another interesting issue for future works.
