SVMlight is an implementation of Support Vector Machines (SVMs) in C. The main features of the program are the following: fast optimization algorithm working set selection based on steepest feasible descent "shrinking" heuristic caching of kernel evaluations use of folding in the linear case solves classification and regression problems. For multivariate and structured outputs use SVMstruct . solves ranking problems (e. g. learning retrieval functions in STRIVER search engine). computes XiAlpha-estimates of the error rate, the precision, and the recall efficiently computes Leave-One-Out estimates of the error rate, the precision, and the recall includes algorithm for approximately training large transductive SVMs (TSVMs) (see also Spectral Graph Transducer) can train SVMs with cost models and example dependent costs allows restarts from specified vector of dual variables handles many thousands of support vectors handles several hundred-thousands of training examples supports standard kernel functions and lets you define your own uses sparse vector representation SVMstruct: SVM learning for multivariate and structured outputs like trees, sequences, and sets (available here). SVMperf: New training algorithm for linear classification SVMs that can be much faster than SVMlight for large datasets. It also lets you direcly optimize multivariate performance measures like F1-Score, ROC-Area, and the Precision/Recall Break-Even Point. (available here). Description SVMlight is an implementation of Vapnik's Support Vector Machine [Vapnik, 1995] for the problem of pattern recognition, for the problem of regression, and for the problem of learning a ranking function. The optimization algorithms used in SVMlight are described in [Joachims, 2002a ]. [Joachims, 1999a]. The algorithm has scalable memory requirements and can handle problems with many thousands of support vectors efficiently. The software also provides methods for assessing the generalization performance efficiently. It includes two efficient estimation methods for both error rate and precision/recall. XiAlphaestimates [Joachims, 2002a, Joachims, 2000b] can be computed at essentially no computational expense, but they are conservatively biased. Almost unbiased estimates provides leave-one-out testing. SVMlight exploits that the results of most leave-one-outs (often more than 99%) are predetermined and need not be computed [Joachims, 2002a]. New in this version is an algorithm for learning ranking functions [Joachims, 2002c]. The goal is to learn a function from preference examples, so that it orders a new set of objects as accurately as possible. Such ranking problems naturally occur in applications like search engines and recommender systems. Futhermore, this version includes an algorithm for training large-scale transductive SVMs. The algorithm proceeds by solving a sequence of optimization problems lower-bounding the solution using a form of local search. A detailed description of the algorithm can be found in [Joachims, 1999c]. A similar transductive learner, which can be thought of as a transductive version of k-Nearest Neighbor is the Spectral Graph Transducer. SVMlight can also train SVMs with cost models (see [Morik et al., 1999]). The code has been used on a large range of problems, including text classification [Joachims, 1999c][Joachims, 1998a], image recognition tasks, bioinformatics and medical applications. Many tasks have the property of sparse instance vectors. This implementation makes use of this property which leads to a very compact and efficient representation. Source Code and Binaries The program is free for scientific use. Please contact me, if you are planning to use the software for commercial purposes. The software must not be further distributed without prior permission of the author. If you use SVMlight in your scientific work, please cite as T. Joachims, Making large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning, B. SchÃ¶lkopf and C. Burges and A. Smola (ed.), MIT-Press, 1999. [PDF][Postscript (gz)] I would also appreciate, if you sent me (a link to) your papers so that I can learn about your research. The implementation was developed on Solaris 2.5 with gcc, but compiles also on SunOS 3.1.4, Solaris 2.7, Linux, IRIX, Windows NT, and Powermac (after small modifications, see FAQ). The source code is available at the following location: http://download.joachims.org/svm_light/current/svm_light.tar.gz If you just want the binaries, you can download them for the following systems: Solaris: http://download.joachims.org/svm_light/current/svm_light_solaris.tar.gz Windows: http://download.joachims.org/svm_light/current/svm_light_windows.zip Cygwin: http://download.joachims.org/svm_light/current/svm_light_cygwin.tar.gz Linux: http://download.joachims.org/svm_light/current/svm_light_linux.tar.gz Please send me email and let me know that you got svm-light. I will put you on my mailing list to inform you about new versions and bug-fixes. SVMlight comes with a quadratic programming tool for solving small intermediate quadratic programming problems. It is based on the method of Hildreth and D'Espo and solves small quadratic programs very efficiently. Nevertheless, if for some reason you want to use another solver, the new version still comes with an interface to PR_LOQO. The PR_LOQO optimizer was written by A. Smola. It can be requested from http://www.kernel-machines.org/code/prloqo.tar.gz. Installation To install SVMlight you need to download svm_light.tar.gz. Create a new directory: mkdir svm_light Move svm_light.tar.gz to this directory and unpack it with gunzip -c svm_light.tar.gz | tar xvf - Now execute make or make all which compiles the system and creates the two executables svm_learn (learning module) svm_classify (classification module) If you do not want to use the built-in optimizer but PR_LOQO instead, create a subdirectory in the svm_light directory with mkdir pr_loqo and copy the files pr_loqo.c and pr_loqo.h in there. Now execute make svm_learn_loqo If the system does not compile properly, check this FAQ. How to use This section explains how to use the SVMlight software. A good introduction to the theory of SVMs is Chris Burges' tutorial. SVMlight consists of a learning module (svm_learn) and a classification module (svm_classify). The classification module can be used to apply the learned model to new examples. See also the examples below for how to use svm_learn and svm_classify. svm_learn is called with the following parameters: svm_learn [options] example_file model_file Available options are: General options: -? - this help -v [0..3] - verbosity level (default 1) Learning options: -z {c,r,p} - select between classification (c), regression (r), and preference ranking (p) (see [Joachims, 2002c]) (default classification) -c float - C: trade-off between training error and margin (default [avg. x*x]^-1) -w [0..] - epsilon width of tube for regression (default 0.1) -j float - Cost: cost-factor, by which training errors on positive examples outweight errors on negative examples (default 1) (see [Morik et al., 1999]) -b [0,1] - use biased hyperplane (i.e. x*w+b0) instead of unbiased hyperplane (i.e. x*w0) (default 1) -i [0,1] - remove inconsistent training examples and retrain (default 0) Performance estimation options: -x [0,1] - compute leave-one-out estimates (default 0) (see [5]) -o ]0..2] - value of rho for XiAlpha-estimator and for pruning leave-one-out computation (default 1.0) (see [Joachims, 2002a]) -k [0..100] - search depth for extended XiAlpha-estimator (default 0) Transduction options (see [Joachims, 1999c], [Joachims, 2002a]): -p [0..1] - fraction of unlabeled examples to be classified into the positive class (default is the ratio of positive and negative examples in the training data) Kernel options: -t int - type of kernel function: 0: linear (default) 1: polynomial (s a*b+c)^d 2: radial basis function exp(-gamma ||a-b||^2) 3: sigmoid tanh(s a*b + c) 4: user defined kernel from kernel.h -d int - parameter d in polynomial kernel -g float - parameter gamma in rbf kernel -s float - parameter s in sigmoid/poly kernel -r float - parameter c in sigmoid/poly kernel -u string - parameter of user defined kernel Optimization options (see [Joachims, 1999a], [Joachims, 2002a]): -q [2..] - maximum size of QP-subproblems (default 10) -n [2..q] - number of new variables entering the working set in each iteration (default n = q). Set n if option is given, reads alphas from file with given and uses them as starting point. (default 'disabled') -# int -> terminate optimization, if no progress after this number of iterations. (default 100000) Output options: -l char - file to write predicted labels of unlabeled examples into after transductive learning -a char - write all alphas to this file after learning (in the same order as in the training set) A more detailed description of the parameters and how they link to the respective algorithms is given in the appendix of [Joachims, 2002a]. The input file example_file contains the training examples. The first lines may contain comments and are ignored if they start with #. Each of the following lines represents one training example and is of the following format: 
