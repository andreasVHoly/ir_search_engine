Inductive methods can be applied in two ways: as interactive tools acquiring knowledge from examples or as a part of learning systems. When inductive methods are used to acquire knowledge, the user provides examples and strongly controls the use of the method. Used as part of learning systems, inductive methods are activated when another system component has the necessity to learn from positive and/or negative examples that constitute the feedback from which the system can achieve the current task. An example of a system having inductive learning integrated with problem solving is LEX [85]. Induction is based on specific facts (examples) instead of general axioms as in deduction. The goal of the induction is to formulate plausible general assertions that both explain the given facts and are capable of predicting unseen facts. In other words, the inductive inference tries to obtain a complete and correct description of a given phenomenon from specific (maybe partial) observations of it. The description inductively obtained is true at least for already seen examples but nothing can be assured for unseen examples. The most frequent application of inductive learning is concept learning. The goal of concept learning is to find symbolic descriptions expressed in high-level terms that are understandable by people. Concept learning can be defined as follows: Given: A set of (positive and negative) examples of a concept and, possibly, some background knowledge. Find: A general description (hypothesis) of the concept describing all the positive examples and none of the negative examples. Background knowledge defines assumptions and constraints imposed on examples and generated descriptions, and any relevant domain knowledge. Background knowledge can be in different forms [52], e.g. in declarative form or in procedural form, as sequences of instructions for executing specific tasks (control knowledge). Concept learning can be viewed as the task of searching through a large space of hypotheses implicitly defined by the hypothesis representation language. The goal of this search is to find the hypothesis (description) that best explains the examples. The language used is very important since it defines the hypothesis space, i.e. what knowledge can be expressed, and, therefore, what knowledge can be learned. The language has to be chosen with care in order to easily express all the desired knowledge. Most commonly used languages are constrained forms of predicate calculus, like decision trees, production rules, semantic nets and frames. The background knowledge includes a preference criterion allowing the reduction of the set of hypotheses to a smaller one containing the most preferable hypotheses. Typically, the preference criterion characterises the desired properties of the searched inductive hypothesis. This criterion is necessary when the description language is complete, i.e. all possible hypotheses can be expressed. An alternative way to constrain the hypothesis space is using a biased description language in which not all the possible hypotheses can be expressed (i.e. the language is incomplete). Utgoff and Mitchell [86] defined bias as anything influencing the way in which the induction is made. Thus, the notion of bias includes any input besides examples, and any parameter or strategy that may be modified by the user of a learning system. The declarative bias, i.e. a bias explicitly given by the user, has as an advantage the possibility to be used in several systems and allows meta-level reasoning about it. This second advantage is important since if a current bias is considered insufficient, it may be changed [85]. Inductive concept learning methods can be classified according to the following perspectives: Supervised~unsupervised: Supervised methods need an oracle that provides the classes (concepts) to which the examples belong and that classifies the examples. Unsupervised methods have to discover the concepts to which the examples belong. Single~multiple concept learning: This classification is according to the number of concepts that have to be learned. Single concept learning can be achieved in two situations: (1) inputs are only positive examples, (2) inputs are positive and negative examples. Multiple concept learning also distinguishes two cases: (1) when the descriptions of the different classes (concepts) are mutually disjoint, (2) when the descriptions of concepts overlap, i.e. an example can satisfy the descriptions of several classes. Multiple concept learning has been implemented in AQVAL/1 and AQI 1 [53]. Propositional~relational learners: Methods using a formalism equivalent to propositional calculus are called attribute-value learners or propositional learners. These methods use objects described as a fixed collection of attributes, each of them taking a value from a corresponding pre-specified set of R. Lopez de Mantaras, E. Armengol / Data & Knowledge Engineering 25 (1998) 99-123 101 values. Methods that learn first-order relational descriptions are called relational learners. They induce descriptions of relations and use objects described in terms of their components and relations among components. The background knowledge is formed by relations which, as the language of examples and concept descriptions, are typically subsets of first order logic. In particular, learners that induce hypotheses in the form of logic programs (Horn clauses) are called inductive logic programming systems. An important application of inductive learning is the automatic construction of KB for Expert Systems, being an alternative to classic knowledge acquisition methods. Inductive techniques can also be used for the refinement of existing KB since they allow the detection of inconsistencies, redundancies, and lack of knowledge, and also allow the simplification of the rules provided by the domain expert. Application domains such as biology, psychology, medicine and genetics have benefited from the capability of inductive methods to detect patterns present in the input examples. In the following sections both propositional and relational learners are explained in more detail. 
