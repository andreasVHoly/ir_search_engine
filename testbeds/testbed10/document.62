An evaluation of machine-learning methods for predicting pneumonia mortality 

The construction of computer decision-support systems from patient databases has longed played an important role in medical informatics. For example, starting in the early 196Os, researchers investigated the construction from data of Bayesian diagnostic systems that assume conditional independence of findings given a disease state [14,25]. As clinical information is stored increasingly in computer databases, the opportunities expand for using this information to help improve patient care and reduce health-care costs. Often, standard statistical methods, such as logistic regression, are used currently to construct predictive models in medicine. In the last decade, however, researchers in artificial intelligence have developed new machine-learning methods that construct computer models from data. In the current journal, for instance, numerous articles within the past few years have appeared discussing unsupervised [10,19] as well as supervised [8,16,26,27] machine-learning research in medicine. Such methods have the potential to complement and augment existing statistical techniques. To understand the degree to which this potential can be realized in medicine, it is important to compare the performance of statistical methods and machine-learning methods on real clinical tasks using real clinical data. This paper reports one such investigation in the construction and application of computer models to predict patient mortality. The particular machine-learning methods investigated are neural-network learning, a rule-learning technique, two causal discovery methods, a simple Bayesian classifier, and a generalized decision-tree induction method. Two statistical techniques, namely, logistic regression and a K-nearest neighbor method, are investigated as well. In the study reported here, we focus on predicting patient mortality in the area of community-acquired pneumonia. Pneumonia is an important disease that affects over 3 million people annually in the US [23]. It is the sixth leading cause of death in this country [24]. In 1987 it was responsible for over 900000 hospital admissions [23], and the resulting health-care costs that year were more than 3 billion dollars. In -the current study, we concentrate on the task of predicting mortality of hospitalized patients from their findings at initial presentation with pneumonia. Such predictions may be useful to clinicians in making decisions about where to treat patients with pneumonia. We want to identify as large a group of patients as possible who can be treated safely at home for pneumonia, because (1) this strategy is likely to reduce the costs of treating pneumonia, and (2) it will allow patients with mild cases of pneumonia to be treated at home, where typically they would be more comfortable. 

The ability to accurately predict mortality in patients with pneumonia may be useful to clinicians in making decisions about where to treat patients with pneumonia. We would like to identify as large a group of pneumonia patients as possible who could be treated safely at home. We studied a population in which approximately 11% of the patients died of pneumonia. For the eight different models we studied, the results show that when 10% of the patient population is predicted to survive, the predictive error rate is no more than half a percent. Additional analysis suggests that this rate may be slightly better than we would expect on a larger sample of patients than the ones we tested. The lowest error rate for each fps level was attained by one of the three following methods: neural networks, HME, and lo Cross validation increases the power of the statistical comparison. Strictly speaking, the independence assumptions underlying the t-test are not valid: thus, these results, while informative, should be considered heuristic. G.F. Cooper et al. 1 Artificial Intelligence in Medicine 9 (1997) 107-138 135 logistic: regression. The results also show, however, that all eight models tested have absolute error rates within 1% of each other. Significantly, most of the models’ error rates are sufficiently close such that a very large test database would be needed to reliably establish whether the rates differ statistically. We have compared the predictive performance of eight models constructed by eight methods. Thus, the results directly reflect the performance of the models, rather than the methods. Nonetheless, for at least one method (neural networks) resampling results suggest that the model’s performance is a good indication of the method’s performance for this domain (relative to the assumptions made in applying the method). Our data do not indicate how the eight methods would perfomn in making predictions in other medical areas, nor would it be expected of them to do so; additional studies would be needed to address such issues of robustness. We examined four simple hybrid models that combine the predictions of five of the eight base models. There was one hybrid model that predicted as well or better than logistic regression for all six levels of fps. While these results look promising, we are not able to conclude at this point that this performance difference is statistically reliable. Recent results do lend hope that hybrids can be found that perform significantly better [22] than base models. The space of possible hybrid models is immense and largely unexplored. As one example, consider that the neural network method performed relatively well in the current study, but used all the variables. This suggests using one of the other methods to identify the variables for the neural network method to use in constructing a predictive model. Many other hybrid combinations are possible. We found that the models differ markedly in the number of variables and parameters they contain. We emphasize, however, that in the current study we did not direct our efforts toward reducing the number of variables and parameters in models generated by the eight methods. Future research may show, for example, that it is possible to reduce the number of variables and parameters used in developing a neural network model, and yet, maintain the low predictive error rate of that model. Such reductions are of interest because models with a smaller number of variables may be more quickly and easily used by clinicians, since those models would require that less data be input. Models with a relatively small number of parameters may be more readily converted into paper-based models that could be used widely in current medical practice. As medical information systems become more widespread and comprehensive in the data they capture, the need for paper-based decision tools is likely to decline. Even so, paper-based models may be useful as medical teaching tools. One model (PCLR) in the current study required only nine variables and ten parameters. We have performed a preliminary conversion of this model, as well as several others described in the current paper, into paper-based models. In a future study, we plan to test the accuracy of these paper-based models for predicting mortality. The current paper focuses on predictions of inpatient mortality. The applicability of such predictions in helping decide whether a patient should be treated at home versus in the hospital requires making the following methodological assumption: 
