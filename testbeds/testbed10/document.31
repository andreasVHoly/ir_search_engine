Research in multiagent systems includes the investigation of algorithms that select actions for multiple agents coexisting in the same environment. Multiagent systems are becoming increasingly relevant within artificial intelligence, as software and robotic agents become more prevalent. Robotic soccer, disaster mitigation and rescue, automated driving and information and e-commerce agents are examples of challenging multiagent domains. As the automation trend continues, we need robust algorithms for coordinating multiple agents, and for effectively responding to other external agents. Multiagent domains require determining a course of action for each agent just as in single-agent domains. Machine learning can be a powerful tool for finding a successful course of action and can greatly simplify the task of specifying appropriate behaviors for an agent. In particular, through learning an agent can discover and exploit the dynamics of the environment and also adapt to unforeseen difficulties in the task. These benefits have caused learning to be studied extensively for single-agent problems with a stationary environment. In multiagent environments, learning is both more important and more difficult, since the selection of actions must take place in the presence of other agents. We consider multiagent domains in which agents are forced to interact with other agents that may have independent goals, assumptions, algorithms, and conventions. We are interested in approaches where the agents can learn and adapt to the other agents’ behavior. Since we assume that the other agents also have the ability to adapt their behavior, we face a difficult learning problem with a moving target. The optimal course of action is changing as all the agents adapt. These external adapting agents violate the basic stationary assumption of traditional techniques for behavior learning. New techniques need to be considered to address the multiagent learning problem. Furthermore, multiagent learning has a strong connection to game theory, where players select actions to maximize payoffs in the presence of other payoff maximizing players. A few efforts have contributed new approaches to the multiagent learning problem, successfully demonstrating algorithms that can learn “optimal” policies under specific assumptions. In this article, we overview some of these algorithms while providing a parallel between game theory and multiagent learning. The analysis of previous algorithms leads us to introduce two desirable properties for multiagent learning algorithms: rationality and convergence. Interestingly, we note that previous algorithms offer either one of these properties but not both. In this article, we contribute a new learning technique: a variable learning rate. We introduce this concept and provide a specific principle to adjust the learning rate, namely the WoLF principle, standing for “Win or Learn Fast”. We successfully develop and apply the WoLF principle within different learning approaches. Given the novelty of the WoLF principle, we face the challenge of determining whether a WoLF-based learning algorithm is rational and convergent according to our own introduced properties of multiagent learning algorithms. We show the rationality property and we contribute a theoretical proof of the convergence of WoLF gradient ascent in a restricted class of iterated matrix games. We then show empirical results suggesting convergence of an extended WoLF algorithm and compare its performance in a variety of game situations used previously by other learning algorithms. The article is organized as follows. In Section 2 we describe the stochastic game framework as a description of the multiagent learning problem. We also examine how previous techniques have one of two crucial shortcomings. In Section 3 we describe the variable learning rate technique and the WoLF principle. We analyze it theoretically on a restricted class of games, proving it overcomes the shortcomings of previous algorithms. In Section 4 we describe a practical algorithm that extends this technique to the general class of stochastic games. Finally, in Section 5 we show results demonstrating the applicability and effectiveness of the algorithms we introduced. 2. Stochastic games and learning In this section we give an overview of the stochastic game framework. We also introduce two desirable properties of multiagent learning algorithms: rationality and convergence. We then examine previous learning algorithms specifically comparing them with respect to these two properties. 2.1. Stochastic game framework Before presenting the formal definition of a stochastic game we examine other related models. We begin by examining Markov Decision Processes—a single-agent, multiple state framework. We then examine matrix games—a multiple-agent, single state framework. Finally, we introduce the stochastic game framework, which can be seen as the merging of MDPs and matrix games. Due to this background of game theory and agentbased systems, we will use the terms agent and player interchangeably. 2.1.1. Markov decision processes A Markov decision process (MDP) [3,13] is a tuple, (S,A,T ,R), where S is the set of states, A is the set of actions, T is a transition function S × A × S → [0, 1], and R is a reward function S ×A → R. The transition function defines a probability distribution over next states as a function of the current state and the agent’s action. The reward function defines the reward received when selecting an action from the given state. Solving MDPs consists of finding a policy, π :S → A, mapping states to actions so as to maximize discounted future reward with discount factor γ . MDPs are the focus of much of the reinforcement learning (RL) work [17,29]. The crucial result that forms the basis for this work is the existence of a stationary and deterministic policy that is optimal. It is such a policy that is the target for RL algorithms. 2.1.2. Matrix games A matrix game or strategic game [21,22] is a tuple (n,A1...n, R1...n), where n is the number of players, Ai is the set of actions available to player i (and A is the joint action space A1 ×···× An), and Ri is player i’s payoff function A → R. The players select actions from their available set and receive a payoff that depends on all the players’ actions. These are often called matrix games, since the Ri functions can be written as n-dimensional matrices. In matrix games players are finding strategies to maximize their payoff. A pure strategy selects some action deterministically. A mixed strategy selects actions according to a probability distribution over the available actions. A strategy for player i is denoted σi ∈ PD(Ai), i.e., a probability distribution over the set of actions available to that player. A pure strategy is one of these distributions that assigns some action a probability of one. We use the notation A−i to refer to the set of joint actions of all players excluding player i, 218 M. Bowling, M. Veloso / Artificial Intelligence 136 (2002) 21
