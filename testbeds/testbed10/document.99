A machine learning approach to reading level assessment Sarah 

E. Petersen a,*, Mari Ostendorf b

1. Introduction The US educational system is faced with the challenging task of educating growing numbers of students for whom English is a second language (US Department of Education, 2005). In the 2001–2002 school year, Washington state had 72,215 students (7.2% of all students) in state programs for Limited English Proficient (LEP) students (Bylsma et al., 2003). In the same year, one quarter of all public school students in California and one in seven students in Texas were classified as LEP (US Department of Education, 2003). Reading is a critical part of language and educational development, but finding appropriate reading material for LEP students is often difficult. To meet the needs of their students, bilingual education instructors seek out ‘‘high interest level” texts at low reading levels, e.g., texts at a first or second grade reading level that support the fifth grade science curriculum. (Teachers of teenagers who are poor readers face a similar problem.) Finding reading materials that fulfill these requirements is difficult and time-consuming, particularly when trying to meetthe needs of different students, and teachers are often forced to rewrite texts themselves to suit the varied needs of their students. Natural language processing technology can help automate the task of selecting appropriate reading material for bilingual students. Information retrieval systems successfully find topical materials and even answer complex queries in text databases and on the World Wide Web. However, an effective automated way to assess the reading level of the retrieved text is still needed. Our strategy is to apply text classification techniques to this problem. In preliminary work (Schwarm and Ostendorf, 2005), we developed a method of reading level assessment that uses support vector machines (SVMs) to combine features from n-gram language models (LMs) and parse trees, with several traditional features used in reading level assessment. We found that SVM-based detectors incorporating features from LMs and other sources outperformed LM-based detectors. In this paper, we present expanded results for the SVM detectors, including:

addressing the problem of generalizing the classifier to handle new data that may include other grade levels beyond those in the hand-labeled training data by introducing unlabeled negative training data (newswire text); investigating the degree to which syntactic features provide a benefit over traditional lexical features; and exploring the usefulness of a regression model as an alternative to the binary detection framework originally proposed, particularly in the context of limited annotated training data. 

We also include experiments with human annotators to provide insights into the task difficulty and to present different methods of evaluating our detectors in comparison to existing approaches to reading level assessment. In the sections to follow, we provide a bit more background on related research in reading level assessment, followed by a description of the data used, the details of the approach and the experiment results. Experimental results strengthen our earlier findings that SVMs outperform traditional methods for reading level assessment and findings in other work that syntactic features provide only a small benefit for this task. They also show that the detection model is a better match to this problem than a regression model, at least for the case where annotated training data is limited. In the work with human annotations, we find that the humans actually do less well than the SVM in labeling the target data, indicating that different groups may use different criteria for reading level assessment and supporting the use of machine learning as means of tuning the decision function to the needs of a particular group.

8. Conclusions and future work In summary, we combine features from n-gram LMs, an automatic parser, and traditional methods of readability assessment in an SVM framework to classify texts based on reading level. We show that unlabeled negative training data can be used to augment a corpus with only positive labels, but more effectively for a detection paradigm than regression. We also confirm other findings that syntactic features have only a small effect on the overall performance of the detectors. Both regression and detection SVMs compare favorably to other existing methods using several different methods to measure performance. The SVM performance is better than that of human annotators when both are compared to the Weekly Reader labels, but the humans appear to be using different annotation criteria since their inter-annotator agreement is higher than their agreement with the Weekly Reader labels. There are many possibilities for improving performance through more extensive exploration of model structures and feature extraction techniques, including using different size OOV lists, new parse features, different feature selection thresholds for the class language models, etc. However, our initial experiments in extending the feature vector showed limited success probably due to the small amount of training data available, despite the reputation of SVMs being able to handle large dimensional feature vectors. The SVM detectors are trainable, which makes it not surprising that they outperform general classifiers, but this is an important characteristic for tuning performance for the needs of particular groups (e.g., native language learners vs. second language learners) or specific needs of particular students. The variability in human annotation also supports a need for automatic learning, which can tune to the particular conventions that specific teachers want to use. Since annotated data is so difficult to find, development of adaptation techniques will be important. Some possible directions include SVM adaptation using active learning, e.g., Tong and Koller (2001) or relevance feedback, e.g., Drucker et al. (2001).
