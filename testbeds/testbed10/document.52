A new style of human–computer interaction is emerging, in which some reasoning and intelligence reside in the computer itself. These intelligent systems and user interfaces attempt to adapt to their users’ needs, to incorporate knowledge of the individual’s preferences, and to assist in making appropriate decisions based on the user’s data history. These approaches use artificial intelligence tosupport the system’s part of the reasoning. One increasingly common approach being brought to intelligent systems and user interfaces is machine learning, in which the system learns new behaviors by examining usage data. Traditionally, machine learning systems have been designed and implemented off-line by experts and then deployed. Recently however, it has become feasible to allow the systems to continue to adapt to end users by learning from their behavior after deployment. Interactive email spam filters, such as in Apple’s Mail system, are prime examples. Although machine learning is often reasonably reliable, it is rarely completely correct. One factor is that statistical methods require many training instances before they can reliably learn user behavior. Sometimes correctness is not critical. For example, a spam filter that successfully collects 90% of dangerous, virus-infested spam leaves the user in a far better situation than having no spam filter at all. Butsometimes overall correctness or even correctness for certain types of data (such as data from a specific class or data with a particular combination of features) is important. For example, recommender systems that recommend substandard suppliers or incorrect parts, language translators that translate incorrectly, decision support systems that lead the user to overlook important factors, and even email classifier algorithms that misfile important messages could cause significant losses to their users and raise significant liability issues for businesses. Further, too much inaccuracy in ‘‘intelligent’’ systems erodes users’ trust. When accuracy matters, allowing the user to help could make a crucial difference. Therefore, approaches have begun to emerge in which the user and the system interact with each other, not just to accomplish the goal of the moment, but also to improve the system’s accuracy in its services to the user over the longer term. Our work falls under the general category of mixed initiative user interfaces, in which users collaboratively interact with intelligent systems to achieve a goal (Horvitz, 1999). As Section 2 explains, this direction is still in its infancy: the norm for the few machine learning systems that communicate with users at all is to allow the user to indicate only that a prediction was wrong or to specify what the correct prediction should have been. This is just a glimpse of the rich knowledge users have about how to make the correct prediction. We began to consider whether end users might be able to provide rich guidance to machine learning systems. We wondered whether enabling them to provide this guidance could substantially improve the speed and accuracy of these systems, especially early on in training when machine learning does not possess a lot of knowledge. Many questions arise from this possibility. Will users be interested in providing rich feedback to machine learning systems? If so, what kind of advice will they give? Will their feedback be usable by machine learning algorithms? If so, how would the algorithms need to be changed so that they could make use of this feedback? In this paper, we begin an exploration of these questions. The overall premise we explore is that, if the machine learning system could explain its reasoning more fully to the user, perhaps the user would, in return, specify why the prediction was wrong and provide other, rich forms of feedback that could improve the accuracy of machine learning. There are implications for both directions of the communication involved in this premise. First, the system’s explanations of why it has made a prediction must be usable and useful to the user. Second, the user’s explanations of what was wrong (or right) about the system’s reasoning must be usable and useful to the system. Both directions of communication must be viable for production/processing by both the system and the user. As a first step to investigate possibilities for both directions of communication, we conducted three studies. The first was a think-aloud study with email users, toinform the design of future machine learning systems aiming to support such communications. In this first study, machine learning algorithms sorted email messages into folders and explained their reasoning using three different explanation paradigms: Rule-based, Keyword-based, and Similarity-based. The participants were asked to provide feedback to improve the predictions. No restrictions were placed upon the form or content of participants’ feedback. We then conducted two follow-up experiments, in which we changed our machine learning algorithms to make use of some of what the users advised in the first study, evaluating the results. In both of these experiments, we conducted off-line experiments to investigate how some of the user feedback could be incorporated into machine learning methods and to evaluate the effectiveness of doing so. Thus, our research questions were: RQ 1. How can machine learning systems explain themselves such that (a) end users can understand the system’s reasoning, and (b) end users are willing to provide the system rich, informative feedback with potential to improve the system’s accuracy? RQ 2. What types of feedback will end users give? That is, how might we categorize the nature of their feedback from the perspective of machine learning, and what sources of background knowledge underlie users’ feedback? RQ 3. Can these types of user feedback be assimilated by existing learning algorithms? If so, exactly how could some of these types of user feedback be incorporated into machine learning algorithms, and does doing so actually improve the performance of algorithms?
