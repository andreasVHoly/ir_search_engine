The advent of combinatorial chemistry in the mid-1980s has allowed the synthesis of hundreds, thousands and even millions of new molecular compounds at a time. Nevertheless, even this level of compound production will fall short of exhausting the trillions of potential combinations within a few thousand years. The need for a more refined search than simply producing and testing every single molecular combination possible has meant that statistical methods and, more recently, intelligent computation have become an integral part of the drug production process. Structure–activity relationship (SAR) analysis is one technique used to reduce the search for new drugs. It also presents an extremely challenging problem to the field of intelligent systems. A successful solution to this problem has the potential to provide significant economic benefit via increased process efficiency. We present evidence that a recently developed, state-of-the-art machine learning technique outperforms several of its competitors when applied to such a problem.
The underlying assumption behind SAR analysis is that there is a relationship between the variation of biological activity within a group of molecular compounds with the variation of their respective structural and chemical features. The analyst searches for a rule or function that predicts a molecule's activity from the values of its physicochemical descriptors. The aim of SAR analysis is to discover such general rules and equations.
It is common to prefix the acronym SAR with either ‘Q’ or ‘q’, indicating ‘quantitative’ or ‘qualitative’, respectively. QSAR involves modelling a continuous activity for quantitative prediction of the activity of previously unseen compounds. qSAR aims to separate the compounds into a number of discrete class types, such as ‘active’ and ‘inactive’ or ‘good’ and ‘bad’. Confusion can arise due to differing use of the two terms. For example, discrete classification problems are sometimes referred to as QSAR, as they quantify the activity types of the compounds examined. A good rule of thumb is that QSAR analysis is a regression problem and qSAR analysis a classification problem.
Activities of interest include chemical reactivity, biological activity and toxicity. In this experiment, we wish to predict qualitative biological activity for drug design, using a publicly available data set. Typically, the analyst examines the descriptors of a finite number of compounds in relation to a known target activity and attempts to model the relationship between them. A priori knowledge of the underlying chemistry may also be considered. The methods used to represent molecular features when producing a SAR vary greatly and a lot of work is carried out on this sub-problem alone. The problem of choosing a suitable representation will not feature further in this paper, however, as we use that given by those who produced the data. The aim of SAR analysis is to discover rules that successfully predict the activities of previously unseen compounds. Solving SAR problems where data sets contain few compounds and many descriptors is difficult with standard statistical approaches. Data sets which describe highly non-linear relationships between structure and activity, pose similar problems.
SAR analysis is applied to many areas of modern drug design and production, on both the small, medium and large scales. Small-scale problems, such as the one presented here, are commonly found when designing combinatorial libraries in the early stages of the production process. Due to the number of potential molecular combinations from existing molecular databases, the search for potentially useful combinations must be refined. This can be done by discriminating between molecules that are likely to produce the desired effect upon synthesis and those that are not—a process termed ‘virtual’ or ‘in silico’ screening. In doing so, the drug designer can build up small sub-libraries of suitable molecules taken from a larger and more diverse library, greatly reducing the search in doing so. SAR analysis is used to find suitable molecules for the sub-library. Accuracy is important in the creation of combinatorial libraries, as to miss an interesting molecule is lose a potential lead for drug discovery.
Artificial intelligence techniques have been applied to SAR analysis since the late 1980s, mainly in response to increased accuracy demands. Intelligent classification techniques that have been applied to this problem include neural networks (Devillers, 1999b), genetic algorithms (Devillers, 1999a) and decision trees (Hawkins et al., 1997). The problem of combining high classification accuracy with informative results has also been tackled via the use of hybrid and prior knowledge techniques, such as expert systems (Gini et al., 1998). Machine learning techniques have, in general, offered greater accuracy than have their statistical forebears, but there exist accompanying problems for the SAR analyst to consider. Neural networks, for example, offer high accuracy in most cases but can suffer from over-fitting the training data (Manallack and Livingstone, 1999). Other problems with the use of neural networks concern the reproducibility of results, due largely to random initialization of the network and variation of stopping criteria, and lack of information regarding the classification produced (Manallack and Livingstone, 1999). Genetic algorithms can suffer in a similar manner. The stochastic nature of both population initialization and the genetic operators used during training can make results hard to reproduce (Goldberg, 1989). Decision trees offer a large amount of information regarding their decisions, in the form of predictive rules. There exist powerful decision tree methods, such as OC1 (Murthy et al., 1994), capable of competing with the previous two techniques, but they commonly require a great deal of tuning to do so. Simpler techniques are available and popular but, as displayed in our results, can fail to attain similar performance levels.
Owing to the reasons outlined above, there is a continuing need for the application of more accurate and informative classification techniques to SAR analysis.
3. Problem description
The data used in this experiment were obtained from the UCI Data Repository (Blake and Merz, 1998) and are described in King et al. (1992). The problem is to predict the inhibition of dihydrofolate reductase by pyrimidines. The biological activity is measured as log(1/Ki), where Ki is the equilibrium constant for the association of the drug to dihydrofolate reductase. QSAR problems are generally formulated as regression problems, i.e. learn the posterior probability of the target, y, given some predictive attributes, x. In this case the target is log(1/Ki) and the attributes are as follows. Each drug has three positions of possible substitution. For each substitution position there are nine descriptors: polarity, size, flexibility, hydrogen-bond donor, hydrogen-bond acceptor, π donor, π acceptor, polarizability and σ effect. Each of the 24 non-hydrogen substituents in the compound set was given an integer value for each of these properties (see King et al. (1992) for details); lack of a substitution is indicated by nine −1s. This gives 27 integer attributes for each drug. The approach here is to avoid solving a regression problem by recasting it as one suitable for classification or inductive logic programming (ILP), the ILP approach is described in King et al. (1992). Here we focus on the classification problem.
The task is to learn the relationship great(dn, dm) which states that drug no. n has a higher activity than drug no. m. Each instance consists of two drugs, giving 54 attributes in total, and a label ‘true’ or ‘false’, indicating the value of the relationship great(). There are 55 compounds in the data set. In order to obtain a good estimate of the performance of the algorithms, the data were partitioned into a five-fold cross-validation series as follows. The 55 compounds were randomly partitioned into 5 sets of 11 compounds. Each fold consisted of a training set A of 44 compounds and a test set B of 11 compounds. The classification training data were constructed by labelling each pair (n,m)∈A×Aaccording to the true value of great(dn, dm). This gives 44×43=1892 examples of great(), in practice, however, the training sets were slightly smaller since great() is not defined when the two drugs have the same activity. The classification test data were constructed by forming all pairs (n,m)∈(A×B)∪(B×A)∪(B×B) (i.e. all comparisons not in the classification training set) which gives slightly less than 44×11+11×44+11×10=1078 examples of great().
Due to the above construction each instance in the data is followed by its inverse, for example if the first instance represents great(d2, d1)=true then the second instance represents great(d1, d2)=false. This produces a symmetric two-class classification problem where the classes are equal in size and have equal misclassification costs.
Each of the training sets was then used to train an algorithm to produce a classification rule. This classification rule can then be used to predict which of two unseen compounds has the greatest activity. The generalization ability of such a rule, that is, the probability that it will correctly rank two unseen compounds in terms of activity, is estimated as follows. Each of the trained classifiers is used to classify the corresponding test set. Each of these five error rates is an estimate of the true error rate, the final estimate of error rate is taken to be their mean. Note that even if the sets of training and test compounds are drawn independently, by construction, the classification test sets are not independent of the classification training sets. Thus, this estimate of true error rate will be biased downwards and will give an optimistic estimate of the error rate on unseen cases. Nevertheless, it is adequate for a comparison of techniques. In the following sections ‘training set’ will mean the set of approximately 1800 labelled instances of great() and similarly for ‘test set’.
Finally, note that in learning the relationship great() we can rank the compounds in terms of their activity without going to the effort of predicting that activity. This is in contrast to the more general and harder problem of learning a set of rules or a regression equation to predict activity. Formulated as a classification problem there are approximately 55×54=2970 examples in 54 dimensions. This is still a small problem in comparison to those that could be formulated by the pharmaceuticals industry. We have chosen this problem to demonstrate the utility of the SVM in cheminformatics, see Section 6 below for a discussion.
4. Learning algorithms
Classifiers typically learn by empirical risk minimization (ERM) (Vapnik, 1998), that is they search for the hypothesis with the lowest error on the training set. Unfortunately, this approach is doomed to failure without some sort of capacity control (Bishop, 1995 and Burges, 1998). To see this, consider a very expressive hypothesis space. If the data are noisy, which is true of most real world applications, then the ERM learner will choose a hypothesis that accurately models the data and the noise. Such a hypothesis will perform badly on unseen data. To overcome this we limit the expressiveness of the hypothesis space. In the remainder of this section are described the learning algorithms used and the methods of capacity control. Note that the SVM is the only algorithm which performs capacity control simultaneously with risk minimization, this is termed structural risk minimization (SRM).
In the following the Euclidean norm of a d-dimensional vector  is denoted by ‖·‖, and is defined to be

where x is a column vector and superscript T denotes the transpose.

